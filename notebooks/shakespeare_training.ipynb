{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/shakespeare_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lBrppZd_A0"
      },
      "source": [
        "# Training a tiny SLM on a corpus of Shakespeare\n",
        "The intention of this notebook is to demonstrate a setup for experimenting with a tiny SLM.\n",
        "The following tools are used:\n",
        "* Colab (https://colab.research.google.com/) for notebook execution\n",
        "* DagsHub (https://dagshub.com/) for project tracking\n",
        "* MLFlow (https://mlflow.org/) for experiment tracking\n",
        "* Hydra (https://hydra.cc/) for configuration management\n",
        "* GPTNeoX (https://github.com/EleutherAI/gpt-neox) for model training\n",
        "* Tensorboard (https://www.tensorflow.org/tensorboard) for experiment monitoring\n",
        "* DVC (https://dvc.org/) for data management\n",
        "* GitHub (https://github.com/) for code management\n",
        "* Backblaze (https://backblaze.com/) for data storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uYyxsLpOuQR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b7930f-46bb-45be-abc3-6ab5b2744c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Date and Time: 2024-11-28 17:17:19.915455\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(\"Current Date and Time:\", datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google import colab\n",
        "  isColab = True\n",
        "except:\n",
        "  isColab = False\n",
        "\n",
        "try:\n",
        "  from google.colab import userdata\n",
        "  if userdata.get('GITHUB_NAME'):\n",
        "    !git config --global user.name \"{userdata.get('GITHUB_NAME')}\"\n",
        "  if userdata.get('GITHUB_EMAIL'):\n",
        "    !git config --global user.email \"{userdata.get('GITHUB_EMAIL')}\"\n",
        "  if userdata.get('AWS_SECRET_ACCESS_KEY'):\n",
        "    !echo \"export AWS_SECRET_ACCESS_KEY={userdata.get('AWS_SECRET_ACCESS_KEY')}\" >> ~/.bashrc\n",
        "    os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "  if userdata.get('AWS_ACCESS_KEY_ID'):\n",
        "    !echo \"export AWS_ACCESS_KEY_ID={userdata.get('AWS_ACCESS_KEY_ID')}\" >> ~/.bashrc\n",
        "    os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "print(\"isColab:\", isColab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR5zBqZ0Q25t",
        "outputId": "3acfaba5-a6bc-4d38-aac5-580eab96cf21"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "isColab: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HBH4eN9Id_A7"
      },
      "outputs": [],
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "workspaceDir = \"/content\"\n",
        "if isColab:\n",
        "  gpt_neox_colabDir = f\"{workspaceDir}/GPT-NeoX-Colab\"\n",
        "else:\n",
        "  gpt_neox_colabDir = f\"/workspace\"\n",
        "GPTNeoXDirName = \"gpt-neox\"\n",
        "GPTNeoXDir = f\"{workspaceDir}/{GPTNeoXDirName}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e27VRVq07s"
      },
      "source": [
        "# Cloning Git Repos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "j_hUsQxlhnou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d10deb-6d4c-45e4-9721-940a07e26f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'GPT-NeoX-Colab' already exists and is not an empty directory.\n",
            "Cloned with PAT\n",
            "CPU times: user 37.1 ms, sys: 4.38 ms, total: 41.5 ms\n",
            "Wall time: 3.24 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX-Colab\n",
        "%cd {workspaceDir}\n",
        "if isColab:\n",
        "  # Don't use --depth 1 because that does not play nice with git-annex\n",
        "  if userdata.get('GITHUB_PAT'):\n",
        "    !git clone --depth 1 https://{userdata.get('GITHUB_PAT')}@github.com/markNZed/GPT-NeoX-Colab.git\n",
        "    print(\"Cloned with PAT\")\n",
        "  else:\n",
        "    !git clone --depth 1 https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "    print(\"Cloned without PAT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Jp8wncmZDAew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd8973e-8258-481e-a295-62f58c19868c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n",
            "Install DVC\n",
            "Downloading .venv.tar.gz\n",
            "\u001b[0mUnzipping .venv.tar.gz\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pigz is already the newest version (2.6-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Untarring .venv.tar\n",
            "CPU times: user 1.82 s, sys: 250 ms, total: 2.07 s\n",
            "Wall time: 5min 33s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import sys\n",
        "%cd {gpt_neox_colabDir}\n",
        "print(\"Install DVC\")\n",
        "%pip install -q python-dotenv dvc[s3]\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv()\n",
        "activate_script = f\"{gpt_neox_colabDir}/.venv/bin/activate\"\n",
        "USE_VENV = True\n",
        "if USE_VENV:\n",
        "  # Disabling pydevd_plugins so we do not get a restart warning\n",
        "  #if \"pydevd_plugins\" in sys.modules:\n",
        "  #  del sys.modules[\"pydevd_plugins\"]\n",
        "  venv_dir = \".venv\"\n",
        "  venv_tar_file = f\"{venv_dir}.tar\"\n",
        "  venv_gz_file = f\"{venv_tar_file}.gz\"\n",
        "  if not os.path.isfile(venv_gz_file):\n",
        "    print(f\"Downloading {venv_gz_file}\")\n",
        "    !dvc pull -q {venv_gz_file}\n",
        "  if not os.path.isfile(venv_tar_file):\n",
        "    print(f\"Unzipping {venv_gz_file}\")\n",
        "    !apt-get install -y pigz\n",
        "    !pigz -d -p 4 {venv_gz_file}\n",
        "  if not os.path.isfile(venv_dir):\n",
        "    print(f\"Untarring {venv_tar_file}\")\n",
        "    !tar -xf {venv_tar_file}\n",
        "elif isColab:\n",
        "    !uv sync -q --dev\n",
        "    !uv run pip install -q -e .\n",
        "    !source {activate_script} && pip install -q -r requirements_colab.txt\n",
        "    !source {activate_script} && pip install -q ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "y8Mic7aoiNxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b90a097-de42-463b-c0c9-6bdd17ca9cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n",
            "Collecting          |5.00 [00:00,  120entry/s]\n",
            "Fetching\n",
            "!\u001b[A\n",
            "  0% |          |0/? [00:00<?,    ?files/s]\u001b[A\n",
            "Fetching\n",
            "Building workspace index          |3.00 [00:00,  743entry/s]\n",
            "Comparing indexes          |4.00 [00:00,  719entry/s]\n",
            "Applying changes          |0.00 [00:00,     ?file/s]\n",
            "Everything is up to date.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#@title Fetch training data\n",
        "%cd {gpt_neox_colabDir}\n",
        "!dvc --quiet pull \"data/shakespeare/shakespeare.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "It4jtnb2yvkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6731fbbf-7c06-4e03-c0d2-90bf1e054d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n"
          ]
        }
      ],
      "source": [
        "#@title Fetch processed training data\n",
        "%cd {gpt_neox_colabDir}\n",
        "#!dvc --quiet pull \"data/shakespeare/shakespeare.jsonl\"\n",
        "#!dvc --quiet pull \"data/shakespeare/shakespeare_text_document.bin\"\n",
        "#!dvc --quiet pull \"data/shakespeare/shakespeare_text_document.idx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "collapsed": true,
        "id": "FjLEIFCR6d8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d334a01-1a25-40fd-e546-207ae445c15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'gpt-neox' already exists and is not an empty directory.\n",
            "CPU times: user 5.83 ms, sys: 1.18 ms, total: 7.01 ms\n",
            "Wall time: 106 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX\n",
        "%cd {workspaceDir}\n",
        "#!git clone ---depth 1 https://github.com/EleutherAI/gpt-neox\n",
        "!git clone -b pipe_parallel_size_1 --depth 1 https://github.com/markNZed/gpt-neox.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va0pS1dvd_BD"
      },
      "source": [
        "# Python Environment\n",
        "It is faster to download a Python virtual environment and unzip it than to install all the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AUmdStRWhrUR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "if not USE_VENV:\n",
        "    # Could not redirect to /dev/null in the standard Colab notebook (maybe no output for a particular time?)\n",
        "    # Currently deepspeed from GTP-NeoX is not compatible with logging in torch >= 2.4\n",
        "    !source {activate_script} && pip install -q torch==2.3 torchaudio==2.3.0 torchvision==0.18.0 transformers==4.38.0 sentence-transformers==2.2.2\n",
        "    !source {activate_script} && pip install -q fsspec==2024.10.0 datasets==2.14.0 evaluate==0.4.3 lm-eval==0.4.1 tensorboard==2.17.0 tensorflow==2.17.0\n",
        "    %cd {GPTNeoXDir}\n",
        "    !source {activate_script} && pip install -q -r ./requirements/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jzX5ohGax6p"
      },
      "source": [
        "# Preparing Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "N-hmZjCc-WnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e5c12d-dabd-4591-8688-55bb19c4a181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n"
          ]
        }
      ],
      "source": [
        "#@title Converting text data to jsonl format\n",
        "import os\n",
        "\n",
        "%cd {GPTNeoXDir}\n",
        "!mkdir -p data\n",
        "\n",
        "# Check if the converted file exists\n",
        "if not os.path.isfile(f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl\"):\n",
        "    !source {activate_script} && python -c \"import gpt_neox_colab.utils; gpt_neox_colab.utils.ml.text2jsonl(\\\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.txt\\\", \\\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl\\\")\"\n",
        "\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl {GPTNeoXDir}/data/shakespeare.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x57thNaLa-yN"
      },
      "source": [
        "# Tokenizing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "JyD8RujkEUsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8badb0-929e-4bb9-bd88-65306cb81133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n",
            "/content/gpt-neox/processed_data\n",
            "Command: \n",
            "    source /content/GPT-NeoX-Colab/.venv/bin/activate && python /content/gpt-neox/tools/datasets/preprocess_data.py         --input /content/gpt-neox/data/shakespeare.jsonl         --output-prefix shakespeare         --tokenizer-type CharLevelTokenizer         --dataset-impl mmap         --append-eod\n",
            "    \n",
            "/content/gpt-neox/megatron/neox_arguments/arguments.py:1101: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "  assert (\n",
            "/content/gpt-neox/megatron/neox_arguments/arguments.py:1110: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "  assert (\n",
            "[2024-11-28 17:26:46,714] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "> building CharLevelTokenizer tokenizer ...\n",
            " > padded vocab (size: 512) with 0 dummy tokens (new size: 512)\n",
            "Vocab size: 512\n",
            "Output prefix: shakespeare\n",
            "> building CharLevelTokenizer tokenizer ...\n",
            " > padded vocab (size: 512) with 0 dummy tokens (new size: 512)\n",
            "Processed 32700 documents (48249.28 docs/s, 1.51 MB/s).: : 32700it [00:00, 47804.92it/s]\n",
            "CPU times: user 66.5 ms, sys: 10.3 ms, total: 76.8 ms\n",
            "Wall time: 6.15 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Tokenizing jsonl formatted data\n",
        "import os\n",
        "\n",
        "%cd {GPTNeoXDir}\n",
        "!mkdir -p processed_data\n",
        "%cd processed_data\n",
        "\n",
        "# Check if the tokenized files exists\n",
        "a = f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.idx\"\n",
        "b = f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.bin\"\n",
        "if not os.path.isfile(a) or not os.path.isfile(b):\n",
        "    cmd = f\"\"\"\n",
        "    source {activate_script} && python {GPTNeoXDir}/tools/datasets/preprocess_data.py \\\n",
        "        --input {GPTNeoXDir}/data/shakespeare.jsonl \\\n",
        "        --output-prefix shakespeare \\\n",
        "        --tokenizer-type CharLevelTokenizer \\\n",
        "        --dataset-impl mmap \\\n",
        "        --append-eod\n",
        "    \"\"\"\n",
        "    print(f\"Command: {cmd}\")\n",
        "    !source {activate_script} && python {GPTNeoXDir}/tools/datasets/preprocess_data.py \\\n",
        "        --input {GPTNeoXDir}/data/shakespeare.jsonl \\\n",
        "        --output-prefix shakespeare \\\n",
        "        --tokenizer-type CharLevelTokenizer \\\n",
        "        --dataset-impl mmap \\\n",
        "        --append-eod\n",
        "    !cp {GPTNeoXDir}/processed_data/shakespeare_text_document.bin {gpt_neox_colabDir}/data/shakespeare\n",
        "    !cp {GPTNeoXDir}/processed_data/shakespeare_text_document.idx {gpt_neox_colabDir}/data/shakespeare\n",
        "\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.bin {GPTNeoXDir}/processed_data\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.idx {GPTNeoXDir}/processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "YNM2gpADtjM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9647270b-ef75-4eb0-9783-14988764151f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "PtOHyiVDhrUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07af7289-e7a0-4b2c-a3cf-26965ee3152b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running command: \n",
            "nohup nohup bash -c \" source /content/GPT-NeoX-Colab/.venv/bin/activate && cd /content/gpt-neox && python ./deepy.py train.py --conf_dir /content/GPT-NeoX-Colab/configs shakespeare shakespeare_deepy \"\n",
            "\n",
            "Started training with PID: 8040\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "# Start a detached background process using the temp config\n",
        "cmd = f\"\"\"\n",
        "nohup nohup bash -c \" source {activate_script} && \\\n",
        "cd {GPTNeoXDir} && \\\n",
        "python ./deepy.py train.py --conf_dir {gpt_neox_colabDir}/configs shakespeare shakespeare_deepy \"\n",
        "\"\"\"\n",
        "print(\"Running command:\", cmd)\n",
        "#cmd = \"nohup bash -c ls\" # Used to test without running on GPU\n",
        "\n",
        "# Start the process and retrieve the PID directly\n",
        "process = subprocess.Popen(\n",
        "    cmd,\n",
        "    shell=True,\n",
        "    executable='/bin/bash',\n",
        "    preexec_fn=os.setsid  # Starts the process in a new session\n",
        ")\n",
        "\n",
        "pid = process.pid\n",
        "print(f\"Started training with PID: {pid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "WVCgdZ1Ma9TI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f8443d7-827a-4a19-8848-c99d4b99e19d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorBoard log directory found. You can now launch TensorBoard.\n"
          ]
        }
      ],
      "source": [
        "#@title Wait until tensorboard log directory is created\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Path to the TensorBoard log directory\n",
        "tensorboard_log_dir = f\"{GPTNeoXDir}/tensorboard\"\n",
        "\n",
        "# Wait for the directory to be created\n",
        "while not os.path.exists(tensorboard_log_dir):\n",
        "    print(\"Waiting for TensorBoard log directory to be created...\")\n",
        "    time.sleep(10)  # Check every X seconds\n",
        "\n",
        "print(\"TensorBoard log directory found. You can now launch TensorBoard.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bfefGgc-ln-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c22a2698-0eb1-46f7-fcab-44d24758af34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Need to delete everything in checkpoints and tensorboard dir for a fresh run\n",
        "%cd {GPTNeoXDir}\n",
        "%tensorboard --logdir tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "1j3_fG5vV7AB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf08d75-3452-45ab-cb6d-83e2ca074e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest log file: /content/gpt-neox/logs/d85b74725624_stdout.txt\n"
          ]
        }
      ],
      "source": [
        "#@title Find the latest log file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Define the log directory and pattern for log files\n",
        "log_dir = f\"{GPTNeoXDir}/logs\"\n",
        "log_pattern = os.path.join(log_dir, \"*_stdout.txt\")\n",
        "\n",
        "# Get the list of log files that match the pattern\n",
        "log_files = glob.glob(log_pattern)\n",
        "\n",
        "# Ensure there are log files in the directory\n",
        "if log_files:\n",
        "    # Find the latest log file based on modification time\n",
        "    latest_log = max(log_files, key=os.path.getmtime)\n",
        "    print(\"Latest log file:\", latest_log)\n",
        "else:\n",
        "    latest_log = None\n",
        "    print(\"No log files found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QszVj7_vSP7L"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn13kNSRacnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7217bb7-65a4-411d-d0c0-de9bc6d1b81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 iterations\n",
            "Training is still running...\n"
          ]
        }
      ],
      "source": [
        "#@title Read the latest log file and extract the iteration count\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "\n",
        "# File to store the last read position (persistence between script runs)\n",
        "file_position = 0\n",
        "# Regular expression to match \"iteration <number> / <total>\"\n",
        "iteration_pattern = re.compile(r\"iteration\\s+(\\d+)\\s*/\\s*\\d+\")\n",
        "\n",
        "def read_new_iterations():\n",
        "    global file_position\n",
        "    # Open the log file and seek to the last position\n",
        "    with open(latest_log, \"r\") as file:\n",
        "        file.seek(file_position)\n",
        "        # Read new lines\n",
        "        new_lines = file.readlines()\n",
        "        file_position = file.tell()\n",
        "        # Process lines containing \"iteration\"\n",
        "        last_match = None\n",
        "        for line in new_lines:\n",
        "            match = iteration_pattern.search(line)\n",
        "            if match:\n",
        "                last_match = match\n",
        "        if last_match:\n",
        "            # Extract the iteration count from the regex match\n",
        "            iteration_count = int(last_match.group(1))\n",
        "            print(f\"{iteration_count} iterations\")\n",
        "\n",
        "# Periodically check if the process has completed\n",
        "while True:\n",
        "    # Poll the process to see if it has terminated\n",
        "    if process.poll() is not None:\n",
        "        # Process has completed\n",
        "        print(\"Training has completed.\")\n",
        "        break\n",
        "    else:\n",
        "        if latest_log:\n",
        "            read_new_iterations()\n",
        "        elif os.path.exists(f\"{experimentDir}/logs\"):\n",
        "            latest_log = get_latest_file(f\"{experimentDir}/logs\", \"*_stdout.txt\")\n",
        "        print(\"Training is still running...\")\n",
        "        time.sleep(30)  # Check every X seconds\n",
        "\n",
        "print(\"Training has finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJb7TUqi4vVg"
      },
      "outputs": [],
      "source": [
        "#@title Display training and validation Loss\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "import os\n",
        "import numpy as np\n",
        "# Path to the latest log file\n",
        "log_dir = \"tensorboard\"\n",
        "log_files = [os.path.join(log_dir, d) for d in os.listdir(log_dir)]\n",
        "latest_log_dir = max(log_files, key=os.path.getmtime)\n",
        "\n",
        "# Initialize EventAccumulator to load scalar data\n",
        "ea = event_accumulator.EventAccumulator(latest_log_dir)\n",
        "ea.Reload()  # Load all logs\n",
        "\n",
        "# List all scalar keys available in the logs\n",
        "scalar_keys = ea.Tags()['scalars']\n",
        "print(\"Available scalar keys:\", scalar_keys)\n",
        "\n",
        "# Extract training and validation losses\n",
        "train_loss = ea.Scalars('train/lm_loss')  # Adjust for actual name if necessary\n",
        "val_loss = ea.Scalars('validation/lm_loss')  # Adjust for actual name if necessary\n",
        "\n",
        "# Convert to lists for plotting\n",
        "train_loss_values = [x.value for x in train_loss]\n",
        "val_loss_values = [x.value for x in val_loss]\n",
        "\n",
        "# Find the lengths of both arrays\n",
        "len_train = len(train_loss_values)\n",
        "len_val = len(val_loss_values)\n",
        "\n",
        "iterations = None\n",
        "# Interpolate the shorter array\n",
        "if len_train != len_val:\n",
        "    if len_train > len_val:\n",
        "        # Interpolate validation loss to match the training loss length\n",
        "        iterations = np.linspace(1, len_train, len_train)\n",
        "        val_iterations = np.linspace(1, len_train, len_val)\n",
        "        val_loss_values = np.interp(iterations, val_iterations, val_loss_values)\n",
        "    else:\n",
        "        # Interpolate training loss to match the validation loss length\n",
        "        iterations = np.linspace(1, len_val, len_val)\n",
        "        train_iterations = np.linspace(1, len_val, len_train)\n",
        "        train_loss_values = np.interp(iterations, train_iterations, train_loss_values)\n",
        "else:\n",
        "    iterations = range(1, len_train + 1)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(iterations, train_loss_values, label='Training Loss')\n",
        "plt.plot(iterations, val_loss_values, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk8DhmmEZFyz"
      },
      "source": [
        "# Inference with GPT-NeoX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKb0Ar6NZFyz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%cd {GPTNeoXDir}\n",
        "# This has issues if used during training -  The server socket has failed to bind to [::]:29500 (errno: 98 - Address already\n",
        "# This will write over the logs\n",
        "!source {activate_script} && python ./deepy.py generate.py -d configs {gpt_neox_colabDir}/configs/shakespeare {gpt_neox_colabDir}/configs/shakespeare_gen\n",
        "!cat sample_output.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajLR98lDi9_8"
      },
      "outputs": [],
      "source": [
        "# 2.21.0 was the last 2 series but it asks for trust_remote_code\n",
        "!source {activate_script} &&  pip install datasets==2.14.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S-foNPFjfwH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# This has issues if used during training -  The server socket has failed to bind to [::]:29500 (errno: 98 - Address already\n",
        "# This will write over the logs\n",
        "# python ./deepy.py eval.py -d configs your_configs.yml --eval_tasks task1 task2 ... taskn\n",
        "# NOTE this will prompt for permission to run a download script - would need an older datasetse library to avoid this\n",
        "%cd {GPTNeoXDir}\n",
        "!source {activate_script} && python ./deepy.py eval.py -d configs {gpt_neox_colabDir}/configs/shakespeare {gpt_neox_colabDir}/configs/shakespeare_gen --eval_tasks hellaswag\n",
        "!cat sample_output.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7AOL1NJ43e"
      },
      "source": [
        "# Inference with Hugging Face\n",
        "\n",
        "## Convert model to HF format\n",
        "Here we are converting our model to `HuggingFace Format`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNdWrrkWfyT-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the checkpoints directory\n",
        "checkpoints_dir = f\"{GPTNeoXDir}/checkpoints\"\n",
        "\n",
        "# Read the 'latest' file to get the latest checkpoint name\n",
        "with open(os.path.join(checkpoints_dir, \"latest\"), \"r\") as f:\n",
        "    latest_checkpoint_name = f.read().strip()\n",
        "\n",
        "# Construct the full path to the latest checkpoint directory\n",
        "latest_checkpoint_path = os.path.join(checkpoints_dir, latest_checkpoint_name)\n",
        "print(\"Path to the latest checkpoint:\", latest_checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rplkNclPXgkH"
      },
      "outputs": [],
      "source": [
        "#@title Convert last checkpoint to huggingface model\n",
        "%cd {GPTNeoXDir}\n",
        "!source {activate_script} && python ./tools/ckpts/convert_neox_to_hf.py --input_dir {latest_checkpoint_path} --config_file {gpt_neox_colabDir}/configs/shakespeare.yml --output_dir {gpt_neox_colabDir}/data/shakespeare --architecture neox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qra1qQtC2oiI"
      },
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01ZRN2IceM8a"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, f\"{gpt_neox_colabDir}/my_env/lib/python3.10/site-packages\")\n",
        "\n",
        "from transformers import GPTNeoXForCausalLM\n",
        "import torch\n",
        "\n",
        "# Move to model directory\n",
        "%cd {gpt_neox_colabDir}\n",
        "\n",
        "# Assuming CharLevelTokenizer is properly imported and instantiated\n",
        "from src.gpt_neox_colab.CharLevelTokenizer import CharLevelTokenizer\n",
        "tokenizer = CharLevelTokenizer(vocab_size=512)\n",
        "\n",
        "# Load your model\n",
        "model_path = f\"{gpt_neox_colabDir}/data/shakespeare\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define a simple char-level tokenizer if not provided\n",
        "def char_level_tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def char_level_detokenize(tokens):\n",
        "    return tokenizer.detokenize(tokens)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user for input\n",
        "#input_text = input(\"Enter your prompt: \")\n",
        "input_text = \"Thou art\"\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([char_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = char_level_detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "import time\n",
        "\n",
        "# Play the audio repeatedly\n",
        "while True:\n",
        "    display(Javascript('''\n",
        "        new Audio(\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Coins_dropped_in_metallic_moneybox_0.ogg\").play()\n",
        "    '''))\n",
        "    time.sleep(30)  # Wait for 30 seconds before replaying"
      ],
      "metadata": {
        "id": "UNCboCsapOPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e60SXzNLiinb"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "# Autplay does not work in VSCode\n",
        "IPython.display.Audio(filename=f\"{gpt_neox_colabDir}/notebooks/beep-01a.mp3\", autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoOuzaRkiinb"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import numpy as np\n",
        "fs = 16000.\n",
        "# Autplay does not work in VSCode\n",
        "IPython.display.Audio(np.sin(2*np.pi*440*np.arange(1 * fs)/fs), rate=fs, autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8W3kedId_Bf"
      },
      "outputs": [],
      "source": [
        "# Here we could disconnect from the Colab GPU resource but we will lose all results\n",
        "#from google.colab import runtime\n",
        "#runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = input(\"Enter your prompt: \")\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([char_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = char_level_detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ],
      "metadata": {
        "id": "YmBmP2UE-amo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UPDATE_VENV = False\n",
        "if not USE_VENV and UPDATE_VENV:\n",
        "  %cd {gpt_neox_colabDir}\n",
        "  %pip install dvc[s3]\n",
        "  !dvc add .venv.tar.gz\n",
        "  !git add .venv.tar.gz.dvc .gitignore\n",
        "  !git commit -m \"Add .venv.tar.gz.dvc to DVC\"\n",
        "  !dvc push -q"
      ],
      "metadata": {
        "id": "ASPRn9EMWzBL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}