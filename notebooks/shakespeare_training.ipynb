{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/shakespeare_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lBrppZd_A0"
      },
      "source": [
        "# Training a tiny SLM on a corpus of Shakespeare\n",
        "The intention of this notebook is to demonstrate a setup for experimenting with a tiny SLM.\n",
        "The following tools are used:\n",
        "* Colab (https://colab.research.google.com/) for notebook execution\n",
        "* DagsHub (https://dagshub.com/) for project tracking\n",
        "* MLFlow (https://mlflow.org/) for experiment tracking\n",
        "* Hydra (https://hydra.cc/) for configuration management\n",
        "* GPTNeoX (https://github.com/EleutherAI/gpt-neox) for model training\n",
        "* Tensorboard (https://www.tensorflow.org/tensorboard) for experiment monitoring\n",
        "* DVC (https://dvc.org/) for data management\n",
        "* GitHub (https://github.com/) for code management\n",
        "* Backblaze (https://backblaze.com/) for data storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYyxsLpOuQR1"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "print(\"Current Date and Time:\", datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google import colab\n",
        "  isColab = True\n",
        "  from google.colab import userdata\n",
        "  if userdata.get('GITHUB_NAME'):\n",
        "    !git config --global user.name \"{userdata.get('GITHUB_NAME')}\"\n",
        "  if userdata.get('GITHUB_EMAIL'):\n",
        "    !git config --global user.email \"{userdata.get('GITHUB_EMAIL')}\"\n",
        "  if userdata.get('AWS_SECRET_ACCESS_KEY'):\n",
        "    !echo \"export AWS_SECRET_ACCESS_KEY={userdata.get('AWS_SECRET_ACCESS_KEY')}\" >> ~/.bashrc\n",
        "  if userdata.get('AWS_ACCESS_KEY_ID'):\n",
        "    !echo \"export AWS_ACCESS_KEY_ID={userdata.get('AWS_ACCESS_KEY_ID')}\" >> ~/.bashrc\n",
        "except:\n",
        "  isColab = False\n",
        "print(\"isColab:\", isColab)"
      ],
      "metadata": {
        "id": "sR5zBqZ0Q25t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBH4eN9Id_A7"
      },
      "outputs": [],
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "workspaceDir = \"/content\"\n",
        "if isColab:\n",
        "  gpt_neox_colabDir = f\"{workspaceDir}/GPT-NeoX-Colab\"\n",
        "else:\n",
        "  gpt_neox_colabDir = f\"/workspace\"\n",
        "GPTNeoXDirName = \"gpt-neox\"\n",
        "GPTNeoXDir = f\"{workspaceDir}/{GPTNeoXDirName}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e27VRVq07s"
      },
      "source": [
        "# Cloning Git Repos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j_hUsQxlhnou"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX-Colab\n",
        "%cd {workspaceDir}\n",
        "if isColab:\n",
        "  # Don't use --depth 1 because that does not play nice with git-annex\n",
        "  if userdata.get('GITHUB_PAT'):\n",
        "    !git clone --depth 1 https://{userdata.get('GITHUB_PAT')}@github.com/markNZed/GPT-NeoX-Colab.git\n",
        "    print(\"Cloned with PAT\")\n",
        "  else:\n",
        "    !git clone --depth 1 https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "    print(\"Cloned without PAT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp8wncmZDAew"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import sys\n",
        "%cd {gpt_neox_colabDir}\n",
        "%pip install -q python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv()\n",
        "activate_script = f\"{gpt_neox_colabDir}/.venv/bin/activate\"\n",
        "USE_VENV = True\n",
        "if USE_VENV:\n",
        "  # Disabling pydevd_plugins so we do not get a restart warning\n",
        "  #if \"pydevd_plugins\" in sys.modules:\n",
        "  #  del sys.modules[\"pydevd_plugins\"]\n",
        "  print(\"Downloading venv\")\n",
        "  %pip install -q dvc[s3]\n",
        "  venv_file = \".venv.tar.gz\"\n",
        "  if not os.path.isfile(venv_file):\n",
        "    !dvc pull -q {venv_file}\n",
        "    !tar -xf {venv_file}\n",
        "elif isColab:\n",
        "    !uv sync -q --dev\n",
        "    !uv run pip install -q -e .\n",
        "    !source {activate_script} && pip install -q -r requirements_colab.txt\n",
        "    !source {activate_script} && pip install -q ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8Mic7aoiNxi"
      },
      "outputs": [],
      "source": [
        "#@title Fetch training data\n",
        "%cd {gpt_neox_colabDir}\n",
        "!source {activate_script} && dvc --quiet pull \"data/shakespeare/shakespeare.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It4jtnb2yvkP"
      },
      "outputs": [],
      "source": [
        "#@title Fetch processed training data\n",
        "%cd {gpt_neox_colabDir}\n",
        "!source {activate_script} && dvc --quiet pull \"data/shakespeare/shakespeare.jsonl\"\n",
        "!source {activate_script} && dvc --quiet pull \"data/shakespeare/shakespeare_text_document.bin\"\n",
        "!source {activate_script} && dvc --quiet pull \"data/shakespeare/shakespeare_text_document.idx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FjLEIFCR6d8m"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX\n",
        "%cd {workspaceDir}\n",
        "#!git clone ---depth 1 https://github.com/EleutherAI/gpt-neox\n",
        "!git clone -b pipe_parallel_size_1 --depth 1 https://github.com/markNZed/gpt-neox.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va0pS1dvd_BD"
      },
      "source": [
        "# Python Environment\n",
        "It is faster to download a Python virtual environment and unzip it than to install all the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUmdStRWhrUR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "if not USE_VENV:\n",
        "    # Could not redirect to /dev/null in the standard Colab notebook (maybe no output for a particular time?)\n",
        "    # Currently deepspeed from GTP-NeoX is not compatible with logging in torch >= 2.4\n",
        "    !source {activate_script} && pip install -q torch==2.3 torchaudio==2.3.0 torchvision==0.18.0 transformers==4.38.0 sentence-transformers==2.2.2\n",
        "    !source {activate_script} && pip install -q fsspec==2024.10.0 datasets==2.14.0 evaluate==0.4.3 lm-eval==0.4.1 tensorboard==2.17.0 tensorflow==2.17.0\n",
        "    %cd {GPTNeoXDir}\n",
        "    !source {activate_script} && pip install -q -r ./requirements/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jzX5ohGax6p"
      },
      "source": [
        "# Preparing Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-hmZjCc-WnV"
      },
      "outputs": [],
      "source": [
        "{#@title Converting text data to jsonl format\n",
        "import os\n",
        "\n",
        "%cd {GPTNeoXDir}\n",
        "!mkdir -p data\n",
        "\n",
        "# Check if the converted file exists\n",
        "if not os.path.isfile(f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl\"):\n",
        "    gpt_neox_colab.utils.ml.text2jsonl(f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.txt\", f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl\")\n",
        "\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl {GPTNeoXDir}/data/shakespeare.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x57thNaLa-yN"
      },
      "source": [
        "# Tokenizing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyD8RujkEUsr"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#@title Tokenizing jsonl formatted data\n",
        "import os\n",
        "\n",
        "%cd {GPTNeoXDir}\n",
        "!mkdir -p processed_data\n",
        "\n",
        "# Check if the tokenized files exists\n",
        "a = f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.idx\"\n",
        "b = f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.bin\"\n",
        "if not os.path.isfile(a) or not os.path.isfile(b):\n",
        "    !source {activate_script} && python tools/datasets/preprocess_data.py \\\n",
        "        --input ./data/shakespeare.jsonl \\\n",
        "        --output-prefix ./processed_data \\\n",
        "        --tokenizer-type CharLevelTokenizer \\\n",
        "        --dataset-impl mmap \\\n",
        "        --append-eod\n",
        "    !cp {GPTNeoXDir}/processed_data/shakespeare_text_document.bin {gpt_neox_colabDir}/data/shakespeare\n",
        "    !cp {GPTNeoXDir}/processed_data/shakespeare_text_document.idx {gpt_neox_colabDir}/data/shakespeare\n",
        "\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.bin {GPTNeoXDir}/processed_data\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.idx {GPTNeoXDir}/processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNM2gpADtjM9"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtOHyiVDhrUZ"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "# Start a detached background process using the temp config\n",
        "cmd = f\"\"\"\n",
        "nohup source {activate_script} && \\\n",
        "cd {GPTNeoXDir} && \\\n",
        "python ./deepy.py train.py --conf_dir {gpt_neox_colabDir}/configs shakespeare shakespeare_deepy\n",
        "\"\"\"\n",
        "print(\"Running command:\", cmd)\n",
        "#cmd = \"nohup bash -c ls\" # Used to test without running on GPU\n",
        "\n",
        "# Start the process and retrieve the PID directly\n",
        "process = subprocess.Popen(\n",
        "    cmd,\n",
        "    shell=True,\n",
        "    executable='/bin/bash',\n",
        "    preexec_fn=os.setsid  # Starts the process in a new session\n",
        ")\n",
        "\n",
        "pid = process.pid\n",
        "print(f\"Started training with PID: {pid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVCgdZ1Ma9TI"
      },
      "outputs": [],
      "source": [
        "#@title Wait until tensorboard log directory is created\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Path to the TensorBoard log directory\n",
        "tensorboard_log_dir = f\"{GPTNeoXDir}/tensorboard\"\n",
        "\n",
        "# Wait for the directory to be created\n",
        "while not os.path.exists(tensorboard_log_dir):\n",
        "    print(\"Waiting for TensorBoard log directory to be created...\")\n",
        "    time.sleep(10)  # Check every X seconds\n",
        "\n",
        "print(\"TensorBoard log directory found. You can now launch TensorBoard.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfefGgc-ln-J"
      },
      "outputs": [],
      "source": [
        "# Need to delete everything in checkpoints and tensorboard dir for a fresh run\n",
        "%cd {GPTNeoXDir}\n",
        "%tensorboard --logdir tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j3_fG5vV7AB"
      },
      "outputs": [],
      "source": [
        "#@title Find the latest log file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Define the log directory and pattern for log files\n",
        "log_dir = f\"{GPTNeoXDir}/logs\"\n",
        "log_pattern = os.path.join(log_dir, \"*_stdout.txt\")\n",
        "\n",
        "# Get the list of log files that match the pattern\n",
        "log_files = glob.glob(log_pattern)\n",
        "\n",
        "# Ensure there are log files in the directory\n",
        "if log_files:\n",
        "    # Find the latest log file based on modification time\n",
        "    latest_log = max(log_files, key=os.path.getmtime)\n",
        "    print(\"Latest log file:\", latest_log)\n",
        "else:\n",
        "    latest_log = None\n",
        "    print(\"No log files found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QszVj7_vSP7L"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn13kNSRacnA"
      },
      "outputs": [],
      "source": [
        "#@title Read the latest log file and extract the iteration count\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "\n",
        "# File to store the last read position (persistence between script runs)\n",
        "file_position = 0\n",
        "# Regular expression to match \"iteration <number> / <total>\"\n",
        "iteration_pattern = re.compile(r\"iteration\\s+(\\d+)\\s*/\\s*\\d+\")\n",
        "\n",
        "def read_new_iterations():\n",
        "    global file_position\n",
        "    # Open the log file and seek to the last position\n",
        "    with open(latest_log, \"r\") as file:\n",
        "        file.seek(file_position)\n",
        "        # Read new lines\n",
        "        new_lines = file.readlines()\n",
        "        file_position = file.tell()\n",
        "        # Process lines containing \"iteration\"\n",
        "        last_match = None\n",
        "        for line in new_lines:\n",
        "            match = iteration_pattern.search(line)\n",
        "            if match:\n",
        "                last_match = match\n",
        "        if last_match:\n",
        "            # Extract the iteration count from the regex match\n",
        "            iteration_count = int(last_match.group(1))\n",
        "            print(f\"{iteration_count} iterations\")\n",
        "\n",
        "# Periodically check if the process has completed\n",
        "while True:\n",
        "    # Poll the process to see if it has terminated\n",
        "    if process.poll() is not None:\n",
        "        # Process has completed\n",
        "        print(\"Training has completed.\")\n",
        "        break\n",
        "    else:\n",
        "        if latest_log:\n",
        "            read_new_iterations()\n",
        "        elif os.path.exists(f\"{experimentDir}/logs\"):\n",
        "            latest_log = get_latest_file(f\"{experimentDir}/logs\", \"*_stdout.txt\")\n",
        "        print(\"Training is still running...\")\n",
        "        time.sleep(30)  # Check every X seconds\n",
        "\n",
        "print(\"Training has finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJb7TUqi4vVg"
      },
      "outputs": [],
      "source": [
        "#@title Display training and validation Loss\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "import os\n",
        "import numpy as np\n",
        "# Path to the latest log file\n",
        "log_dir = \"tensorboard\"\n",
        "log_files = [os.path.join(log_dir, d) for d in os.listdir(log_dir)]\n",
        "latest_log_dir = max(log_files, key=os.path.getmtime)\n",
        "\n",
        "# Initialize EventAccumulator to load scalar data\n",
        "ea = event_accumulator.EventAccumulator(latest_log_dir)\n",
        "ea.Reload()  # Load all logs\n",
        "\n",
        "# List all scalar keys available in the logs\n",
        "scalar_keys = ea.Tags()['scalars']\n",
        "print(\"Available scalar keys:\", scalar_keys)\n",
        "\n",
        "# Extract training and validation losses\n",
        "train_loss = ea.Scalars('train/lm_loss')  # Adjust for actual name if necessary\n",
        "val_loss = ea.Scalars('validation/lm_loss')  # Adjust for actual name if necessary\n",
        "\n",
        "# Convert to lists for plotting\n",
        "train_loss_values = [x.value for x in train_loss]\n",
        "val_loss_values = [x.value for x in val_loss]\n",
        "\n",
        "# Find the lengths of both arrays\n",
        "len_train = len(train_loss_values)\n",
        "len_val = len(val_loss_values)\n",
        "\n",
        "iterations = None\n",
        "# Interpolate the shorter array\n",
        "if len_train != len_val:\n",
        "    if len_train > len_val:\n",
        "        # Interpolate validation loss to match the training loss length\n",
        "        iterations = np.linspace(1, len_train, len_train)\n",
        "        val_iterations = np.linspace(1, len_train, len_val)\n",
        "        val_loss_values = np.interp(iterations, val_iterations, val_loss_values)\n",
        "    else:\n",
        "        # Interpolate training loss to match the validation loss length\n",
        "        iterations = np.linspace(1, len_val, len_val)\n",
        "        train_iterations = np.linspace(1, len_val, len_train)\n",
        "        train_loss_values = np.interp(iterations, train_iterations, train_loss_values)\n",
        "else:\n",
        "    iterations = range(1, len_train + 1)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(iterations, train_loss_values, label='Training Loss')\n",
        "plt.plot(iterations, val_loss_values, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk8DhmmEZFyz"
      },
      "source": [
        "# Inference with GPT-NeoX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKb0Ar6NZFyz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%cd {GPTNeoXDir}\n",
        "# This has issues if used during training -  The server socket has failed to bind to [::]:29500 (errno: 98 - Address already\n",
        "# This will write over the logs\n",
        "!source {activate_script} && python ./deepy.py generate.py -d configs {gpt_neox_colabDir}/configs/shakespeare {gpt_neox_colabDir}/configs/shakespeare_gen\n",
        "!cat sample_output.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajLR98lDi9_8"
      },
      "outputs": [],
      "source": [
        "# 2.21.0 was the last 2 series but it asks for trust_remote_code\n",
        "!source {activate_script} &&  pip install datasets==2.14.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S-foNPFjfwH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# This has issues if used during training -  The server socket has failed to bind to [::]:29500 (errno: 98 - Address already\n",
        "# This will write over the logs\n",
        "# python ./deepy.py eval.py -d configs your_configs.yml --eval_tasks task1 task2 ... taskn\n",
        "# NOTE this will prompt for permission to run a download script - would need an older datasetse library to avoid this\n",
        "%cd {GPTNeoXDir}\n",
        "!source {activate_script} && python ./deepy.py eval.py -d configs {gpt_neox_colabDir}/configs/shakespeare {gpt_neox_colabDir}/configs/shakespeare_gen --eval_tasks hellaswag\n",
        "!cat sample_output.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7AOL1NJ43e"
      },
      "source": [
        "# Inference with Hugging Face\n",
        "\n",
        "## Convert model to HF format\n",
        "Here we are converting our model to `HuggingFace Format`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNdWrrkWfyT-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the checkpoints directory\n",
        "checkpoints_dir = f\"{GPTNeoXDir}/checkpoints\"\n",
        "\n",
        "# Read the 'latest' file to get the latest checkpoint name\n",
        "with open(os.path.join(checkpoints_dir, \"latest\"), \"r\") as f:\n",
        "    latest_checkpoint_name = f.read().strip()\n",
        "\n",
        "# Construct the full path to the latest checkpoint directory\n",
        "latest_checkpoint_path = os.path.join(checkpoints_dir, latest_checkpoint_name)\n",
        "print(\"Path to the latest checkpoint:\", latest_checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rplkNclPXgkH"
      },
      "outputs": [],
      "source": [
        "#@title Convert last checkpoint to huggingface model\n",
        "%cd {GPTNeoXDir}\n",
        "!source {activate_script} && python ./tools/ckpts/convert_neox_to_hf.py --input_dir {latest_checkpoint_path} --config_file {gpt_neox_colabDir}/configs/shakespeare.yml --output_dir {gpt_neox_colabDir}/data/shakespeare --architecture neox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qra1qQtC2oiI"
      },
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01ZRN2IceM8a"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, f\"{gpt_neox_colabDir}/my_env/lib/python3.10/site-packages\")\n",
        "\n",
        "from transformers import GPTNeoXForCausalLM\n",
        "import torch\n",
        "\n",
        "# Move to model directory\n",
        "%cd {gpt_neox_colabDir}\n",
        "\n",
        "# Assuming CharLevelTokenizer is properly imported and instantiated\n",
        "from gpt_neox_colab import CharLevelTokenizer\n",
        "tokenizer = CharLevelTokenizer.CharLevelTokenizer(vocab_size=512)\n",
        "\n",
        "# Load your model\n",
        "model_path = f\"{gpt_neox_colabDir}/data/shakespeare\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define a simple char-level tokenizer if not provided\n",
        "def char_level_tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def char_level_detokenize(tokens):\n",
        "    return tokenizer.detokenize(tokens)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user for input\n",
        "#input_text = input(\"Enter your prompt: \")\n",
        "input_text = \"Thou art\"\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([char_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = char_level_detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abEMTFL3hrUc"
      },
      "outputs": [],
      "source": [
        "if isColab:\n",
        "  import time\n",
        "  while True:\n",
        "    output.eval_js(\"new Audio(\\\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Coins_dropped_in_metallic_moneybox_0.ogg\\\").play()\")\n",
        "    time.sleep(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e60SXzNLiinb"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "# Autplay does not work in VSCode\n",
        "IPython.display.Audio(filename=f\"{gpt_neox_colabDir}/notebooks/beep-01a.mp3\", autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoOuzaRkiinb"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import numpy as np\n",
        "fs = 16000.\n",
        "# Autplay does not work in VSCode\n",
        "IPython.display.Audio(np.sin(2*np.pi*440*np.arange(1 * fs)/fs), rate=fs, autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8W3kedId_Bf"
      },
      "outputs": [],
      "source": [
        "# Here we could disconnect from the Colab GPU resource but we will lose all results\n",
        "#from google.colab import runtime\n",
        "#runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = input(\"Enter your prompt: \")\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([char_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = char_level_detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ],
      "metadata": {
        "id": "YmBmP2UE-amo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not USE_VENV and UPDATE_VENV:\n",
        "  %cd {gpt_neox_colabDir}\n",
        "  %pip install dvc[s3]\n",
        "  !dvc add -q .venv.tar.gz\n",
        "  !git add .venv.tar.gz.dvc .gitignore\n",
        "  !git commit -m \"Add .venv.tar.gz.dvc to DVC\"\n",
        "  !dvc push -q"
      ],
      "metadata": {
        "id": "ASPRn9EMWzBL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}