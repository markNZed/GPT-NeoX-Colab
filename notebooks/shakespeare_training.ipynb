{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/shakespeare_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lBrppZd_A0"
      },
      "source": [
        "# Training a tiny SLM on a corpus of Shakespeare\n",
        "The intention of this notebook is to demonstrate a setup for experimenting with a tiny SLM.\n",
        "The following tools are used:\n",
        "* Colab (https://colab.research.google.com/) for notebook execution\n",
        "* DagsHub (https://dagshub.com/) for project tracking\n",
        "* MLFlow (https://mlflow.org/) for experiment tracking\n",
        "* Hydra (https://hydra.cc/) for configuration management\n",
        "* GPTNeoX (https://github.com/EleutherAI/gpt-neox) for model training\n",
        "* Tensorboard (https://www.tensorflow.org/tensorboard) for experiment monitoring\n",
        "* DVC (https://dvc.org/) for data management\n",
        "* GitHub (https://github.com/) for code management\n",
        "* Backblaze (https://backblaze.com/) for data storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYyxsLpOuQR1"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "print(\"Current Date and Time:\", datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR5zBqZ0Q25t"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google import colab\n",
        "  isColab = True\n",
        "except:\n",
        "  isColab = False\n",
        "\n",
        "try:\n",
        "  from google.colab import userdata\n",
        "  if userdata.get('GITHUB_NAME'):\n",
        "    !git config --global user.name \"{userdata.get('GITHUB_NAME')}\"\n",
        "  if userdata.get('GITHUB_EMAIL'):\n",
        "    !git config --global user.email \"{userdata.get('GITHUB_EMAIL')}\"\n",
        "  if userdata.get('AWS_SECRET_ACCESS_KEY'):\n",
        "    !echo \"export AWS_SECRET_ACCESS_KEY={userdata.get('AWS_SECRET_ACCESS_KEY')}\" >> ~/.bashrc\n",
        "    os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "  if userdata.get('AWS_ACCESS_KEY_ID'):\n",
        "    !echo \"export AWS_ACCESS_KEY_ID={userdata.get('AWS_ACCESS_KEY_ID')}\" >> ~/.bashrc\n",
        "    os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "print(\"isColab:\", isColab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBH4eN9Id_A7"
      },
      "outputs": [],
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "workspaceDir = \"/content\"\n",
        "gpt_neox_colabDir = f\"{workspaceDir}/GPT-NeoX-Colab\"\n",
        "if not isColab:\n",
        "  !sudo ln -s /workspace /content/GPT-NeoX-Colab\n",
        "GPTNeoXDirName = \"gpt-neox\"\n",
        "GPTNeoXDir = f\"{workspaceDir}/{GPTNeoXDirName}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e27VRVq07s"
      },
      "source": [
        "# Cloning Git Repos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j_hUsQxlhnou"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX-Colab\n",
        "%cd {workspaceDir}\n",
        "if isColab:\n",
        "  # Don't use --depth 1 because that does not play nice with git-annex\n",
        "  if userdata.get('GITHUB_PAT'):\n",
        "    !git clone --depth 1 https://{userdata.get('GITHUB_PAT')}@github.com/markNZed/GPT-NeoX-Colab.git\n",
        "    print(\"Cloned with PAT\")\n",
        "  else:\n",
        "    !git clone --depth 1 https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "    print(\"Cloned without PAT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp8wncmZDAew"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import sys\n",
        "%cd {gpt_neox_colabDir}\n",
        "print(\"Install DVC\")\n",
        "%pip install -q python-dotenv dvc[s3]==3.2.0 s3fs==2024.2.0 fsspec==2024.2.0 gcsfs==2024.2.0\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv()\n",
        "activate_script = f\"{gpt_neox_colabDir}/.venv/bin/activate\"\n",
        "USE_VENV = False\n",
        "if USE_VENV:\n",
        "  # Disabling pydevd_plugins so we do not get a restart warning\n",
        "  #if \"pydevd_plugins\" in sys.modules:\n",
        "  #  del sys.modules[\"pydevd_plugins\"]\n",
        "  venv_dir = \".venv\"\n",
        "  venv_tar_file = f\"{venv_dir}.tar\"\n",
        "  venv_gz_file = f\"{venv_tar_file}.gz\"\n",
        "  if not os.path.isdir(venv_dir):\n",
        "    if not os.path.isfile(venv_gz_file):\n",
        "      print(f\"Downloading {venv_gz_file}\")\n",
        "      !dvc pull -q {venv_gz_file}\n",
        "    if not os.path.isfile(venv_tar_file):\n",
        "      print(f\"Unzipping {venv_gz_file}\")\n",
        "      !sudo apt-get install -y pigz\n",
        "      !pigz -d -p 8 {venv_gz_file}\n",
        "    if not os.path.isfile(venv_dir):\n",
        "      print(f\"Untarring {venv_tar_file}\")\n",
        "      !tar -xf {venv_tar_file}\n",
        "      !rm {venv_tar_file}\n",
        "elif isColab:\n",
        "    !uv sync -q --dev\n",
        "    !uv run pip install -q -e .\n",
        "    !source {activate_script} && pip install -q -r requirements_colab.txt\n",
        "    !source {activate_script} && pip install -q ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8Mic7aoiNxi",
        "outputId": "98a5689b-1505-4781-88fd-6d11e9abcb6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#@title Fetch training data\n",
        "%cd {gpt_neox_colabDir}\n",
        "!dvc --quiet pull -q \"data/shakespeare\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FjLEIFCR6d8m",
        "outputId": "698173a2-b4e3-45d3-dfb4-4dd0f16ff5d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'gpt-neox' already exists and is not an empty directory.\n",
            "CPU times: user 7.02 ms, sys: 0 ns, total: 7.02 ms\n",
            "Wall time: 106 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX\n",
        "%cd {workspaceDir}\n",
        "#!git clone ---depth 1 https://github.com/EleutherAI/gpt-neox\n",
        "!git clone -b pipe_parallel_size_1 --depth 1 https://github.com/markNZed/gpt-neox.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va0pS1dvd_BD"
      },
      "source": [
        "# Python Environment\n",
        "It is faster to download a Python virtual environment and unzip it than to install all the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "id": "AUmdStRWhrUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee40b0a2-70b1-4687-ebde-aa8719d58dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "if not USE_VENV:\n",
        "    # Could not redirect to /dev/null in the standard Colab notebook (maybe no output for a particular time?)\n",
        "    # Currently deepspeed from GTP-NeoX is not compatible with logging in torch >= 2.4\n",
        "    !source {activate_script} && pip install -q torch==2.3 torchaudio==2.3.0 torchvision==0.18.0 transformers==4.38.0 sentence-transformers==2.2.2\n",
        "    !source {activate_script} && pip install -q fsspec==2024.2.0 datasets==2.14.0 evaluate==0.4.3 lm-eval==0.4.1 tensorboard==2.17.1 tensorflow==2.17.1\n",
        "    %cd {GPTNeoXDir}\n",
        "    !source {activate_script} && pip install -q -r ./requirements/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jzX5ohGax6p"
      },
      "source": [
        "# Preparing Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-hmZjCc-WnV",
        "outputId": "852383d9-ead1-4326-cd89-05884211a2ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n"
          ]
        }
      ],
      "source": [
        "#@title Converting text data to jsonl format\n",
        "import os\n",
        "\n",
        "%cd {GPTNeoXDir}\n",
        "!mkdir -p data\n",
        "\n",
        "# Check if the converted file exists\n",
        "if not os.path.isfile(f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl\"):\n",
        "    !source {activate_script} && python -c \"import gpt_neox_colab.utils; gpt_neox_colab.utils.ml.text2jsonl(\\\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.txt\\\", \\\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl\\\")\"\n",
        "\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl {GPTNeoXDir}/data/shakespeare.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x57thNaLa-yN"
      },
      "source": [
        "# Tokenizing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyD8RujkEUsr",
        "outputId": "f6fb1ce3-7929-4606-a06d-3b4268dcd13f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n",
            "/content/gpt-neox/processed_data\n",
            "CPU times: user 15.2 ms, sys: 1.2 ms, total: 16.4 ms\n",
            "Wall time: 312 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Tokenizing jsonl formatted data\n",
        "import os\n",
        "\n",
        "%cd {GPTNeoXDir}\n",
        "!mkdir -p processed_data\n",
        "%cd processed_data\n",
        "\n",
        "# Check if the tokenized files exists\n",
        "a = f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.idx\"\n",
        "b = f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.bin\"\n",
        "if not os.path.isfile(a) or not os.path.isfile(b):\n",
        "    cmd = f\"\"\"\n",
        "    source {activate_script} && python {GPTNeoXDir}/tools/datasets/preprocess_data.py \\\n",
        "        --input {GPTNeoXDir}/data/shakespeare.jsonl \\\n",
        "        --output-prefix shakespeare \\\n",
        "        --tokenizer-type CharLevelTokenizer \\\n",
        "        --dataset-impl mmap \\\n",
        "        --append-eod\n",
        "    \"\"\"\n",
        "    print(f\"Command: {cmd}\")\n",
        "    !source {activate_script} && python {GPTNeoXDir}/tools/datasets/preprocess_data.py \\\n",
        "        --input {GPTNeoXDir}/data/shakespeare.jsonl \\\n",
        "        --output-prefix shakespeare \\\n",
        "        --tokenizer-type CharLevelTokenizer \\\n",
        "        --dataset-impl mmap \\\n",
        "        --append-eod\n",
        "    !cp {GPTNeoXDir}/processed_data/shakespeare_text_document.bin {gpt_neox_colabDir}/data/shakespeare\n",
        "    !cp {GPTNeoXDir}/processed_data/shakespeare_text_document.idx {gpt_neox_colabDir}/data/shakespeare\n",
        "\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.bin {GPTNeoXDir}/processed_data\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.idx {GPTNeoXDir}/processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNM2gpADtjM9",
        "outputId": "9b0a4e4d-cfed-49da-b9a2-c46e3803e65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtOHyiVDhrUZ",
        "outputId": "175b7734-d0dd-41a7-db0c-8ff133eb7b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running command: \n",
            "nohup nohup bash -c \" source /content/GPT-NeoX-Colab/.venv/bin/activate && cd /content/gpt-neox && python ./deepy.py train.py --conf_dir /content/GPT-NeoX-Colab/configs shakespeare shakespeare_deepy \"\n",
            "\n",
            "Started training with PID: 4824\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "# Start a detached background process using the temp config\n",
        "# If we want to run from scratch again: rm -rf /content/gpt-neox/logs/* /content/gpt-neox/tensorboard/* /content/gpt-neox/checkpoints/*\n",
        "cmd = f\"\"\"\n",
        "nohup nohup bash -c \" source {activate_script} && \\\n",
        "cd {GPTNeoXDir} && \\\n",
        "python ./deepy.py train.py --conf_dir {gpt_neox_colabDir}/configs shakespeare shakespeare_deepy \"\n",
        "\"\"\"\n",
        "print(\"Running command:\", cmd)\n",
        "#cmd = \"nohup bash -c ls\" # Used to test without running on GPU\n",
        "\n",
        "# Redirect stdout and stderr to os.devnull\n",
        "with open(os.devnull, 'w') as devnull:\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        shell=True,\n",
        "        executable='/bin/bash',\n",
        "        preexec_fn=os.setsid,  # Starts the process in a new session\n",
        "        stdout=devnull,  # Suppress stdout\n",
        "        stderr=devnull   # Suppress stderr\n",
        "    )\n",
        "\n",
        "pid = process.pid\n",
        "print(f\"Started training with PID: {pid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WVCgdZ1Ma9TI",
        "outputId": "95a37c95-3d75-4377-e956-5cf731fe700c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8c33756298d6>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorboard_log_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Waiting for TensorBoard log directory to be created...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Check every X seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TensorBoard log directory found. You can now launch TensorBoard.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Wait until tensorboard log directory is created\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Path to the TensorBoard log directory\n",
        "tensorboard_log_dir = f\"{GPTNeoXDir}/tensorboard\"\n",
        "\n",
        "# Wait for the directory to be created\n",
        "while not os.path.exists(tensorboard_log_dir):\n",
        "    print(\"Waiting for TensorBoard log directory to be created...\")\n",
        "    time.sleep(10)  # Check every X seconds\n",
        "\n",
        "print(\"TensorBoard log directory found. You can now launch TensorBoard.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfefGgc-ln-J"
      },
      "outputs": [],
      "source": [
        "# Need to delete everything in checkpoints and tensorboard dir for a fresh run\n",
        "%cd {GPTNeoXDir}\n",
        "%tensorboard --logdir tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j3_fG5vV7AB"
      },
      "outputs": [],
      "source": [
        "#@title Find the latest log file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Define the log directory and pattern for log files\n",
        "experimentDir = f\"{GPTNeoXDir}/logs\"\n",
        "log_pattern = os.path.join(experimentDir, \"*_stdout.txt\")\n",
        "\n",
        "# Get the list of log files that match the pattern\n",
        "log_files = glob.glob(log_pattern)\n",
        "\n",
        "# Ensure there are log files in the directory\n",
        "if log_files:\n",
        "    # Find the latest log file based on modification time\n",
        "    latest_log = max(log_files, key=os.path.getmtime)\n",
        "    print(\"Latest log file:\", latest_log)\n",
        "else:\n",
        "    latest_log = None\n",
        "    print(\"No log files found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QszVj7_vSP7L"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn13kNSRacnA"
      },
      "outputs": [],
      "source": [
        "#@title Read the latest log file and extract the iteration count\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "\n",
        "# File to store the last read position (persistence between script runs)\n",
        "file_position = 0\n",
        "# Regular expression to match \"iteration <number> / <total>\"\n",
        "iteration_pattern = re.compile(r\"iteration\\s+(\\d+)\\s*/\\s*\\d+\")\n",
        "\n",
        "def read_new_iterations():\n",
        "    global file_position\n",
        "    # Open the log file and seek to the last position\n",
        "    with open(latest_log, \"r\") as file:\n",
        "        file.seek(file_position)\n",
        "        # Read new lines\n",
        "        new_lines = file.readlines()\n",
        "        file_position = file.tell()\n",
        "        # Process lines containing \"iteration\"\n",
        "        last_match = None\n",
        "        for line in new_lines:\n",
        "            match = iteration_pattern.search(line)\n",
        "            if match:\n",
        "                last_match = match\n",
        "        if last_match:\n",
        "            # Extract the iteration count from the regex match\n",
        "            iteration_count = int(last_match.group(1))\n",
        "            print(f\"{iteration_count} iterations\")\n",
        "\n",
        "# Periodically check if the process has completed\n",
        "while True:\n",
        "    # Poll the process to see if it has terminated\n",
        "    if process.poll() is not None:\n",
        "        # Process has completed\n",
        "        print(\"Training has completed.\")\n",
        "        break\n",
        "    else:\n",
        "        if latest_log:\n",
        "            read_new_iterations()\n",
        "        elif os.path.exists(f\"{experimentDir}/logs\"):\n",
        "            latest_log = get_latest_file(f\"{experimentDir}/logs\", \"*_stdout.txt\")\n",
        "        print(\"Training is still running...\")\n",
        "        time.sleep(30)  # Check every X seconds\n",
        "\n",
        "print(\"Training has finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJb7TUqi4vVg"
      },
      "outputs": [],
      "source": [
        "#@title Display training and validation Loss\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "import os\n",
        "import numpy as np\n",
        "# Path to the latest log file\n",
        "log_dir = \"tensorboard\"\n",
        "log_files = [os.path.join(log_dir, d) for d in os.listdir(log_dir)]\n",
        "latest_log_dir = max(log_files, key=os.path.getmtime)\n",
        "\n",
        "# Initialize EventAccumulator to load scalar data\n",
        "ea = event_accumulator.EventAccumulator(latest_log_dir)\n",
        "ea.Reload()  # Load all logs\n",
        "\n",
        "# List all scalar keys available in the logs\n",
        "scalar_keys = ea.Tags()['scalars']\n",
        "print(\"Available scalar keys:\", scalar_keys)\n",
        "\n",
        "# Extract training and validation losses\n",
        "train_loss = ea.Scalars('train/lm_loss')  # Adjust for actual name if necessary\n",
        "val_loss = ea.Scalars('validation/lm_loss')  # Adjust for actual name if necessary\n",
        "\n",
        "# Convert to lists for plotting\n",
        "train_loss_values = [x.value for x in train_loss]\n",
        "val_loss_values = [x.value for x in val_loss]\n",
        "\n",
        "# Find the lengths of both arrays\n",
        "len_train = len(train_loss_values)\n",
        "len_val = len(val_loss_values)\n",
        "\n",
        "iterations = None\n",
        "# Interpolate the shorter array\n",
        "if len_train != len_val:\n",
        "    if len_train > len_val:\n",
        "        # Interpolate validation loss to match the training loss length\n",
        "        iterations = np.linspace(1, len_train, len_train)\n",
        "        val_iterations = np.linspace(1, len_train, len_val)\n",
        "        val_loss_values = np.interp(iterations, val_iterations, val_loss_values)\n",
        "    else:\n",
        "        # Interpolate training loss to match the validation loss length\n",
        "        iterations = np.linspace(1, len_val, len_val)\n",
        "        train_iterations = np.linspace(1, len_val, len_train)\n",
        "        train_loss_values = np.interp(iterations, train_iterations, train_loss_values)\n",
        "else:\n",
        "    iterations = range(1, len_train + 1)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(iterations, train_loss_values, label='Training Loss')\n",
        "plt.plot(iterations, val_loss_values, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk8DhmmEZFyz"
      },
      "source": [
        "# Inference with GPT-NeoX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKb0Ar6NZFyz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%cd {GPTNeoXDir}\n",
        "# This has issues if used during training -  The server socket has failed to bind to [::]:29500 (errno: 98 - Address already\n",
        "# This will write over the logs\n",
        "!source {activate_script} && python ./deepy.py generate.py -d configs {gpt_neox_colabDir}/configs/shakespeare {gpt_neox_colabDir}/configs/shakespeare_gen\n",
        "!cat sample_output.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajLR98lDi9_8"
      },
      "outputs": [],
      "source": [
        "# 2.21.0 was the last 2 series but it asks for trust_remote_code\n",
        "!source {activate_script} &&  pip install datasets==2.14.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S-foNPFjfwH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# This has issues if used during training -  The server socket has failed to bind to [::]:29500 (errno: 98 - Address already\n",
        "# This will write over the logs\n",
        "# python ./deepy.py eval.py -d configs your_configs.yml --eval_tasks task1 task2 ... taskn\n",
        "# NOTE this will prompt for permission to run a download script - would need an older datasetse library to avoid this\n",
        "%cd {GPTNeoXDir}\n",
        "!source {activate_script} && python ./deepy.py eval.py -d configs {gpt_neox_colabDir}/configs/shakespeare {gpt_neox_colabDir}/configs/shakespeare_gen --eval_tasks hellaswag\n",
        "!cat sample_output.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7AOL1NJ43e"
      },
      "source": [
        "# Inference with Hugging Face\n",
        "\n",
        "## Convert model to HF format\n",
        "Here we are converting our model to `HuggingFace Format`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNdWrrkWfyT-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the checkpoints directory\n",
        "checkpoints_dir = f\"{GPTNeoXDir}/checkpoints\"\n",
        "\n",
        "# Read the 'latest' file to get the latest checkpoint name\n",
        "with open(os.path.join(checkpoints_dir, \"latest\"), \"r\") as f:\n",
        "    latest_checkpoint_name = f.read().strip()\n",
        "\n",
        "# Construct the full path to the latest checkpoint directory\n",
        "latest_checkpoint_path = os.path.join(checkpoints_dir, latest_checkpoint_name)\n",
        "print(\"Path to the latest checkpoint:\", latest_checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rplkNclPXgkH"
      },
      "outputs": [],
      "source": [
        "#@title Convert last checkpoint to huggingface model\n",
        "%cd {GPTNeoXDir}\n",
        "!source {activate_script} && python ./tools/ckpts/convert_neox_to_hf.py --input_dir {latest_checkpoint_path} --config_file {gpt_neox_colabDir}/configs/shakespeare.yml --output_dir {gpt_neox_colabDir}/data/shakespeare --architecture neox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qra1qQtC2oiI"
      },
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01ZRN2IceM8a"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, f\"{gpt_neox_colabDir}/my_env/lib/python3.10/site-packages\")\n",
        "\n",
        "from transformers import GPTNeoXForCausalLM\n",
        "import torch\n",
        "\n",
        "# Move to model directory\n",
        "%cd {gpt_neox_colabDir}\n",
        "\n",
        "# Assuming CharLevelTokenizer is properly imported and instantiated\n",
        "from src.gpt_neox_colab.CharLevelTokenizer import CharLevelTokenizer\n",
        "tokenizer = CharLevelTokenizer(vocab_size=512)\n",
        "\n",
        "# Load your model\n",
        "model_path = f\"{gpt_neox_colabDir}/data/shakespeare\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define a simple char-level tokenizer if not provided\n",
        "def char_level_tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def char_level_detokenize(tokens):\n",
        "    return tokenizer.detokenize(tokens)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user for input\n",
        "#input_text = input(\"Enter your prompt: \")\n",
        "input_text = \"Thou art\"\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([char_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = char_level_detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNCboCsapOPa"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "import time\n",
        "\n",
        "# Play the audio repeatedly\n",
        "while True:\n",
        "    display(Javascript('''\n",
        "        new Audio(\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Coins_dropped_in_metallic_moneybox_0.ogg\").play()\n",
        "    '''))\n",
        "    time.sleep(30)  # Wait for 30 seconds before replaying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e60SXzNLiinb"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "# Autplay does not work in VSCode\n",
        "IPython.display.Audio(filename=f\"{gpt_neox_colabDir}/notebooks/beep-01a.mp3\", autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoOuzaRkiinb"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import numpy as np\n",
        "fs = 16000.\n",
        "# Autplay does not work in VSCode\n",
        "IPython.display.Audio(np.sin(2*np.pi*440*np.arange(1 * fs)/fs), rate=fs, autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8W3kedId_Bf"
      },
      "outputs": [],
      "source": [
        "# Here we could disconnect from the Colab GPU resource but we will lose all results\n",
        "#from google.colab import runtime\n",
        "#runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmBmP2UE-amo"
      },
      "outputs": [],
      "source": [
        "input_text = input(\"Enter your prompt: \")\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([char_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = char_level_detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ASPRn9EMWzBL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "213c1367-bbdb-44ca-ac27-66da8a134b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n",
            "Uploading .venv.tar.gz to DVC\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph\n",
            "Computing md5 for a large file '/content/GPT-NeoX-Colab/.venv.tar.gz'. This is only done once.\n",
            "Adding...:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\n",
            "!\u001b[A\n",
            "  0%|          |/content/GPT-NeoX-Colab/.venv.tar.g0.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "  2% 58.0M/3.54G [00:00<00:06, 599MB/s{'info': ''}]                             \u001b[A\n",
            "  3% 116M/3.54G [00:00<00:06, 604MB/s{'info': ''}] \u001b[A\n",
            "  5% 174M/3.54G [00:00<00:05, 605MB/s{'info': ''}]\u001b[A\n",
            "  6% 232M/3.54G [00:00<00:05, 604MB/s{'info': ''}]\u001b[A\n",
            "  8% 290M/3.54G [00:00<00:05, 603MB/s{'info': ''}]\u001b[A\n",
            " 10% 348M/3.54G [00:00<00:05, 603MB/s{'info': ''}]\u001b[A\n",
            " 11% 406M/3.54G [00:00<00:05, 604MB/s{'info': ''}]\u001b[A\n",
            " 13% 464M/3.54G [00:00<00:05, 604MB/s{'info': ''}]\u001b[A\n",
            " 14% 522M/3.54G [00:00<00:05, 605MB/s{'info': ''}]\u001b[A\n",
            " 16% 580M/3.54G [00:01<00:05, 605MB/s{'info': ''}]\u001b[A\n",
            " 18% 638M/3.54G [00:01<00:05, 604MB/s{'info': ''}]\u001b[A\n",
            " 19% 696M/3.54G [00:01<00:05, 605MB/s{'info': ''}]\u001b[A\n",
            " 21% 755M/3.54G [00:01<00:04, 607MB/s{'info': ''}]\u001b[A\n",
            " 22% 814M/3.54G [00:01<00:04, 610MB/s{'info': ''}]\u001b[A\n",
            " 24% 873M/3.54G [00:01<00:04, 609MB/s{'info': ''}]\u001b[A\n",
            " 26% 931M/3.54G [00:01<00:04, 609MB/s{'info': ''}]\u001b[A\n",
            " 27% 989M/3.54G [00:01<00:04, 607MB/s{'info': ''}]\u001b[A\n",
            " 29% 1.02G/3.54G [00:01<00:04, 607MB/s{'info': ''}]\u001b[A\n",
            " 31% 1.08G/3.54G [00:01<00:04, 608MB/s{'info': ''}]\u001b[A\n",
            " 32% 1.14G/3.54G [00:02<00:04, 608MB/s{'info': ''}]\u001b[A\n",
            " 34% 1.19G/3.54G [00:02<00:04, 608MB/s{'info': ''}]\u001b[A\n",
            " 35% 1.25G/3.54G [00:02<00:04, 608MB/s{'info': ''}]\u001b[A\n",
            " 37% 1.31G/3.54G [00:02<00:03, 609MB/s{'info': ''}]\u001b[A\n",
            " 39% 1.37G/3.54G [00:02<00:03, 609MB/s{'info': ''}]\u001b[A\n",
            " 40% 1.42G/3.54G [00:02<00:03, 610MB/s{'info': ''}]\u001b[A\n",
            " 42% 1.48G/3.54G [00:02<00:03, 613MB/s{'info': ''}]\u001b[A\n",
            " 44% 1.54G/3.54G [00:02<00:03, 613MB/s{'info': ''}]\u001b[A\n",
            " 45% 1.60G/3.54G [00:02<00:03, 613MB/s{'info': ''}]\u001b[A\n",
            " 47% 1.66G/3.54G [00:02<00:03, 613MB/s{'info': ''}]\u001b[A\n",
            " 48% 1.71G/3.54G [00:03<00:03, 612MB/s{'info': ''}]\u001b[A\n",
            " 50% 1.77G/3.54G [00:03<00:03, 608MB/s{'info': ''}]\u001b[A\n",
            " 52% 1.83G/3.54G [00:03<00:03, 608MB/s{'info': ''}]\u001b[A\n",
            " 53% 1.88G/3.54G [00:03<00:02, 611MB/s{'info': ''}]\u001b[A\n",
            " 55% 1.94G/3.54G [00:03<00:02, 612MB/s{'info': ''}]\u001b[A\n",
            " 57% 2.00G/3.54G [00:03<00:02, 611MB/s{'info': ''}]\u001b[A\n",
            " 58% 2.06G/3.54G [00:03<00:02, 611MB/s{'info': ''}]\u001b[A\n",
            " 60% 2.11G/3.54G [00:03<00:02, 612MB/s{'info': ''}]\u001b[A\n",
            " 61% 2.17G/3.54G [00:03<00:02, 614MB/s{'info': ''}]\u001b[A\n",
            " 63% 2.23G/3.54G [00:03<00:02, 612MB/s{'info': ''}]\u001b[A\n",
            " 65% 2.29G/3.54G [00:04<00:02, 611MB/s{'info': ''}]\u001b[A\n",
            " 66% 2.34G/3.54G [00:04<00:02, 610MB/s{'info': ''}]\u001b[A\n",
            " 68% 2.40G/3.54G [00:04<00:01, 610MB/s{'info': ''}]\u001b[A\n",
            " 70% 2.46G/3.54G [00:04<00:01, 609MB/s{'info': ''}]\u001b[A\n",
            " 71% 2.51G/3.54G [00:04<00:01, 610MB/s{'info': ''}]\u001b[A\n",
            " 73% 2.57G/3.54G [00:04<00:01, 611MB/s{'info': ''}]\u001b[A\n",
            " 74% 2.63G/3.54G [00:04<00:01, 610MB/s{'info': ''}]\u001b[A\n",
            " 76% 2.69G/3.54G [00:04<00:01, 610MB/s{'info': ''}]\u001b[A\n",
            " 78% 2.74G/3.54G [00:04<00:01, 611MB/s{'info': ''}]\u001b[A\n",
            " 79% 2.80G/3.54G [00:04<00:01, 612MB/s{'info': ''}]\u001b[A\n",
            " 81% 2.86G/3.54G [00:05<00:01, 614MB/s{'info': ''}]\u001b[A\n",
            " 83% 2.92G/3.54G [00:05<00:01, 615MB/s{'info': ''}]\u001b[A\n",
            " 84% 2.98G/3.54G [00:05<00:00, 616MB/s{'info': ''}]\u001b[A\n",
            " 86% 3.04G/3.54G [00:05<00:00, 618MB/s{'info': ''}]\u001b[A\n",
            " 88% 3.09G/3.54G [00:05<00:00, 620MB/s{'info': ''}]\u001b[A\n",
            " 89% 3.15G/3.54G [00:05<00:00, 620MB/s{'info': ''}]\u001b[A\n",
            " 91% 3.21G/3.54G [00:05<00:00, 620MB/s{'info': ''}]\u001b[A\n",
            " 92% 3.27G/3.54G [00:05<00:00, 620MB/s{'info': ''}]\u001b[A\n",
            " 94% 3.33G/3.54G [00:05<00:00, 622MB/s{'info': ''}]\u001b[A\n",
            " 96% 3.39G/3.54G [00:05<00:00, 625MB/s{'info': ''}]\u001b[A\n",
            " 97% 3.45G/3.54G [00:06<00:00, 625MB/s{'info': ''}]\u001b[A\n",
            " 99% 3.50G/3.54G [00:06<00:00, 628MB/s{'info': ''}]\u001b[A\n",
            "                                                   \u001b[A\n",
            "!\u001b[A\n",
            "  0% |          |0/? [00:00<?,    ?files/s]\u001b[A\n",
            "                                           \u001b[A\n",
            "!\u001b[A\n",
            "  0%|          |Checking out .venv.tar.gz             0/? [00:00<?,    ?files/s]\u001b[A\n",
            "Checking out .venv.tar.gz:   0% 0/1 [00:00<?, ?files/s{'info': ''}]             \u001b[A\n",
            "Checking out .venv.tar.gz: 100% 1/1 [00:09<00:00,  9.05s/files{'info': ''}]\u001b[A\n",
            "Adding...: 100% 1/1 [00:16<00:00, 16.51s/file{'info': ''}]\n",
            "\n",
            "To track the changes with git, run:\n",
            "\n",
            "\tgit add .venv.tar.gz.dvc\n",
            "\n",
            "To enable auto staging, run:\n",
            "\n",
            "\tdvc config core.autostage true\n",
            "\u001b[0m[main b273b66] Add .venv.tar.gz.dvc to DVC\n",
            " 1 file changed, 2 insertions(+), 2 deletions(-)\n",
            "\u001b[0mCPU times: user 1.22 s, sys: 220 ms, total: 1.44 s\n",
            "Wall time: 3min 37s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "def push_venv_to_dvc():\n",
        "  if not USE_VENV:\n",
        "    %cd {gpt_neox_colabDir}\n",
        "    venv_dir = \".venv\"\n",
        "    venv_tar_file = f\"{venv_dir}.tar\"\n",
        "    venv_gz_file = f\"{venv_tar_file}.gz\"\n",
        "    if os.path.isdir(venv_dir):\n",
        "      if not os.path.isfile(venv_tar_file):\n",
        "        print(f\"Tarring {venv_tar_file}\")\n",
        "        !tar -cf .venv.tar .venv\n",
        "      if not os.path.isfile(venv_gz_file):\n",
        "        print(f\"Zipping {venv_tar_file}\")\n",
        "        !sudo apt-get install -y pigz\n",
        "        !pigz -p 8 {venv_tar_file}\n",
        "      print(f\"Uploading {venv_gz_file} to DVC\")\n",
        "      !dvc add {venv_gz_file}\n",
        "      !git add .venv.tar.gz.dvc\n",
        "      !git commit -m \"Add .venv.tar.gz.dvc to DVC\"\n",
        "      !dvc push -q\n",
        "      !git push\n",
        "push_venv_to_dvc()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}