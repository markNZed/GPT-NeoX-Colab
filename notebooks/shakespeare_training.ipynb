{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/shakespeare_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lBrppZd_A0"
      },
      "source": [
        "# Training a tiny SLM on a corpus of Shakespeare\n",
        "The intention of this notebook is to demonstrate a setup for experimenting with a tiny SLM.\n",
        "The following tools are used:\n",
        "* Colab (https://colab.research.google.com/) for notebook execution\n",
        "* DagsHub (https://dagshub.com/) for project tracking\n",
        "* MLFlow (https://mlflow.org/) for experiment tracking\n",
        "* Hydra (https://hydra.cc/) for configuration management\n",
        "* GPTNeoX (https://github.com/EleutherAI/gpt-neox) for model training\n",
        "* Tensorboard (https://www.tensorflow.org/tensorboard) for experiment monitoring\n",
        "* DVC (https://dvc.org/) for data management\n",
        "* GitHub (https://github.com/) for code management\n",
        "* Backblaze (https://backblaze.com/) for data storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uYyxsLpOuQR1",
        "outputId": "84665539-354e-41a3-9c00-09a9897901ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Date and Time: 2024-11-28 14:45:04.258249\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(\"Current Date and Time:\", datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google import colab\n",
        "  isColab = True\n",
        "  from google.colab import userdata\n",
        "  if userdata.get('GITHUB_NAME'):\n",
        "    !git config --global user.name \"{userdata.get('GITHUB_NAME')}\"\n",
        "  if userdata.get('GITHUB_EMAIL'):\n",
        "    !git config --global user.email \"{userdata.get('GITHUB_EMAIL')}\"\n",
        "  if userdata.get('AWS_SECRET_ACCESS_KEY'):\n",
        "    !echo \"export AWS_SECRET_ACCESS_KEY={userdata.get('AWS_SECRET_ACCESS_KEY')}\" >> ~/.bashrc\n",
        "  if userdata.get('AWS_ACCESS_KEY_ID'):\n",
        "    !echo \"export AWS_ACCESS_KEY_ID={userdata.get('AWS_ACCESS_KEY_ID')}\" >> ~/.bashrc\n",
        "except:\n",
        "  isColab = False\n",
        "print(\"isColab:\", isColab)"
      ],
      "metadata": {
        "id": "sR5zBqZ0Q25t",
        "outputId": "c0606006-8ea5-43b3-ad8c-164d6f59d198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "isColab: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HBH4eN9Id_A7"
      },
      "outputs": [],
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "workspaceDir = \"/content\"\n",
        "if isColab:\n",
        "  gpt_neox_colabDir = f\"{workspaceDir}/GPT-NeoX-Colab\"\n",
        "else:\n",
        "  gpt_neox_colabDir = f\"/workspace\"\n",
        "GPTNeoXDirName = \"gpt-neox\"\n",
        "GPTNeoXDir = f\"{workspaceDir}/{GPTNeoXDirName}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e27VRVq07s"
      },
      "source": [
        "# Cloning Git Repos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "j_hUsQxlhnou",
        "outputId": "71734a1e-712b-4b34-9d01-efd6c4977c76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'GPT-NeoX-Colab'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 72 (delta 5), reused 34 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (72/72), 8.09 MiB | 12.49 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "Cloned with PAT\n",
            "CPU times: user 61.3 ms, sys: 7.22 ms, total: 68.5 ms\n",
            "Wall time: 4.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX-Colab\n",
        "%cd {workspaceDir}\n",
        "if isColab:\n",
        "  # Don't use --depth 1 because that does not play nice with git-annex\n",
        "  if userdata.get('GITHUB_PAT'):\n",
        "    !git clone --depth 1 https://{userdata.get('GITHUB_PAT')}@github.com/markNZed/GPT-NeoX-Colab.git\n",
        "    print(\"Cloned with PAT\")\n",
        "  else:\n",
        "    !git clone --depth 1 https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "    print(\"Cloned without PAT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jp8wncmZDAew",
        "outputId": "a9fe8de3-bcc1-4c50-ab3f-73f49c5671b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for gtp_neox_colab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.4/901.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.7/548.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pysftp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gtp_neox_colab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "CPU times: user 1.03 s, sys: 133 ms, total: 1.16 s\n",
            "Wall time: 2min 23s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import sys\n",
        "%cd {gpt_neox_colabDir}\n",
        "%pip install -q python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv()\n",
        "\n",
        "USE_VENV = False\n",
        "if USE_VENV:\n",
        "  # Disabling pydevd_plugins so we do not get a restart warning\n",
        "  #if \"pydevd_plugins\" in sys.modules:\n",
        "  #  del sys.modules[\"pydevd_plugins\"]\n",
        "  %pip install -q dvc[s3]\n",
        "  if not os.path.isfile(\"my_env.tar.gz\"):\n",
        "    !dvc --quiet pull .venv.tar.gz\n",
        "    !tar -xf .venv.tar.gz\n",
        "  activate_script = f\"{gpt_neox_colabDir}/my_env/bin/activate\"\n",
        "elif isColab:\n",
        "    !uv sync -q --dev\n",
        "    !uv run pip install -q -e .\n",
        "    activate_script = f\"{gpt_neox_colabDir}/.venv/bin/activate\"\n",
        "    #!sudo apt-get update && sudo apt-get install -y python3.10-venv\n",
        "    #!pip install -q virtualenv\n",
        "    #!python3 -m venv my_env\n",
        "    #activate_script = f\"{gpt_neox_colabDir}/my_env/bin/activate\"\n",
        "    !source {activate_script} && pip install -q -r requirements_colab.txt\n",
        "    !source {activate_script} && pip install -q .\n",
        "else:\n",
        "    activate_script = f\"{gpt_neox_colabDir}/.venv/bin/activate\"\n",
        "    !pip install -q dvc[s3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y8Mic7aoiNxi",
        "outputId": "1e3b6c3e-0fcd-4a93-b344-ce67fbc5f1ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n",
            "Collecting          |3.00 [00:01, 1.50entry/s]\n",
            "Fetching\n",
            "!\u001b[A\n",
            "  0% |          |0/? [00:00<?,    ?files/s]\u001b[A\n",
            "                                           \u001b[A\n",
            "Querying remote cache:   0% 0/1 [00:00<?, ?files/s]\u001b[A\n",
            "Querying remote cache:   0% 0/1 [00:00<?, ?files/s{'info': ''}]\u001b[A\n",
            "Querying remote cache: 100% 1/1 [00:00<00:00,  4.76files/s{'info': ''}]\u001b[A\n",
            "                                                                       \u001b[A\n",
            "Fetching from s3:   0% 0/2 [00:00<?, ?file/s]\u001b[A\n",
            "Fetching from s3:   0% 0/2 [00:00<?, ?file/s{'info': ''}]\u001b[A\n",
            "Fetching from s3:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\u001b[A\n",
            "\n",
            "  0% 0.00/1.06M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "  0% 0.00/1.06M [00:00<?, ?B/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  2% 23.6k/1.06M [00:00<00:08, 135kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  4% 47.6k/1.06M [00:00<00:06, 175kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  6% 63.6k/1.06M [00:00<00:06, 156kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 10% 104k/1.06M [00:00<00:04, 236kB/s{'info': ''}] \u001b[A\u001b[A\n",
            "\n",
            " 13% 144k/1.06M [00:00<00:03, 290kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 19% 208k/1.06M [00:00<00:02, 387kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 28% 304k/1.06M [00:00<00:01, 553kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 40% 432k/1.06M [00:00<00:00, 771kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 56% 608k/1.06M [00:01<00:00, 1.07MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 80% 872k/1.06M [00:01<00:00, 1.55MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "                                                   \u001b[A\u001b[A\n",
            "Fetching from s3: 100% 1/1 [00:01<00:00,  1.55s/file{'info': ''}]\u001b[A\n",
            "\n",
            "  0% 0.00/786 [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "  0% 0.00/786 [00:00<?, ?B/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "                                         \u001b[A\u001b[A\n",
            "  0%|          |Fetching from s3                  2/? [00:02<00:00,  1.29file/s]\u001b[A\n",
            "Fetching\n",
            "Building workspace index          |0.00 [00:00,    ?entry/s]\n",
            "Comparing indexes          |4.00 [00:00,  128entry/s]\n",
            "Applying changes          |1.00 [00:00,   126file/s]\n",
            "\u001b[32mA\u001b[0m       data/\n",
            "1 file added and 2 files fetched\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#@title Fetch training data\n",
        "%cd {gpt_neox_colabDir}\n",
        "!source {activate_script} && dvc --quiet pull \"data/shakespeare/shakespeare.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "It4jtnb2yvkP",
        "outputId": "9654cd8f-9248-4be9-8574-2a32da54198a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n",
            "Collecting          |3.00 [00:00,  135entry/s]\n",
            "Fetching\n",
            "!\u001b[A\n",
            "  0% |          |0/? [00:00<?,    ?files/s]\u001b[A\n",
            "                                           \u001b[A\n",
            "Querying remote cache:   0% 0/1 [00:00<?, ?files/s]\u001b[A\n",
            "Querying remote cache:   0% 0/1 [00:00<?, ?files/s{'info': ''}]\u001b[A\n",
            "Querying remote cache: 100% 1/1 [00:00<00:00,  4.78files/s{'info': ''}]\u001b[A\n",
            "                                                                       \u001b[A\n",
            "!\u001b[A\n",
            "  0% Querying remote cache|          |0/0 [00:00<?,    ?files/s]\u001b[A\n",
            "                                                                \u001b[A\n",
            "Fetching from s3:   0% 0/1 [00:00<?, ?file/s]\u001b[A\n",
            "Fetching from s3:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\u001b[A\n",
            "\n",
            "  0% 0.00/1.43M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "  0% 0.00/1.43M [00:00<?, ?B/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  2% 23.6k/1.43M [00:00<00:10, 135kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  3% 47.6k/1.43M [00:00<00:08, 175kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  4% 63.6k/1.43M [00:00<00:09, 156kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  7% 104k/1.43M [00:00<00:05, 235kB/s{'info': ''}] \u001b[A\u001b[A\n",
            "\n",
            " 10% 144k/1.43M [00:00<00:04, 289kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 14% 208k/1.43M [00:00<00:03, 386kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 21% 304k/1.43M [00:00<00:02, 551kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 29% 432k/1.43M [00:00<00:01, 769kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 41% 608k/1.43M [00:01<00:00, 1.07MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 59% 872k/1.43M [00:01<00:00, 1.54MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 85% 1.21M/1.43M [00:01<00:00, 2.19MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "                                                    \u001b[A\u001b[A\n",
            "Fetching from s3: 100% 1/1 [00:01<00:00,  1.63s/file{'info': ''}]\u001b[A\n",
            "Fetching\n",
            "Building workspace index          |2.00 [00:00, 3.76kentry/s]\n",
            "Comparing indexes          |4.00 [00:00,  131entry/s]\n",
            "Applying changes          |1.00 [00:00,   115file/s]\n",
            "\u001b[33mM\u001b[0m       data/\n",
            "1 file modified and 1 file fetched\n",
            "Collecting          |3.00 [00:00,  150entry/s]\n",
            "Fetching\n",
            "!\u001b[A\n",
            "  0% |          |0/? [00:00<?,    ?files/s]\u001b[A\n",
            "                                           \u001b[A\n",
            "Querying remote cache:   0% 0/1 [00:00<?, ?files/s]\u001b[A\n",
            "Querying remote cache:   0% 0/1 [00:00<?, ?files/s{'info': ''}]\u001b[A\n",
            "Querying remote cache: 100% 1/1 [00:00<00:00,  4.71files/s{'info': ''}]\u001b[A\n",
            "                                                                       \u001b[A\n",
            "!\u001b[A\n",
            "  0% Querying remote cache|          |0/0 [00:00<?,    ?files/s]\u001b[A\n",
            "                                                                \u001b[A\n",
            "Fetching from s3:   0% 0/1 [00:00<?, ?file/s]\u001b[A\n",
            "Fetching from s3:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\u001b[A\n",
            "\n",
            "  0% 0.00/2.11M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "  0% 0.00/2.11M [00:00<?, ?B/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  1% 23.6k/2.11M [00:00<00:16, 137kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  2% 47.6k/2.11M [00:00<00:12, 177kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  3% 63.6k/2.11M [00:00<00:13, 158kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  5% 104k/2.11M [00:00<00:08, 238kB/s{'info': ''}] \u001b[A\u001b[A\n",
            "\n",
            "  7% 152k/2.11M [00:00<00:06, 302kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 10% 216k/2.11M [00:00<00:04, 399kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 14% 312k/2.11M [00:00<00:03, 569kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 21% 448k/2.11M [00:00<00:02, 806kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 29% 632k/2.11M [00:01<00:01, 1.12MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 42% 912k/2.11M [00:01<00:00, 1.63MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 60% 1.27M/2.11M [00:01<00:00, 2.30MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 86% 1.81M/2.11M [00:01<00:00, 3.32MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "                                                    \u001b[A\u001b[A\n",
            "Fetching from s3: 100% 1/1 [00:01<00:00,  1.98s/file{'info': ''}]\u001b[A\n",
            "Fetching\n",
            "Building workspace index          |2.00 [00:00, 3.94kentry/s]\n",
            "Comparing indexes          |4.00 [00:00, 76.9entry/s]\n",
            "Applying changes          |1.00 [00:00,   102file/s]\n",
            "\u001b[33mM\u001b[0m       data/\n",
            "1 file modified and 1 file fetched\n",
            "Collecting          |3.00 [00:00,  153entry/s]\n",
            "Fetching\n",
            "!\u001b[A\n",
            "  0% |          |0/? [00:00<?,    ?files/s]\u001b[A\n",
            "                                           \u001b[A\n",
            "Querying remote cache:   0% 0/1 [00:00<?, ?files/s]\u001b[A\n",
            "Querying remote cache:   0% 0/1 [00:00<?, ?files/s{'info': ''}]\u001b[A\n",
            "Querying remote cache: 100% 1/1 [00:00<00:00,  4.82files/s{'info': ''}]\u001b[A\n",
            "                                                                       \u001b[A\n",
            "!\u001b[A\n",
            "  0% Querying remote cache|          |0/0 [00:00<?,    ?files/s]\u001b[A\n",
            "                                                                \u001b[A\n",
            "Fetching from s3:   0% 0/1 [00:00<?, ?file/s]\u001b[A\n",
            "Fetching from s3:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\u001b[A\n",
            "\n",
            "  0% 0.00/640k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "  0% 0.00/640k [00:00<?, ?B/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  4% 23.6k/640k [00:00<00:04, 136kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  7% 47.6k/640k [00:00<00:03, 175kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 10% 63.6k/640k [00:00<00:03, 162kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 16% 104k/640k [00:00<00:02, 241kB/s{'info': ''}] \u001b[A\u001b[A\n",
            "\n",
            " 22% 144k/640k [00:00<00:01, 293kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 32% 208k/640k [00:00<00:01, 394kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 47% 304k/640k [00:00<00:00, 557kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 67% 432k/640k [00:00<00:00, 778kB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 96% 616k/640k [00:01<00:00, 1.10MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "                                                  \u001b[A\u001b[A\n",
            "Fetching from s3: 100% 1/1 [00:01<00:00,  1.58s/file{'info': ''}]\u001b[A\n",
            "Fetching\n",
            "Building workspace index          |2.00 [00:00, 3.52kentry/s]\n",
            "Comparing indexes          |4.00 [00:00,  135entry/s]\n",
            "Applying changes          |1.00 [00:00,   127file/s]\n",
            "\u001b[33mM\u001b[0m       data/\n",
            "1 file modified and 1 file fetched\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#@title Fetch processed training data\n",
        "%cd {gpt_neox_colabDir}\n",
        "!source {activate_script} && dvc --quiet pull \"data/shakespeare/shakespeare.jsonl\"\n",
        "!source {activate_script} && dvc --quiet pull \"data/shakespeare/shakespeare_text_document.bin\"\n",
        "!source {activate_script} && dvc --quiet pull \"data/shakespeare/shakespeare_text_document.idx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "FjLEIFCR6d8m",
        "outputId": "e7cfe483-b57b-46af-c760-73b97114919b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'gpt-neox'...\n",
            "remote: Enumerating objects: 296, done.\u001b[K\n",
            "remote: Counting objects: 100% (296/296), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 296 (delta 74), reused 136 (delta 43), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (296/296), 2.50 MiB | 9.66 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n",
            "CPU times: user 22.6 ms, sys: 2.76 ms, total: 25.4 ms\n",
            "Wall time: 1.31 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX\n",
        "%cd {workspaceDir}\n",
        "#!git clone ---depth 1 https://github.com/EleutherAI/gpt-neox\n",
        "!git clone -b pipe_parallel_size_1 --depth 1 https://github.com/markNZed/gpt-neox.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va0pS1dvd_BD"
      },
      "source": [
        "# Python Environment\n",
        "It is faster to download a Python virtual environment and unzip it than to install all the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AUmdStRWhrUR",
        "collapsed": true,
        "outputId": "094c73c0-564d-4394-ff46-831e84dd14b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gtp-neox-colab 0.1.0 requires tensorflow==2.17.1, but you have tensorflow 2.17.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m/content/gpt-neox\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lm_dataformat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "if not USE_VENV:\n",
        "    # Could not redirect to /dev/null in the standard Colab notebook (maybe no output for a particular time?)\n",
        "    # Currently deepspeed from GTP-NeoX is not compatible with logging in torch >= 2.4\n",
        "    !source {activate_script} && pip install -q torch==2.3 torchaudio==2.3.0 torchvision==0.18.0 transformers==4.38.0 sentence-transformers==2.2.2\n",
        "    !source {activate_script} && pip install -q fsspec==2024.10.0 datasets==2.14.0 evaluate==0.4.3 lm-eval==0.4.1 tensorboard==2.17.0 tensorflow==2.17.0\n",
        "    %cd {GPTNeoXDir}\n",
        "    !source {activate_script} && pip install -q -r ./requirements/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jzX5ohGax6p"
      },
      "source": [
        "# Preparing Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N-hmZjCc-WnV",
        "outputId": "add3318b-6e72-451e-ba45-42faed1d5479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n"
          ]
        }
      ],
      "source": [
        "#@title Converting text data to jsonl format\n",
        "import os\n",
        "\n",
        "%cd {GPTNeoXDir}\n",
        "!mkdir -p data\n",
        "\n",
        "# Check if the converted file exists\n",
        "if not os.path.isfile(f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl\"):\n",
        "    gpt_neox_colab.utils.ml.text2jsonl(f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.txt\", f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl\")\n",
        "\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare.jsonl {GPTNeoXDir}/data/shakespeare.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x57thNaLa-yN"
      },
      "source": [
        "# Tokenizing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JyD8RujkEUsr",
        "outputId": "c431feb8-4ee8-4f1f-e2f4-de3a76211dc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n",
            "CPU times: user 15.3 ms, sys: 4.04 ms, total: 19.3 ms\n",
            "Wall time: 314 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Tokenizing jsonl formatted data\n",
        "import os\n",
        "\n",
        "%cd {GPTNeoXDir}\n",
        "!mkdir -p processed_data\n",
        "\n",
        "# Check if the tokenized files exists\n",
        "a = f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.idx\"\n",
        "b = f\"{gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.bin\"\n",
        "if not os.path.isfile(a) or not os.path.isfile(b):\n",
        "    !source {activate_script} && python tools/datasets/preprocess_data.py \\\n",
        "        --input ./data/shakespeare.jsonl \\\n",
        "        --output-prefix ./processed_data \\\n",
        "        --tokenizer-type CharLevelTokenizer \\\n",
        "        --dataset-impl mmap \\\n",
        "        --append-eod\n",
        "    !cp {GPTNeoXDir}/processed_data/shakespeare_text_document.bin {gpt_neox_colabDir}/data/shakespeare\n",
        "    !cp {GPTNeoXDir}/processed_data/shakespeare_text_document.idx {gpt_neox_colabDir}/data/shakespeare\n",
        "\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.bin {GPTNeoXDir}/processed_data\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.idx {GPTNeoXDir}/processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YNM2gpADtjM9"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PtOHyiVDhrUZ",
        "outputId": "71b42b02-7bdd-4779-9d76-32e62cf3f20f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running command: nohup bash -c \"source /content/GPT-NeoX-Colab/.venv/bin/activate && cd /content/gpt-neox && python ./deepy.py train.py --conf_dir /content/GPT-NeoX-Colab/configs shakespeare shakespeare_deepy\" \n",
            "Started training with PID: 3332\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "# Start a detached background process using the temp config\n",
        "cmd = f\"\"\"\n",
        "nohup source {activate_script} && \\\n",
        "cd {GPTNeoXDir} && \\\n",
        "python ./deepy.py train.py --conf_dir {gpt_neox_colabDir}/configs shakespeare shakespeare_deepy\n",
        "\"\"\"\n",
        "print(\"Running command:\", cmd)\n",
        "#cmd = \"nohup bash -c ls\" # Used to test without running on GPU\n",
        "\n",
        "# Start the process and retrieve the PID directly\n",
        "process = subprocess.Popen(\n",
        "    cmd,\n",
        "    shell=True,\n",
        "    executable='/bin/bash',\n",
        "    preexec_fn=os.setsid  # Starts the process in a new session\n",
        ")\n",
        "\n",
        "pid = process.pid\n",
        "print(f\"Started training with PID: {pid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WVCgdZ1Ma9TI",
        "outputId": "3953e12c-5b28-4be7-df94-149c4f89aa18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n",
            "Waiting for TensorBoard log directory to be created...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8c33756298d6>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorboard_log_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Waiting for TensorBoard log directory to be created...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Check every X seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TensorBoard log directory found. You can now launch TensorBoard.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Wait until tensorboard log directory is created\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Path to the TensorBoard log directory\n",
        "tensorboard_log_dir = f\"{GPTNeoXDir}/tensorboard\"\n",
        "\n",
        "# Wait for the directory to be created\n",
        "while not os.path.exists(tensorboard_log_dir):\n",
        "    print(\"Waiting for TensorBoard log directory to be created...\")\n",
        "    time.sleep(10)  # Check every X seconds\n",
        "\n",
        "print(\"TensorBoard log directory found. You can now launch TensorBoard.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfefGgc-ln-J"
      },
      "outputs": [],
      "source": [
        "# Need to delete everything in checkpoints and tensorboard dir for a fresh run\n",
        "%cd {GPTNeoXDir}\n",
        "%tensorboard --logdir tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j3_fG5vV7AB"
      },
      "outputs": [],
      "source": [
        "#@title Find the latest log file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Define the log directory and pattern for log files\n",
        "log_dir = f\"{GPTNeoXDir}/logs\"\n",
        "log_pattern = os.path.join(log_dir, \"*_stdout.txt\")\n",
        "\n",
        "# Get the list of log files that match the pattern\n",
        "log_files = glob.glob(log_pattern)\n",
        "\n",
        "# Ensure there are log files in the directory\n",
        "if log_files:\n",
        "    # Find the latest log file based on modification time\n",
        "    latest_log = max(log_files, key=os.path.getmtime)\n",
        "    print(\"Latest log file:\", latest_log)\n",
        "else:\n",
        "    latest_log = None\n",
        "    print(\"No log files found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QszVj7_vSP7L"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn13kNSRacnA"
      },
      "outputs": [],
      "source": [
        "#@title Read the latest log file and extract the iteration count\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "\n",
        "# File to store the last read position (persistence between script runs)\n",
        "file_position = 0\n",
        "# Regular expression to match \"iteration <number> / <total>\"\n",
        "iteration_pattern = re.compile(r\"iteration\\s+(\\d+)\\s*/\\s*\\d+\")\n",
        "\n",
        "def read_new_iterations():\n",
        "    global file_position\n",
        "    # Open the log file and seek to the last position\n",
        "    with open(latest_log, \"r\") as file:\n",
        "        file.seek(file_position)\n",
        "        # Read new lines\n",
        "        new_lines = file.readlines()\n",
        "        file_position = file.tell()\n",
        "        # Process lines containing \"iteration\"\n",
        "        last_match = None\n",
        "        for line in new_lines:\n",
        "            match = iteration_pattern.search(line)\n",
        "            if match:\n",
        "                last_match = match\n",
        "        if last_match:\n",
        "            # Extract the iteration count from the regex match\n",
        "            iteration_count = int(last_match.group(1))\n",
        "            print(f\"{iteration_count} iterations\")\n",
        "\n",
        "# Periodically check if the process has completed\n",
        "while True:\n",
        "    # Poll the process to see if it has terminated\n",
        "    if process.poll() is not None:\n",
        "        # Process has completed\n",
        "        print(\"Training has completed.\")\n",
        "        break\n",
        "    else:\n",
        "        if latest_log:\n",
        "            read_new_iterations()\n",
        "        elif os.path.exists(f\"{experimentDir}/logs\"):\n",
        "            latest_log = get_latest_file(f\"{experimentDir}/logs\", \"*_stdout.txt\")\n",
        "        print(\"Training is still running...\")\n",
        "        time.sleep(30)  # Check every X seconds\n",
        "\n",
        "print(\"Training has finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJb7TUqi4vVg"
      },
      "outputs": [],
      "source": [
        "#@title Display training and validation Loss\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "import os\n",
        "import numpy as np\n",
        "# Path to the latest log file\n",
        "log_dir = \"tensorboard\"\n",
        "log_files = [os.path.join(log_dir, d) for d in os.listdir(log_dir)]\n",
        "latest_log_dir = max(log_files, key=os.path.getmtime)\n",
        "\n",
        "# Initialize EventAccumulator to load scalar data\n",
        "ea = event_accumulator.EventAccumulator(latest_log_dir)\n",
        "ea.Reload()  # Load all logs\n",
        "\n",
        "# List all scalar keys available in the logs\n",
        "scalar_keys = ea.Tags()['scalars']\n",
        "print(\"Available scalar keys:\", scalar_keys)\n",
        "\n",
        "# Extract training and validation losses\n",
        "train_loss = ea.Scalars('train/lm_loss')  # Adjust for actual name if necessary\n",
        "val_loss = ea.Scalars('validation/lm_loss')  # Adjust for actual name if necessary\n",
        "\n",
        "# Convert to lists for plotting\n",
        "train_loss_values = [x.value for x in train_loss]\n",
        "val_loss_values = [x.value for x in val_loss]\n",
        "\n",
        "# Find the lengths of both arrays\n",
        "len_train = len(train_loss_values)\n",
        "len_val = len(val_loss_values)\n",
        "\n",
        "iterations = None\n",
        "# Interpolate the shorter array\n",
        "if len_train != len_val:\n",
        "    if len_train > len_val:\n",
        "        # Interpolate validation loss to match the training loss length\n",
        "        iterations = np.linspace(1, len_train, len_train)\n",
        "        val_iterations = np.linspace(1, len_train, len_val)\n",
        "        val_loss_values = np.interp(iterations, val_iterations, val_loss_values)\n",
        "    else:\n",
        "        # Interpolate training loss to match the validation loss length\n",
        "        iterations = np.linspace(1, len_val, len_val)\n",
        "        train_iterations = np.linspace(1, len_val, len_train)\n",
        "        train_loss_values = np.interp(iterations, train_iterations, train_loss_values)\n",
        "else:\n",
        "    iterations = range(1, len_train + 1)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(iterations, train_loss_values, label='Training Loss')\n",
        "plt.plot(iterations, val_loss_values, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk8DhmmEZFyz"
      },
      "source": [
        "# Inference with GPT-NeoX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKb0Ar6NZFyz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%cd {GPTNeoXDir}\n",
        "# This has issues if used during training -  The server socket has failed to bind to [::]:29500 (errno: 98 - Address already\n",
        "# This will write over the logs\n",
        "!source {activate_script} && python ./deepy.py generate.py -d configs {gpt_neox_colabDir}/configs/shakespeare {gpt_neox_colabDir}/configs/shakespeare_gen\n",
        "!cat sample_output.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ajLR98lDi9_8",
        "outputId": "f6798aab-3c07-4232-fd66-c1a8deadb031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.14.6\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (0.3.7)\n",
            "Requirement already satisfied: pandas in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (0.70.15)\n",
            "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.14.6)\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (3.11.8)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (0.26.3)\n",
            "Requirement already satisfied: packaging in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from datasets==2.14.6) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (1.18.0)\n",
            "Requirement already satisfied: filelock in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from pandas->datasets==2.14.6) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from pandas->datasets==2.14.6) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from pandas->datasets==2.14.6) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /content/GPT-NeoX-Colab/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6) (1.16.0)\n",
            "Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "Installing collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.0\n",
            "    Uninstalling datasets-2.14.0:\n",
            "      Successfully uninstalled datasets-2.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dvc 3.57.0 requires fsspec>=2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\n",
            "dvc-data 3.16.7 requires fsspec>=2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\n",
            "dvc-objects 5.1.0 requires fsspec>=2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\n",
            "gtp-neox-colab 0.1.0 requires tensorflow==2.17.1, but you have tensorflow 2.17.0 which is incompatible.\n",
            "s3fs 2024.10.0 requires fsspec==2024.10.0.*, but you have fsspec 2023.10.0 which is incompatible.\n",
            "scmrepo 3.3.9 requires fsspec[tqdm]>=2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.14.6 fsspec-2023.10.0\n"
          ]
        }
      ],
      "source": [
        "# 2.21.0 was the last 2 series but it asks for trust_remote_code\n",
        "!source {activate_script} &&  pip install datasets==2.14.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S-foNPFjfwH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# This has issues if used during training -  The server socket has failed to bind to [::]:29500 (errno: 98 - Address already\n",
        "# This will write over the logs\n",
        "# python ./deepy.py eval.py -d configs your_configs.yml --eval_tasks task1 task2 ... taskn\n",
        "# NOTE this will prompt for permission to run a download script - would need an older datasetse library to avoid this\n",
        "%cd {GPTNeoXDir}\n",
        "!source {activate_script} && python ./deepy.py eval.py -d configs {gpt_neox_colabDir}/configs/shakespeare {gpt_neox_colabDir}/configs/shakespeare_gen --eval_tasks hellaswag\n",
        "!cat sample_output.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7AOL1NJ43e"
      },
      "source": [
        "# Inference with Hugging Face\n",
        "\n",
        "## Convert model to HF format\n",
        "Here we are converting our model to `HuggingFace Format`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNdWrrkWfyT-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the checkpoints directory\n",
        "checkpoints_dir = f\"{GPTNeoXDir}/checkpoints\"\n",
        "\n",
        "# Read the 'latest' file to get the latest checkpoint name\n",
        "with open(os.path.join(checkpoints_dir, \"latest\"), \"r\") as f:\n",
        "    latest_checkpoint_name = f.read().strip()\n",
        "\n",
        "# Construct the full path to the latest checkpoint directory\n",
        "latest_checkpoint_path = os.path.join(checkpoints_dir, latest_checkpoint_name)\n",
        "print(\"Path to the latest checkpoint:\", latest_checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rplkNclPXgkH"
      },
      "outputs": [],
      "source": [
        "#@title Convert last checkpoint to huggingface model\n",
        "%cd {GPTNeoXDir}\n",
        "!source {activate_script} && python ./tools/ckpts/convert_neox_to_hf.py --input_dir {latest_checkpoint_path} --config_file {gpt_neox_colabDir}/configs/shakespeare.yml --output_dir {gpt_neox_colabDir}/data/shakespeare --architecture neox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qra1qQtC2oiI"
      },
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01ZRN2IceM8a"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, f\"{gpt_neox_colabDir}/my_env/lib/python3.10/site-packages\")\n",
        "\n",
        "from transformers import GPTNeoXForCausalLM\n",
        "import torch\n",
        "\n",
        "# Move to model directory\n",
        "%cd {gpt_neox_colabDir}\n",
        "\n",
        "# Assuming CharLevelTokenizer is properly imported and instantiated\n",
        "from gpt_neox_colab import CharLevelTokenizer\n",
        "tokenizer = CharLevelTokenizer.CharLevelTokenizer(vocab_size=512)\n",
        "\n",
        "# Load your model\n",
        "model_path = f\"{gpt_neox_colabDir}/data/shakespeare\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define a simple char-level tokenizer if not provided\n",
        "def char_level_tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def char_level_detokenize(tokens):\n",
        "    return tokenizer.detokenize(tokens)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user for input\n",
        "#input_text = input(\"Enter your prompt: \")\n",
        "input_text = \"Thou art\"\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([char_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = char_level_detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abEMTFL3hrUc"
      },
      "outputs": [],
      "source": [
        "if isColab:\n",
        "  import time\n",
        "  while True:\n",
        "    output.eval_js(\"new Audio(\\\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Coins_dropped_in_metallic_moneybox_0.ogg\\\").play()\")\n",
        "    time.sleep(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e60SXzNLiinb"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "# Autplay does not work in VSCode\n",
        "IPython.display.Audio(filename=f\"{gpt_neox_colabDir}/notebooks/beep-01a.mp3\", autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoOuzaRkiinb"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import numpy as np\n",
        "fs = 16000.\n",
        "# Autplay does not work in VSCode\n",
        "IPython.display.Audio(np.sin(2*np.pi*440*np.arange(1 * fs)/fs), rate=fs, autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8W3kedId_Bf"
      },
      "outputs": [],
      "source": [
        "# Here we could disconnect from the Colab GPU resource but we will lose all results\n",
        "#from google.colab import runtime\n",
        "#runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = input(\"Enter your prompt: \")\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([char_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = char_level_detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ],
      "metadata": {
        "id": "YmBmP2UE-amo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not USE_VENV and UPDATE_VENV:\n",
        "  %cd {gpt_neox_colabDir}\n",
        "  %pip install dvc[s3]\n",
        "  !dvc add -q .venv.tar.gz\n",
        "  !git add .venv.tar.gz.dvc .gitignore\n",
        "  !git commit -m \"Add .venv.tar.gz.dvc to DVC\"\n",
        "  !dvc push -q"
      ],
      "metadata": {
        "id": "ASPRn9EMWzBL",
        "outputId": "b5691af4-a7a3-4159-e036-65254e178c50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph\n",
            "Adding...:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\n",
            "!\u001b[A\n",
            "          |0.00 [00:00,     ?file/s]\u001b[A\n",
            "                                    \u001b[A\n",
            "!\u001b[A\n",
            "  0% |          |0/? [00:00<?,    ?files/s]\u001b[A\n",
            "                                           \u001b[A\n",
            "Adding .venv.tar.gz to cache:   0% 0/1 [00:00<?, ?file/s]\u001b[A\n",
            "Adding .venv.tar.gz to cache:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\u001b[A\n",
            "\n",
            "/content/GPT-NeoX-Colab/.venv.tar.gz:   0% 0.00/3.55G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "  0% 0.00/3.55G [00:00<?, ?B/s{'info': ''}]                          \u001b[A\u001b[A\n",
            "\n",
            "  1% 42.0M/3.55G [00:00<00:08, 440MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  2% 82.0M/3.55G [00:00<00:08, 424MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  3% 122M/3.55G [00:00<00:08, 420MB/s{'info': ''}] \u001b[A\u001b[A\n",
            "\n",
            "  4% 162M/3.55G [00:00<00:08, 419MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  6% 202M/3.55G [00:00<00:08, 417MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  7% 242M/3.55G [00:00<00:08, 416MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  8% 281M/3.55G [00:00<00:08, 411MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  9% 321M/3.55G [00:00<00:08, 413MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 10% 361M/3.55G [00:00<00:08, 412MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 11% 400M/3.55G [00:01<00:08, 408MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 12% 440M/3.55G [00:01<00:08, 408MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 13% 479M/3.55G [00:01<00:08, 408MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 14% 518M/3.55G [00:01<00:08, 406MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 15% 556M/3.55G [00:01<00:08, 403MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 16% 596M/3.55G [00:01<00:07, 406MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 17% 635M/3.55G [00:01<00:07, 405MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 19% 672M/3.55G [00:01<00:07, 400MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 20% 709M/3.55G [00:01<00:07, 394MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 20% 741M/3.55G [00:01<00:08, 375MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 21% 778M/3.55G [00:02<00:07, 376MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 22% 816M/3.55G [00:02<00:07, 381MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 23% 853M/3.55G [00:02<00:07, 382MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 24% 889M/3.55G [00:02<00:07, 380MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 26% 927M/3.55G [00:02<00:07, 382MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 27% 963M/3.55G [00:02<00:07, 378MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 28% 0.98G/3.55G [00:02<00:07, 382MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 29% 1.01G/3.55G [00:02<00:07, 377MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 30% 1.05G/3.55G [00:02<00:06, 385MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 31% 1.09G/3.55G [00:02<00:06, 395MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 32% 1.13G/3.55G [00:03<00:06, 393MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 33% 1.16G/3.55G [00:03<00:06, 384MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 34% 1.20G/3.55G [00:03<00:06, 388MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 35% 1.24G/3.55G [00:03<00:06, 394MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 36% 1.27G/3.55G [00:03<00:06, 396MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 37% 1.31G/3.55G [00:03<00:06, 396MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 38% 1.35G/3.55G [00:03<00:05, 394MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 39% 1.38G/3.55G [00:03<00:06, 387MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 40% 1.42G/3.55G [00:03<00:05, 395MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 41% 1.46G/3.55G [00:03<00:05, 404MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 42% 1.50G/3.55G [00:04<00:05, 400MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 43% 1.54G/3.55G [00:04<00:05, 400MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 44% 1.58G/3.55G [00:04<00:05, 400MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 46% 1.62G/3.55G [00:04<00:05, 407MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 47% 1.65G/3.55G [00:04<00:04, 408MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 48% 1.69G/3.55G [00:04<00:04, 401MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 49% 1.73G/3.55G [00:04<00:04, 398MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 50% 1.77G/3.55G [00:04<00:04, 399MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 51% 1.80G/3.55G [00:04<00:04, 402MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 52% 1.84G/3.55G [00:04<00:04, 403MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 53% 1.88G/3.55G [00:05<00:04, 399MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 54% 1.92G/3.55G [00:05<00:04, 400MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 55% 1.96G/3.55G [00:05<00:04, 403MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 56% 2.00G/3.55G [00:05<00:04, 405MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 57% 2.03G/3.55G [00:05<00:03, 406MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 58% 2.07G/3.55G [00:05<00:03, 406MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 60% 2.11G/3.55G [00:05<00:03, 404MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 61% 2.15G/3.55G [00:05<00:03, 405MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 62% 2.19G/3.55G [00:05<00:03, 406MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 63% 2.23G/3.55G [00:05<00:03, 413MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 64% 2.27G/3.55G [00:06<00:03, 413MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 65% 2.30G/3.55G [00:06<00:03, 407MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 66% 2.34G/3.55G [00:06<00:03, 406MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 67% 2.38G/3.55G [00:06<00:03, 408MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 68% 2.42G/3.55G [00:06<00:02, 413MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 69% 2.46G/3.55G [00:06<00:02, 416MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 70% 2.50G/3.55G [00:06<00:02, 413MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 72% 2.54G/3.55G [00:06<00:02, 412MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 73% 2.57G/3.55G [00:06<00:02, 392MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 73% 2.59G/3.55G [00:07<00:03, 331MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 74% 2.61G/3.55G [00:07<00:03, 298MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 74% 2.63G/3.55G [00:07<00:03, 269MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 75% 2.65G/3.55G [00:07<00:03, 251MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 75% 2.68G/3.55G [00:07<00:03, 240MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 76% 2.70G/3.55G [00:07<00:03, 242MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 77% 2.72G/3.55G [00:07<00:03, 247MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 78% 2.75G/3.55G [00:07<00:03, 260MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 79% 2.79G/3.55G [00:07<00:02, 284MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 79% 2.82G/3.55G [00:07<00:02, 301MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 80% 2.85G/3.55G [00:08<00:02, 309MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 81% 2.88G/3.55G [00:08<00:02, 332MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 82% 2.92G/3.55G [00:08<00:02, 330MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 83% 2.94G/3.55G [00:08<00:02, 298MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 84% 2.96G/3.55G [00:08<00:02, 287MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 85% 3.00G/3.55G [00:08<00:01, 304MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 85% 3.02G/3.55G [00:08<00:01, 291MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 86% 3.06G/3.55G [00:08<00:01, 312MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 87% 3.10G/3.55G [00:08<00:01, 340MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 88% 3.13G/3.55G [00:08<00:01, 339MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 89% 3.17G/3.55G [00:09<00:01, 359MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 90% 3.20G/3.55G [00:09<00:01, 364MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 91% 3.24G/3.55G [00:09<00:00, 375MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 92% 3.27G/3.55G [00:09<00:00, 377MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 93% 3.30G/3.55G [00:09<00:00, 360MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 94% 3.34G/3.55G [00:09<00:00, 371MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 95% 3.38G/3.55G [00:09<00:00, 378MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 96% 3.41G/3.55G [00:09<00:00, 368MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 97% 3.45G/3.55G [00:09<00:00, 376MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 98% 3.48G/3.55G [00:09<00:00, 379MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 99% 3.52G/3.55G [00:10<00:00, 389MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "                                                   \u001b[A\u001b[A\n",
            "Adding .venv.tar.gz to cache: 100% 1/1 [00:10<00:00, 10.16s/file{'info': ''}]\u001b[A\n",
            "                                                                             \u001b[A\n",
            "Checking out /content/GPT-NeoX-Colab/.venv.tar.gz:   0% 0/1 [00:00<?, ?files/s]\u001b[A\n",
            "  0% 0/1 [00:00<?, ?files/s{'info': ''}]                                       \u001b[A\n",
            "\n",
            "  0% 0.00/3.55G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "  0% 0.00/3.55G [00:00<?, ?B/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  2% 81.0M/3.55G [00:00<00:04, 843MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  4% 160M/3.55G [00:00<00:04, 833MB/s{'info': ''}] \u001b[A\u001b[A\n",
            "\n",
            "  7% 238M/3.55G [00:00<00:04, 823MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "  9% 319M/3.55G [00:00<00:04, 830MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 11% 396M/3.55G [00:00<00:04, 818MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 13% 471M/3.55G [00:00<00:04, 807MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 15% 543M/3.55G [00:00<00:04, 788MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 17% 629M/3.55G [00:00<00:03, 821MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 19% 703M/3.55G [00:00<00:03, 805MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 22% 782M/3.55G [00:01<00:03, 812MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 24% 864M/3.55G [00:01<00:03, 823MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 26% 926M/3.55G [00:01<00:03, 711MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 26% 957M/3.55G [00:01<00:04, 598MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 27% 995M/3.55G [00:01<00:05, 539MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 29% 1.01G/3.55G [00:01<00:05, 503MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 30% 1.05G/3.55G [00:01<00:05, 473MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 30% 1.06G/3.55G [00:02<00:21, 126MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 30% 1.06G/3.55G [00:02<00:36, 73.0MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 30% 1.06G/3.55G [00:03<00:57, 46.2MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 30% 1.06G/3.55G [00:03<01:13, 36.2MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 31% 1.10G/3.55G [00:03<00:34, 76.5MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 32% 1.14G/3.55G [00:03<00:21, 123MB/s{'info': ''}] \u001b[A\u001b[A\n",
            "\n",
            " 33% 1.18G/3.55G [00:04<00:14, 170MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 35% 1.22G/3.55G [00:04<00:11, 216MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 36% 1.26G/3.55G [00:04<00:09, 259MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 37% 1.30G/3.55G [00:04<00:08, 294MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 38% 1.34G/3.55G [00:04<00:07, 324MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 39% 1.38G/3.55G [00:04<00:06, 348MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 40% 1.42G/3.55G [00:04<00:06, 368MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 41% 1.46G/3.55G [00:04<00:05, 381MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 42% 1.50G/3.55G [00:04<00:05, 393MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 43% 1.54G/3.55G [00:04<00:05, 398MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 45% 1.58G/3.55G [00:05<00:05, 402MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 46% 1.62G/3.55G [00:05<00:05, 400MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 47% 1.65G/3.55G [00:05<00:05, 402MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 49% 1.72G/3.55G [00:05<00:03, 499MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 50% 1.79G/3.55G [00:05<00:03, 550MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 51% 1.83G/3.55G [00:05<00:03, 512MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 53% 1.86G/3.55G [00:05<00:03, 478MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 54% 1.90G/3.55G [00:05<00:03, 460MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 55% 1.94G/3.55G [00:05<00:03, 442MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 56% 1.98G/3.55G [00:05<00:03, 437MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 57% 2.02G/3.55G [00:06<00:03, 429MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 58% 2.06G/3.55G [00:06<00:03, 424MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 59% 2.10G/3.55G [00:06<00:03, 417MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 60% 2.13G/3.55G [00:06<00:03, 413MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 61% 2.17G/3.55G [00:06<00:03, 414MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 62% 2.21G/3.55G [00:06<00:03, 409MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 63% 2.25G/3.55G [00:06<00:03, 406MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 64% 2.29G/3.55G [00:06<00:03, 402MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 66% 2.33G/3.55G [00:06<00:03, 405MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 67% 2.36G/3.55G [00:06<00:03, 406MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 68% 2.40G/3.55G [00:07<00:03, 408MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 69% 2.44G/3.55G [00:07<00:02, 409MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 70% 2.48G/3.55G [00:07<00:02, 406MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 71% 2.52G/3.55G [00:07<00:02, 406MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 72% 2.56G/3.55G [00:07<00:02, 408MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 73% 2.60G/3.55G [00:07<00:02, 412MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 74% 2.63G/3.55G [00:07<00:02, 396MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 75% 2.65G/3.55G [00:07<00:02, 354MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 76% 2.68G/3.55G [00:07<00:02, 323MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 76% 2.70G/3.55G [00:07<00:02, 306MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 77% 2.73G/3.55G [00:08<00:02, 296MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 78% 2.75G/3.55G [00:08<00:03, 278MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 78% 2.78G/3.55G [00:08<00:02, 275MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 79% 2.80G/3.55G [00:08<00:02, 270MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 80% 2.83G/3.55G [00:08<00:02, 266MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 80% 2.85G/3.55G [00:08<00:02, 266MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 81% 2.88G/3.55G [00:08<00:02, 267MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 82% 2.90G/3.55G [00:08<00:02, 262MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 83% 2.93G/3.55G [00:08<00:02, 264MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 83% 2.96G/3.55G [00:09<00:02, 263MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 84% 2.98G/3.55G [00:09<00:02, 264MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 85% 3.01G/3.55G [00:09<00:02, 272MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 86% 3.04G/3.55G [00:09<00:01, 277MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 86% 3.06G/3.55G [00:09<00:01, 281MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 87% 3.09G/3.55G [00:09<00:01, 278MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 88% 3.12G/3.55G [00:09<00:01, 286MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 89% 3.14G/3.55G [00:09<00:01, 284MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 89% 3.17G/3.55G [00:09<00:01, 283MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 90% 3.20G/3.55G [00:09<00:01, 297MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 91% 3.24G/3.55G [00:10<00:00, 335MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 93% 3.28G/3.55G [00:10<00:00, 360MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 94% 3.32G/3.55G [00:10<00:00, 374MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 95% 3.36G/3.55G [00:10<00:00, 380MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 96% 3.40G/3.55G [00:10<00:00, 393MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 97% 3.44G/3.55G [00:10<00:00, 403MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            " 99% 3.51G/3.55G [00:10<00:00, 515MB/s{'info': ''}]\u001b[A\u001b[A\n",
            "\n",
            "                                                   \u001b[A\u001b[A\n",
            "100% 1/1 [00:11<00:00, 11.47s/files{'info': ''}]\u001b[A\n",
            "Adding...: 100% 1/1 [00:32<00:00, 32.01s/file{'info': ''}]\n",
            "\n",
            "To track the changes with git, run:\n",
            "\n",
            "\tgit add .venv.tar.gz.dvc\n",
            "\n",
            "To enable auto staging, run:\n",
            "\n",
            "\tdvc config core.autostage true\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}