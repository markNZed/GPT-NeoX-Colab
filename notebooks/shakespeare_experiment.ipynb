{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/shakespeare_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flp2Dht6ytqE"
   },
   "source": [
    "# Experiment\n",
    "This is a demonstration of how experiments can be run using DagsHub and MLflow.\n",
    "We will train three different versions of the tiny LLM using different batch sizes and compare the results.\n",
    "\n",
    "## ToDo\n",
    "- Shorten the training time for testing\n",
    "- Run tests in parallel\n",
    "- Extract functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LECzxKWK8CyS"
   },
   "source": [
    "## Login to Dagshub\n",
    "To avoid requirest in the middle of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxd2wfpq1F9u",
    "outputId": "52f72a4a-4163-44f0-f5e1-951f1c23aeed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/251.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/139.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%pip install -q dagshub\n",
    "import dagshub\n",
    "try:\n",
    "  from google.colab import userdata\n",
    "  os.environ[\"DAGSHUB_USER_TOKEN\"] = userdata.get(\"DAGSHUB_USER_TOKEN\")\n",
    "except:\n",
    "  pass\n",
    "try:\n",
    "  if os.environ[\"DAGSHUB_USER_TOKEN\"]:\n",
    "    pass\n",
    "except:\n",
    "  os.environ[\"DAGSHUB_USER_TOKEN\"] = dagshub.auth.get_token()\n",
    "dagshub.auth.add_app_token(token=os.environ[\"DAGSHUB_USER_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Iega62GjytqH"
   },
   "outputs": [],
   "source": [
    "#@title Setup paths\n",
    "# We could modify these paths to \"stub\" behavior for test/dev\n",
    "# A file like .ipython/profile_default/startup/10-test.py could restore these vars\n",
    "workspaceDir = \"/content\"\n",
    "GPTNeoXDirName = \"gpt-neox\"\n",
    "GPTNeoXDir = f\"{workspaceDir}/{GPTNeoXDirName}\"\n",
    "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
    "GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUxVImVvytqL",
    "outputId": "e29ce565-da87-4483-b1b5-29e83a70acdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "fatal: destination path 'GPT-NeoX-Colab' already exists and is not an empty directory.\n",
      "/content/GPT-NeoX-Colab\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for GPTNeoXColab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Data retrieval successful.\n",
      "Data retrieval successful.\n",
      "CPU times: user 111 ms, sys: 17.4 ms, total: 128 ms\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#@title Clone GPT-NeoX-Colab\n",
    "%cd {workspaceDir}\n",
    "# Don't use --depth 1 because that does not play nice with git-annex\n",
    "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
    "%cd {GPTNeoXColabDir}\n",
    "%pip install -q -r requirements_colab.txt\n",
    "%pip install -q .\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
    "import GPTNeoXColab\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.bin\")\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.idx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvpVaIbLytqM"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#@title Clone GPT-NeoX\n",
    "%cd {workspaceDir}\n",
    "#!git clone --depth 1 https://github.com/EleutherAI/gpt-neox\n",
    "!git clone -b pipe_parallel_size_1 --depth 1 https://github.com/markNZed/gpt-neox.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6EJBRBSpiSXF"
   },
   "outputs": [],
   "source": [
    "!mkdir -p {GPTNeoXDir}/processed_data\n",
    "!cp {GPTNeoXColabDir}/data/shakespeare/shakespeare_text_document.* {GPTNeoXDir}/processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WH8eetUBytqN",
    "outputId": "2e975e84-e3a1-4c9d-a988-0a6dfcde88a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Downloading my_env.tar.gz\n",
      "Unzipping my_env.tar.gz\n",
      "Untarring my_env.tar.gz\n",
      "CPU times: user 19.2 s, sys: 8.33 s, total: 27.5 s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#@title Load prebuilt Python environment for Colab\n",
    "import GPTNeoXColab\n",
    "%cd {workspaceDir}\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GPTNeoXColab.utils.colab.download_my_env()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTyERIkj8elu"
   },
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RmiyhUgSSt7",
    "outputId": "9bbdd4d5-cc82-4da7-8d09-ceb7067cdb01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
      "Collecting pynvml\n",
      "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
      "Downloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pynvml\n",
      "Successfully installed pynvml-11.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil\n",
    "# Install this for GPU metric logging\n",
    "!pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "umfYbg3TytqJ"
   },
   "outputs": [],
   "source": [
    "import GPTNeoXColab\n",
    "import os\n",
    "from pathlib import Path\n",
    "ROOT_DIR = GPTNeoXColab.utils.colab.find_project_root()\n",
    "RELATIVE_ROOT_DIR = os.path.relpath(ROOT_DIR, Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6PTFQWG08tKv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# File to store the last read position (persistence between script runs)\n",
    "file_position = 0\n",
    "# Regular expression to match \"iteration <number> / <total>\"\n",
    "iteration_pattern = re.compile(r\"iteration\\s+(\\d+)\\s*/\\s*\\d+\")\n",
    "\n",
    "def get_latest_file(dir, pattern = \"*_stdout.txt\"):\n",
    "  # Define the log directory and pattern for log files\n",
    "  glob_pattern = os.path.join(dir, pattern)\n",
    "  # Get the list of log files that match the pattern\n",
    "  files = glob.glob(glob_pattern)\n",
    "  # Ensure there are log files in the directory\n",
    "  if files:\n",
    "      # Find the latest log file based on modification time\n",
    "      file = max(files, key=os.path.getmtime)\n",
    "      print(\"Latest file:\", file)\n",
    "  else:\n",
    "      file = None\n",
    "      print(\"No files found. Waiting and retrying.\")\n",
    "      time.sleep(10)  # Check every X seconds\n",
    "      file = get_latest_file(dir, pattern)\n",
    "  return file\n",
    "\n",
    "def read_new_iterations(latest_log):\n",
    "    global file_position\n",
    "    # Open the log file and seek to the last position\n",
    "    with open(latest_log, \"r\") as file:\n",
    "        file.seek(file_position)\n",
    "        # Read new lines\n",
    "        new_lines = file.readlines()\n",
    "        file_position = file.tell()\n",
    "        # Process lines containing \"iteration\"\n",
    "        last_match = None\n",
    "        for line in new_lines:\n",
    "            match = iteration_pattern.search(line)\n",
    "            if match:\n",
    "                last_match = match\n",
    "        if last_match:\n",
    "            # Extract the iteration count from the regex match\n",
    "            iteration_count = int(last_match.group(1))\n",
    "            print(f\"{iteration_count} iterations\")\n",
    "\n",
    "# Function to check if the process is running\n",
    "def is_process_running(pid):\n",
    "    try:\n",
    "        os.kill(pid, 0)  # Sending signal 0 to check if the process exists\n",
    "        return True\n",
    "    except OSError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XtEiZxZBV2iD"
   },
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import os\n",
    "\n",
    "def get_scalar_from_tensorboard(file, key):\n",
    "    # Load TensorBoard events\n",
    "    event_acc = EventAccumulator(file)\n",
    "    event_acc.Reload()\n",
    "    print(event_acc.Tags())\n",
    "    # Extract loss scalar events\n",
    "    if key in event_acc.Tags().get('scalars', []):\n",
    "        events = event_acc.Scalars(key)\n",
    "        value = events[-1].value  # Get the last logged value\n",
    "        return value\n",
    "    else:\n",
    "        print(f\"{key} not found in TensorBoard logs.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install GitPython\n",
    "%pip install ipynbname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "\n",
    "repo = Repo(GPTNeoXColabDir)\n",
    "commit_id = repo.head.commit.hexsha\n",
    "branch_name = repo.active_branch.name\n",
    "repo_url = next(repo.remotes.origin.urls)\n",
    "print(f\"Commit ID: {commit_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iaBFYofFytqP",
    "outputId": "a15f017b-aca7-40f2-d52d-e2e48050b490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gpt-neox\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"MarkNZed/GPT-NeoX-Colab\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"MarkNZed/GPT-NeoX-Colab\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository MarkNZed/GPT-NeoX-Colab initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository MarkNZed/GPT-NeoX-Colab initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve notebook name: name 'ipynbname' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'commit_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 129\u001b[0m\n\u001b[1;32m    127\u001b[0m client \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mtracking\u001b[38;5;241m.\u001b[39mMlflowClient()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# https://mlflow.org/docs/latest/tracking/tracking-api.html#system-tags\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m client\u001b[38;5;241m.\u001b[39mset_tag(exp_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlflow.source.git.commit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcommit_id\u001b[49m)\n\u001b[1;32m    130\u001b[0m client\u001b[38;5;241m.\u001b[39mset_tag(exp_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlflow.source.git.branch\u001b[39m\u001b[38;5;124m\"\u001b[39m, branch_name)\n\u001b[1;32m    131\u001b[0m client\u001b[38;5;241m.\u001b[39mset_tag(exp_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlflow.source.git.repoURL\u001b[39m\u001b[38;5;124m\"\u001b[39m, repo_url)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'commit_id' is not defined"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from hydra import initialize_config_dir, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import mlflow\n",
    "import time\n",
    "import dagshub\n",
    "import ipynbname\n",
    "\n",
    "%cd {GPTNeoXDir}\n",
    "\n",
    "dagshub.init(repo_owner='MarkNZed', repo_name='GPT-NeoX-Colab', mlflow=True)\n",
    "experiment_group = \"Added System Metrics\"\n",
    "mlflow.set_experiment(experiment_group)\n",
    "mlflow.enable_system_metrics_logging()\n",
    "\n",
    "def load_and_merge_configs(base_conf_dir, experiment_name):\n",
    "    # Initialize Hydra with the base config directory\n",
    "    initialize_config_dir(config_dir=base_conf_dir, version_base=\"1.1\")\n",
    "\n",
    "    # Load the base configurations (shakespeare and shakespeare_deepy) and experiment overrides\n",
    "    base_cfg = compose(config_name=\"shakespeare.yml\")\n",
    "    OmegaConf.set_struct(base_cfg, False) # No struct checking for matching structure in merge\n",
    "    deepy_cfg = compose(config_name=\"shakespeare_deepy.yml\")\n",
    "    OmegaConf.set_struct(deepy_cfg, False) # No struct checking for matching structure in merge\n",
    "    experiment_cfg = compose(config_name=\"hydra\", overrides=[f\"experiments={experiment_name}\"])\n",
    "    OmegaConf.set_struct(experiment_cfg, False) # No struct checking for matching structure in merge\n",
    "\n",
    "    mlflow.log_params(OmegaConf.to_container(experiment_cfg, resolve=True))\n",
    "\n",
    "    experiment_overrides = experiment_cfg.get(\"experiments\", {})\n",
    "    OmegaConf.set_struct(experiment_overrides, False) # No struct checking for matching structure in merge\n",
    "\n",
    "    print(experiment_overrides)\n",
    "\n",
    "    # Merge the configurations: base -> deepy -> experiment\n",
    "    cfg = OmegaConf.merge(base_cfg, deepy_cfg, experiment_overrides)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "def run_experiment(cfg, experiment_name):\n",
    "    print(\"Running experiment:\", experiment_name)\n",
    "    experimentDir = f\"{GPTNeoXDir}/experiments/{experiment_name}\"\n",
    "    !sudo rm -rf {experimentDir}\n",
    "    !mkdir -p {experimentDir}\n",
    "    !rm -f train_process.pid\n",
    "    #print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "    # Create a temporary directory for configs\n",
    "    temp_config_dir = tempfile.mkdtemp()\n",
    "    temp_config_file = os.path.join(temp_config_dir, 'temp_config.yml')\n",
    "\n",
    "    # Save the modified config to the temporary file in JSON-like structure within a YAML file\n",
    "    with open(temp_config_file, 'w') as f:\n",
    "        # Dump the config as JSON but save it with a .yml extension\n",
    "        OmegaConf.save(OmegaConf.create(OmegaConf.to_container(cfg, resolve=True)), f)\n",
    "\n",
    "    # Start a detached background process using the temp config\n",
    "    cmd = f\"\"\"nohup bash -c \"source {workspaceDir}/my_env/bin/activate && \\\n",
    "        cd {GPTNeoXDir} && \\\n",
    "        python ./deepy.py train.py --conf_dir {temp_config_dir} \\\n",
    "        temp_config\" & echo $! > train_process.pid\"\"\"\n",
    "    print(\"Running command:\", cmd)\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        shell=True,\n",
    "        executable='/bin/bash',\n",
    "        preexec_fn=os.setsid  # Starts the process in a new session\n",
    "    )\n",
    "\n",
    "    print(\"Training initiated.\")\n",
    "\n",
    "    while not os.path.exists(\"train_process.pid\"):\n",
    "        print(\"Waiting for train_process.pid to be created...\")\n",
    "        time.sleep(10)  # Check every X seconds\n",
    "\n",
    "    # Read the PID from the file\n",
    "    with open(\"train_process.pid\", \"r\") as f:\n",
    "        pid = int(f.read().strip())\n",
    "        print(\"Found train_process.pid \", pid)\n",
    "\n",
    "    while not os.path.exists(f\"{experimentDir}/logs\"):\n",
    "        print(\"Waiting for logs to be created...\")\n",
    "        time.sleep(10)  # Check every X seconds\n",
    "\n",
    "    latest_log = get_latest_file(f\"{experimentDir}/logs\", \"*_stdout.txt\")\n",
    "\n",
    "    # Monitor the training process\n",
    "    while is_process_running(pid):\n",
    "        read_new_iterations(latest_log)\n",
    "        print(\"Training is still running...\")\n",
    "        time.sleep(30)  # Check every X seconds\n",
    "\n",
    "    print(\"Training has finished.\")\n",
    "\n",
    "    latest_events_file = get_latest_file(f\"{experimentDir}/tensorboard\", \"events.out.tfevents.*\")\n",
    "    loss_key = \"test/lm_loss\"\n",
    "    loss = get_scalar_from_tensorboard(latest_events_file, loss_key)\n",
    "    print(f\"Logging metric {loss_key} {loss}\")\n",
    "    mlflow.log_metric(loss_key, loss)\n",
    "\n",
    "    # Clean up the temporary directory after training\n",
    "    # (Optional: You might want to keep it for debugging)\n",
    "    # shutil.rmtree(temp_config_dir)\n",
    "\n",
    "notebook_path = ipynbname.path()\n",
    "\n",
    "# List of experiment names\n",
    "experiments = [\"experiment1\", \"experiment2\", \"experiment3\"]\n",
    "\n",
    "for experiment in experiments:\n",
    "\n",
    "    exp_id = GPTNeoXColab.utils.ml.get_or_create_experiment_id(experiment_group)\n",
    "\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    # https://mlflow.org/docs/latest/tracking/tracking-api.html#system-tags\n",
    "    client.set_tag(exp_id, \"mlflow.source.git.commit\", commit_id)\n",
    "    client.set_tag(exp_id, \"mlflow.source.git.branch\", branch_name)\n",
    "    client.set_tag(exp_id, \"mlflow.source.git.repoURL\", repo_url)\n",
    "    client.set_tag(exp_id, \"mlflow.source.type\", \"NOTEBOOK\")\n",
    "    client.set_tag(exp_id, \"mlflow.source.name\", notebook_path)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # Clear Hydra's global state if it’s already initialized\n",
    "        if GlobalHydra.instance().is_initialized():\n",
    "            GlobalHydra.instance().clear()\n",
    "        # Load and merge configurations\n",
    "        base_conf_dir = f\"{GPTNeoXColabDir}/configs\"\n",
    "        cfg = load_and_merge_configs(base_conf_dir, experiment)\n",
    "        # Start training with the merged configuration\n",
    "        run_experiment(cfg, experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/shakespeare_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "This is a demonstration of how experiments can be run using DagsHub and MLflow.\n",
    "We will train three different versions of the tiny SLM using different batch sizes and compare the results.\n",
    "\n",
    "## ToDo\n",
    "- Shorten the training time for testing\n",
    "- Run tests in parallel\n",
    "- Extract functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to Dagshub\n",
    "To avoid requiring a login in the middle of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pip install -q dagshub\n",
    "import dagshub\n",
    "try:\n",
    "  from google.colab import userdata\n",
    "  os.environ[\"DAGSHUB_USER_TOKEN\"] = userdata.get(\"DAGSHUB_USER_TOKEN\")\n",
    "except:\n",
    "  pass\n",
    "try:\n",
    "  if os.environ[\"DAGSHUB_USER_TOKEN\"]:\n",
    "    pass\n",
    "except:\n",
    "  os.environ[\"DAGSHUB_USER_TOKEN\"] = dagshub.auth.get_token()\n",
    "dagshub.auth.add_app_token(token=os.environ[\"DAGSHUB_USER_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup paths\n",
    "# We could modify these paths to \"stub\" behavior for test/dev\n",
    "# A file like .ipython/profile_default/startup/10-test.py could restore these vars\n",
    "workspaceDir = \"/content\"\n",
    "GPTNeoXDirName = \"gpt-neox\"\n",
    "GPTNeoXDir = f\"{workspaceDir}/{GPTNeoXDirName}\"\n",
    "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
    "GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#@title Clone GPT-NeoX-Colab\n",
    "%cd {workspaceDir}\n",
    "# Don't use --depth 1 because that does not play nice with git-annex\n",
    "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
    "%cd {GPTNeoXColabDir}\n",
    "%pip install -q -r requirements_colab.txt\n",
    "%pip install -q .\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
    "import GPTNeoXColab\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.bin\")\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.idx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#@title Clone GPT-NeoX\n",
    "%cd {workspaceDir}\n",
    "!git clone --depth 1 https://github.com/EleutherAI/gpt-neox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {GPTNeoXDir}/processed_data\n",
    "!cp {GPTNeoXColabDir}/data/shakespeare/shakespeare_text_document.* {GPTNeoXDir}/processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#@title Load prebuilt Python environment for Colab\n",
    "import GPTNeoXColab\n",
    "%cd {workspaceDir}\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GPTNeoXColab.utils.colab.download_my_env()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPTNeoXColab\n",
    "import os\n",
    "from pathlib import Path\n",
    "ROOT_DIR = GPTNeoXColab.utils.colab.find_project_root()\n",
    "RELATIVE_ROOT_DIR = os.path.relpath(ROOT_DIR, Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# File to store the last read position (persistence between script runs)\n",
    "file_position = 0\n",
    "# Regular expression to match \"iteration <number> / <total>\"\n",
    "iteration_pattern = re.compile(r\"iteration\\s+(\\d+)\\s*/\\s*\\d+\")\n",
    "\n",
    "def read_new_iterations():\n",
    "    global file_position\n",
    "    # Open the log file and seek to the last position\n",
    "    with open(latest_log, \"r\") as file:\n",
    "        file.seek(file_position)\n",
    "        # Read new lines\n",
    "        new_lines = file.readlines()\n",
    "        file_position = file.tell()\n",
    "        # Process lines containing \"iteration\"\n",
    "        last_match = None\n",
    "        for line in new_lines:\n",
    "            match = iteration_pattern.search(line)\n",
    "            if match:\n",
    "                last_match = match\n",
    "        if last_match:\n",
    "            # Extract the iteration count from the regex match\n",
    "            iteration_count = int(last_match.group(1))\n",
    "            print(f\"{iteration_count} iterations\")\n",
    "\n",
    "# Function to check if the process is running\n",
    "def is_process_running(pid):\n",
    "    try:\n",
    "        os.kill(pid, 0)  # Sending signal 0 to check if the process exists\n",
    "        return True\n",
    "    except OSError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from hydra import initialize_config_dir, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import mlflow\n",
    "import time\n",
    "import dagshub\n",
    "\n",
    "%cd {GPTNeoXDir}\n",
    "\n",
    "dagshub.init(repo_owner='MarkNZed', repo_name='GPT-NeoX-Colab', mlflow=True)\n",
    "experiment_group = \"Testing2\"\n",
    "mlflow.set_experiment(experiment_group)\n",
    "\n",
    "def load_and_merge_configs(base_conf_dir, experiment_name):\n",
    "    # Initialize Hydra with the base config directory\n",
    "    initialize_config_dir(config_dir=base_conf_dir, version_base=\"1.1\")\n",
    "\n",
    "    # Load the base configurations (shakespeare and shakespeare_deepy) and experiment overrides\n",
    "    base_cfg = compose(config_name=\"shakespeare.yml\")\n",
    "    OmegaConf.set_struct(base_cfg, False) # No struct checking for matching structure in merge\n",
    "    deepy_cfg = compose(config_name=\"shakespeare_deepy.yml\")\n",
    "    OmegaConf.set_struct(deepy_cfg, False) # No struct checking for matching structure in merge\n",
    "    experiment_cfg = compose(config_name=\"hydra\", overrides=[f\"experiments={experiment_name}\"])\n",
    "    OmegaConf.set_struct(experiment_cfg, False) # No struct checking for matching structure in merge\n",
    "\n",
    "    experiment_overrides = experiment_cfg.get(\"experiments\", {})\n",
    "    OmegaConf.set_struct(experiment_overrides, False) # No struct checking for matching structure in merge\n",
    "\n",
    "    print(experiment_overrides)\n",
    "\n",
    "    # Merge the configurations: base -> deepy -> experiment\n",
    "    cfg = OmegaConf.merge(base_cfg, deepy_cfg, experiment_overrides)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "def run_experiment(cfg, experiment_name):\n",
    "    print(\"Running experiment:\", experiment_name)\n",
    "    experimentDir = f\"{GPTNeoXDir}/experiments/{experiment_name}\"\n",
    "    !mkdir -p {experimentDir}\n",
    "    #print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params(OmegaConf.to_container(cfg, resolve=True))\n",
    "\n",
    "    # Create a temporary directory for configs\n",
    "    temp_config_dir = tempfile.mkdtemp()\n",
    "    temp_config_file = os.path.join(temp_config_dir, 'temp_config.yml')\n",
    "\n",
    "    # Save the modified config to the temporary file in JSON-like structure within a YAML file\n",
    "    with open(temp_config_file, 'w') as f:\n",
    "        # Dump the config as JSON but save it with a .yml extension\n",
    "        OmegaConf.save(OmegaConf.create(OmegaConf.to_container(cfg, resolve=True)), f)\n",
    "\n",
    "    # Start a detached background process using the temp config\n",
    "    cmd = f\"\"\"nohup bash -c \"source {workspaceDir}/my_env/bin/activate && \\\n",
    "        cd {GPTNeoXDir} && \\\n",
    "        python ./deepy.py train.py --conf_dir {temp_config_dir} \\\n",
    "        temp_config\" & echo $! > train_process.pid\"\"\"\n",
    "    print(\"Running command:\", cmd)\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        shell=True,\n",
    "        executable='/bin/bash',\n",
    "        preexec_fn=os.setsid  # Starts the process in a new session\n",
    "    )\n",
    "\n",
    "    print(\"Training initiated.\")\n",
    "\n",
    "    while not os.path.exists(\"train_process.pid\"):\n",
    "        print(\"Waiting for train_process.pid to be created...\")\n",
    "        time.sleep(10)  # Check every X seconds\n",
    "\n",
    "    # Read the PID from the file\n",
    "    with open(\"train_process.pid\", \"r\") as f:\n",
    "        pid = int(f.read().strip())\n",
    "        print(\"PID:\", pid)\n",
    "\n",
    "    # Monitor the training process\n",
    "    while is_process_running(pid):\n",
    "        read_new_iterations()\n",
    "        print(\"Training is still running...\")\n",
    "        time.sleep(30)  # Check every X seconds\n",
    "\n",
    "    print(\"Training has finished.\")\n",
    "\n",
    "    !rm \"train_process.pid\"\n",
    "\n",
    "    # Clean up the temporary directory after training\n",
    "    # (Optional: You might want to keep it for debugging)\n",
    "    # shutil.rmtree(temp_config_dir)\n",
    "\n",
    "# List of experiment names\n",
    "experiments = [\"experiment1\", \"experiment2\", \"experiment3\"]\n",
    "\n",
    "for experiment in experiments:\n",
    "\n",
    "    exp_id = GPTNeoXColab.utils.ml.get_or_create_experiment_id(experiment_group)\n",
    "    # Set the experiment name and start the run\n",
    "    #with mlflow.start_run(experiment_id=exp_id, nested=True):\n",
    "    with mlflow.start_run(nested=True):\n",
    "    #with mlflow.start_run():\n",
    "        # Clear Hydra's global state if it’s already initialized\n",
    "        if GlobalHydra.instance().is_initialized():\n",
    "            GlobalHydra.instance().clear()\n",
    "        # Load and merge configurations\n",
    "        base_conf_dir = f\"{GPTNeoXColabDir}/configs\"\n",
    "        cfg = load_and_merge_configs(base_conf_dir, experiment)\n",
    "        # Start training with the merged configuration\n",
    "        run_experiment(cfg, experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Wait until logs directory is created\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Path to the log directory\n",
    "logs_dir = f\"{GPTNeoXDir}/logs\"\n",
    "\n",
    "# Wait for the directory to be created\n",
    "while not os.path.exists(logs_dir):\n",
    "    print(\"Waiting for logs directory to be created...\")\n",
    "    time.sleep(10)  # Check every X seconds\n",
    "\n",
    "print(\"logs directory found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Find the latest log file\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the log directory and pattern for log files\n",
    "log_dir = f\"{GPTNeoXDir}/logs\"\n",
    "log_pattern = os.path.join(log_dir, \"*_stdout.txt\")\n",
    "\n",
    "# Get the list of log files that match the pattern\n",
    "log_files = glob.glob(log_pattern)\n",
    "\n",
    "# Ensure there are log files in the directory\n",
    "if log_files:\n",
    "    # Find the latest log file based on modification time\n",
    "    latest_log = max(log_files, key=os.path.getmtime)\n",
    "    print(\"Latest log file:\", latest_log)\n",
    "else:\n",
    "    latest_log = None\n",
    "    print(\"No log files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we could disconnect from the GPU resource\n",
    "#from google.colab import runtime\n",
    "#runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JGqO-azytqR"
   },
   "outputs": [],
   "source": [
    "# Here we could disconnect from the GPU resource\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
