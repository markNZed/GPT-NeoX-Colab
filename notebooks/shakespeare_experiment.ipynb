{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/shakespeare_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flp2Dht6ytqE"
      },
      "source": [
        "# Experiment\n",
        "This is a demonstration of how experiments can be run using DagsHub and MLflow.\n",
        "We will train three different versions of the tiny LLM using different batch sizes and compare the results.\n",
        "\n",
        "## ToDo\n",
        "- Shorten the training time for testing\n",
        "- Run tests in parallel\n",
        "- Extract functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LECzxKWK8CyS"
      },
      "source": [
        "## Login to Dagshub\n",
        "To avoid requirest in the middle of the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wxd2wfpq1F9u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "#os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = userdata.get(\"MLFLOW_TRACKING_PASSWORD\")\n",
        "#os.environ[\"MLFLOW_TRACKING_USERNAME\"] = userdata.get(\"MLFLOW_TRACKING_USERNAME\")\n",
        "os.environ[\"DAGSHUB_USER_TOKEN\"] = userdata.get(\"DAGSHUB_USER_TOKEN\")\n",
        "if not os.environ[\"DAGSHUB_USER_TOKEN\"]:\n",
        "  dagshub.auth.get_token()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Iega62GjytqH"
      },
      "outputs": [],
      "source": [
        "#@title Setup paths\n",
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "# Use functions so if the Notebook is restarted we do not lose the values\n",
        "def workspaceDir():\n",
        "    return \"/content\"\n",
        "def GPTNeoXDirName():\n",
        "    return \"gpt-neox\"\n",
        "def GPTNeoXDir():\n",
        "    return f\"{workspaceDir()}/{GPTNeoXDirName()}\"\n",
        "def GPTNeoXColabDirName():\n",
        "    return \"GPT-NeoX-Colab\"\n",
        "def GPTNeoXColabDir():\n",
        "    return f\"{workspaceDir()}/{GPTNeoXColabDirName()}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUxVImVvytqL",
        "outputId": "3d11e700-8ea7-474e-db67-07a4259a2043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'GPT-NeoX-Colab'...\n",
            "remote: Enumerating objects: 1129, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 1129 (delta 35), reused 41 (delta 19), pack-reused 1052 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1129/1129), 13.78 MiB | 10.22 MiB/s, done.\n",
            "Resolving deltas: 100% (612/612), done.\n",
            "/content/GPT-NeoX-Colab\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.5/251.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.0/426.0 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.5/456.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.6/367.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.4/901.4 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.7/548.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.1/569.1 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.1/113.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.2/722.2 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for configobj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pysftp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for GPTNeoXColab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Data retrieval successful.\n",
            "Data retrieval successful.\n",
            "CPU times: user 6.98 s, sys: 851 ms, total: 7.84 s\n",
            "Wall time: 2min 45s\n"
          ]
        }
      ],
      "source": [
        "#@title Clone GPT-NeoX-Colab\n",
        "%%time\n",
        "%cd {workspaceDir()}\n",
        "# Don't use --depth 1 because that does not play nice with git-annex\n",
        "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "%cd {GPTNeoXColabDir()}\n",
        "%pip install -q -r requirements_colab.txt\n",
        "%pip install -q .\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(f\"{GPTNeoXColabDir()}/.env\")\n",
        "import GPTNeoXColab\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.bin\")\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.idx\")\n",
        "!mkdir -p {GPTNeoXDir()}/processed_data\n",
        "!cp {GPTNeoXColabDir()}/data/shakespeare/shakespeare_text_document.* {GPTNeoXDir()}/processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvpVaIbLytqM",
        "outputId": "3e3a29d7-b306-4822-8f2c-65281bf1ff4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'gpt-neox'...\n",
            "remote: Enumerating objects: 296, done.\u001b[K\n",
            "remote: Counting objects: 100% (296/296), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 296 (delta 74), reused 136 (delta 43), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (296/296), 2.50 MiB | 6.23 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n",
            "CPU times: user 16.4 ms, sys: 1.31 ms, total: 17.7 ms\n",
            "Wall time: 1.01 s\n"
          ]
        }
      ],
      "source": [
        "#@title Clone GPT-NeoX\n",
        "%%time\n",
        "%cd {workspaceDir()}\n",
        "!git clone --depth 1 https://github.com/EleutherAI/gpt-neox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH8eetUBytqN",
        "outputId": "c80d3b67-ff8f-4836-b8a9-8fccb4cc8f7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading my_env.tar.gz\n",
            "Unzipping my_env.tar.gz\n",
            "Untarring my_env.tar.gz\n",
            "CPU times: user 10.6 s, sys: 8.75 s, total: 19.4 s\n",
            "Wall time: 2min 31s\n"
          ]
        }
      ],
      "source": [
        "#@title Load prebuilt Python environment for Colab\n",
        "%%time\n",
        "%cd {workspaceDir()}\n",
        "GPTNeoXColab.utils.colab.download_my_env()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTyERIkj8elu"
      },
      "source": [
        "# Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "umfYbg3TytqJ"
      },
      "outputs": [],
      "source": [
        "import GPTNeoXColab\n",
        "import os\n",
        "from pathlib import Path\n",
        "ROOT_DIR = GPTNeoXColab.utils.colab.find_project_root()\n",
        "RELATIVE_ROOT_DIR = os.path.relpath(ROOT_DIR, Path.cwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oE-4NPwkytqO"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"experiment1\"  # Change this to dynamically load different experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9aMQNFL6mbm",
        "outputId": "20d3c908-6a6f-40ee-a07c-59f78fc86958"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:dagshub.auth.tokens:The added token already exists in the token cache, skipping\n"
          ]
        }
      ],
      "source": [
        "import dagshub\n",
        "dagshub.auth.add_app_token(token=os.environ[\"DAGSHUB_USER_TOKEN\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "iaBFYofFytqP",
        "outputId": "262bd060-db3e-447b-c5fc-bed64dcec7e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"MarkNZed/GPT-NeoX-Colab\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"MarkNZed/GPT-NeoX-Colab\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository MarkNZed/GPT-NeoX-Colab initialized!\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Repository MarkNZed/GPT-NeoX-Colab initialized!\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: base_experiment\n",
            "dagshub:\n",
            "  repo_owner: MarkNZed\n",
            "  repo_name: GPT-NeoX-Colab\n",
            "output_dir: ../outputs\n",
            "experiment_name: base_experiment\n",
            "seed: 42\n",
            "model:\n",
            "  hidden_dim: 64\n",
            "training:\n",
            "  batch_size: 32\n",
            "  learning_rate: 0.001\n",
            "  epochs: 2\n",
            "data: null\n",
            "experiments:\n",
            "  experiment_name: experiment_1\n",
            "  seed: 123\n",
            "\n",
            "/content/gpt-neox\n",
            "Training complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/11/11 11:22:14 INFO mlflow.tracking._tracking_service.client: 🏃 View run thundering-bee-987 at: https://dagshub.com/MarkNZed/GPT-NeoX-Colab.mlflow/#/experiments/3/runs/21f714bd429e48ffbe7fb31a1626c73d.\n",
            "2024/11/11 11:22:14 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://dagshub.com/MarkNZed/GPT-NeoX-Colab.mlflow/#/experiments/3.\n"
          ]
        }
      ],
      "source": [
        "#@title Run the training in a detached background process\n",
        "%cd {workspaceDir()}\n",
        "import subprocess\n",
        "import os\n",
        "import dagshub\n",
        "import mlflow\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "from hydra.core.global_hydra import GlobalHydra\n",
        "from hydra import initialize, compose\n",
        "\n",
        "# Clear Hydra's global state if it’s already initialized\n",
        "if GlobalHydra.instance().is_initialized():\n",
        "    GlobalHydra.instance().clear()\n",
        "\n",
        "initialize(config_path=f\"{RELATIVE_ROOT_DIR}/configs\", version_base=\"1.1\")\n",
        "\n",
        "cfg = compose(config_name=\"hydra\", overrides=[f\"experiments={experiment_name}\"])\n",
        "\n",
        "# Set MLflow tracking URI for DagsHub\n",
        "#MLFLOW_TRACKING_URI = f\"https://dagshub.com/{cfg.dagshub.repo_owner}/{cfg.dagshub.repo_name}.mlflow\"\n",
        "\n",
        "os.environ[\"DAGSHUB_USER\"] = cfg.dagshub.repo_owner\n",
        "os.environ[\"DAGSHUB_TOKEN\"] = \"\"\n",
        "\n",
        "# Initialize DagsHub logging\n",
        "try:\n",
        "    # Will setup MLFLOW_TRACKING_URI MLFLOW_TRACKING_USERNAME MLFLOW_TRACKING_PASSWORD\n",
        "    dagshub.init(repo_owner=cfg.dagshub.repo_owner, repo_name=cfg.dagshub.repo_name, mlflow=True)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize DagsHub logging: {e}\")\n",
        "\n",
        "def train_model(cfg: DictConfig):\n",
        "    print(\"Running experiment:\", cfg.experiment_name)\n",
        "    print(OmegaConf.to_yaml(cfg))\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_params(OmegaConf.to_container(cfg, resolve=True))\n",
        "\n",
        "    %cd {GPTNeoXDir()}\n",
        "    # The deepy.py script assumes it is running in the root of GTP-NeoX repo\n",
        "    # Start a detached background process\n",
        "    process = subprocess.Popen(\n",
        "        f\"nohup bash -c \\\"source {workspaceDir()}/my_env/bin/activate && python ./deepy.py train.py --conf_dir {GPTNeoXColabDir()}/configs shakespeare shakespeare_deepy\\\" & echo $! > train_process.pid\",\n",
        "        shell=True,\n",
        "        executable='/bin/bash',\n",
        "        preexec_fn=subprocess.os.setsid  # Starts the process in a new session so interrupting Notebook does not kill the training\n",
        "    )\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "exp_id = GPTNeoXColab.utils.ml.get_or_create_experiment_id(\"tutorial\")\n",
        "\n",
        "# Set the experiment name and start the run\n",
        "# mlflow.set_experiment(experiment_name)\n",
        "with mlflow.start_run(experiment_id=exp_id):\n",
        "    train_model(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXW4XGr-93pR"
      },
      "outputs": [],
      "source": [
        "#@title Wait until logs directory is created\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Path to the log directory\n",
        "logs_dir = f\"{GPTNeoXDir()}/logs\"\n",
        "\n",
        "# Wait for the directory to be created\n",
        "while not os.path.exists(logs_dir):\n",
        "    print(\"Waiting for logs directory to be created...\")\n",
        "    time.sleep(10)  # Check every X seconds\n",
        "\n",
        "print(\"ogs directory found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCU9AaWO9sMb",
        "outputId": "2ed7ddf1-735a-4668-a79b-2cf7664b5a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latest log file: /content/gpt-neox/logs/d1d8af4f6537_stdout.txt\n"
          ]
        }
      ],
      "source": [
        "#@title Find the latest log file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Define the log directory and pattern for log files\n",
        "log_dir = f\"{GPTNeoXDir()}/logs\"\n",
        "log_pattern = os.path.join(log_dir, \"*_stdout.txt\")\n",
        "\n",
        "# Get the list of log files that match the pattern\n",
        "log_files = glob.glob(log_pattern)\n",
        "\n",
        "# Ensure there are log files in the directory\n",
        "if log_files:\n",
        "    # Find the latest log file based on modification time\n",
        "    latest_log = max(log_files, key=os.path.getmtime)\n",
        "    print(\"Latest log file:\", latest_log)\n",
        "else:\n",
        "    latest_log = None\n",
        "    print(\"No log files found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PTFQWG08tKv",
        "outputId": "dbad1910-e5ea-4614-9c91-4bb70a0561dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PID: 7889\n",
            "Training is still running...\n",
            "40 iterations\n",
            "Training is still running...\n",
            "80 iterations\n",
            "Training is still running...\n",
            "120 iterations\n",
            "Training is still running...\n",
            "160 iterations\n",
            "Training is still running...\n",
            "200 iterations\n",
            "Training is still running...\n"
          ]
        }
      ],
      "source": [
        "#@title Read the latest log file and extract the iteration count\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "\n",
        "# File to store the last read position (persistence between script runs)\n",
        "file_position = 0\n",
        "# Regular expression to match \"iteration <number> / <total>\"\n",
        "iteration_pattern = re.compile(r\"iteration\\s+(\\d+)\\s*/\\s*\\d+\")\n",
        "\n",
        "def read_new_iterations():\n",
        "    global file_position\n",
        "    # Open the log file and seek to the last position\n",
        "    with open(latest_log, \"r\") as file:\n",
        "        file.seek(file_position)\n",
        "        # Read new lines\n",
        "        new_lines = file.readlines()\n",
        "        file_position = file.tell()\n",
        "        # Process lines containing \"iteration\"\n",
        "        last_match = None\n",
        "        for line in new_lines:\n",
        "            match = iteration_pattern.search(line)\n",
        "            if match:\n",
        "                last_match = match\n",
        "        if last_match:\n",
        "            # Extract the iteration count from the regex match\n",
        "            iteration_count = int(last_match.group(1))\n",
        "            print(f\"{iteration_count} iterations\")\n",
        "\n",
        "# Read the PID from the file\n",
        "with open(\"train_process.pid\", \"r\") as f:\n",
        "    pid = int(f.read().strip())\n",
        "    print(\"PID:\", pid)\n",
        "\n",
        "# Function to check if the process is running\n",
        "def is_process_running(pid):\n",
        "    try:\n",
        "        os.kill(pid, 0)  # Sending signal 0 to check if the process exists\n",
        "        return True\n",
        "    except OSError:\n",
        "        return False\n",
        "\n",
        "# Monitor the training process\n",
        "while is_process_running(pid):\n",
        "    read_new_iterations()\n",
        "    print(\"Training is still running...\")\n",
        "    time.sleep(30)  # Check every X seconds\n",
        "\n",
        "print(\"Training has finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0JGqO-azytqR"
      },
      "outputs": [],
      "source": [
        "# Here we could disconnect from the GPU resource\n",
        "#from google.colab import runtime\n",
        "#runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
