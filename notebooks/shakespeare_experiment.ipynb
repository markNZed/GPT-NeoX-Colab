{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/shakespeare_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flp2Dht6ytqE"
      },
      "source": [
        "# Experiment\n",
        "This is a demonstration of how experiments can be run using DagsHub and MLflow.\n",
        "We will train three different versions of the tiny LLM using different batch sizes and compare the results.\n",
        "\n",
        "## ToDo\n",
        "- Shorten the training time for testing\n",
        "- Run tests in parallel\n",
        "- Extract functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LECzxKWK8CyS"
      },
      "source": [
        "## Login to Dagshub\n",
        "To avoid requirest in the middle of the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxd2wfpq1F9u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "%pip install -q dagshub\n",
        "import dagshub\n",
        "try:\n",
        "  from google.colab import userdata\n",
        "  os.environ[\"DAGSHUB_USER_TOKEN\"] = userdata.get(\"DAGSHUB_USER_TOKEN\")\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  if os.environ[\"DAGSHUB_USER_TOKEN\"]:\n",
        "    pass\n",
        "except:\n",
        "  os.environ[\"DAGSHUB_USER_TOKEN\"] = dagshub.auth.get_token()\n",
        "dagshub.auth.add_app_token(token=os.environ[\"DAGSHUB_USER_TOKEN\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iega62GjytqH"
      },
      "outputs": [],
      "source": [
        "#@title Setup paths\n",
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "# A file like .ipython/profile_default/startup/10-test.py could restore these vars\n",
        "workspaceDir = \"/content\"\n",
        "GPTNeoXDirName = \"gpt-neox\"\n",
        "GPTNeoXDir = f\"{workspaceDir}/{GPTNeoXDirName}\"\n",
        "gpt_neox_colabDirName = \"GPT-NeoX-Colab\"\n",
        "gpt_neox_colabDir = f\"{workspaceDir}/{gpt_neox_colabDirName}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUxVImVvytqL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX-Colab\n",
        "%cd {workspaceDir}\n",
        "# Don't use --depth 1 because that does not play nice with git-annex\n",
        "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "%cd {gpt_neox_colabDir}\n",
        "%pip install -q -r requirements_colab.txt\n",
        "%pip install -q .\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(f\"{gpt_neox_colabDir}/.env\")\n",
        "import gpt_neox_colab\n",
        "gpt_neox_colab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.bin\")\n",
        "gpt_neox_colab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.idx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvpVaIbLytqM"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX\n",
        "%cd {workspaceDir}\n",
        "#!git clone --depth 1 https://github.com/EleutherAI/gpt-neox\n",
        "!git clone -b pipe_parallel_size_1 --depth 1 https://github.com/markNZed/gpt-neox.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EJBRBSpiSXF"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {GPTNeoXDir}/processed_data\n",
        "!cp {gpt_neox_colabDir}/data/shakespeare/shakespeare_text_document.* {GPTNeoXDir}/processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH8eetUBytqN"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#@title Load prebuilt Python environment for Colab\n",
        "import gpt_neox_colab\n",
        "%cd {workspaceDir}\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    gpt_neox_colab.utils.colab.download_my_env()\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTyERIkj8elu"
      },
      "source": [
        "# Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RmiyhUgSSt7"
      },
      "outputs": [],
      "source": [
        "!pip install psutil\n",
        "# Install this for GPU metric logging\n",
        "!pip install pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umfYbg3TytqJ"
      },
      "outputs": [],
      "source": [
        "import gpt_neox_colab\n",
        "import os\n",
        "from pathlib import Path\n",
        "ROOT_DIR = gpt_neox_colab.utils.colab.find_project_root()\n",
        "RELATIVE_ROOT_DIR = os.path.relpath(ROOT_DIR, Path.cwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PTFQWG08tKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "import time\n",
        "\n",
        "# File to store the last read position (persistence between script runs)\n",
        "file_position = 0\n",
        "# Regular expression to match \"iteration <number> / <total>\"\n",
        "iteration_pattern = re.compile(r\"iteration\\s+(\\d+)\\s*/\\s*\\d+\")\n",
        "\n",
        "def get_latest_file(dir, pattern = \"*_stdout.txt\"):\n",
        "  # Define the log directory and pattern for log files\n",
        "  glob_pattern = os.path.join(dir, pattern)\n",
        "  # Get the list of log files that match the pattern\n",
        "  files = glob.glob(glob_pattern)\n",
        "  # Ensure there are log files in the directory\n",
        "  if files:\n",
        "      # Find the latest log file based on modification time\n",
        "      file = max(files, key=os.path.getmtime)\n",
        "      print(\"Latest file:\", file)\n",
        "  else:\n",
        "      file = None\n",
        "  return file\n",
        "\n",
        "def read_new_iterations(latest_log):\n",
        "    global file_position\n",
        "    # Open the log file and seek to the last position\n",
        "    with open(latest_log, \"r\") as file:\n",
        "        file.seek(file_position)\n",
        "        # Read new lines\n",
        "        new_lines = file.readlines()\n",
        "        file_position = file.tell()\n",
        "        # Process lines containing \"iteration\"\n",
        "        last_match = None\n",
        "        for line in new_lines:\n",
        "            match = iteration_pattern.search(line)\n",
        "            if match:\n",
        "                last_match = match\n",
        "        if last_match:\n",
        "            # Extract the iteration count from the regex match\n",
        "            iteration_count = int(last_match.group(1))\n",
        "            print(f\"{iteration_count} iterations\")\n",
        "\n",
        "# Function to check if the process is running\n",
        "def is_process_running(pid):\n",
        "    try:\n",
        "        os.kill(pid, 0)  # Sending signal 0 to check if the process exists\n",
        "        return True\n",
        "    except OSError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtEiZxZBV2iD"
      },
      "outputs": [],
      "source": [
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "import os\n",
        "\n",
        "def get_scalar_from_tensorboard(file, key):\n",
        "    # Load TensorBoard events\n",
        "    event_acc = EventAccumulator(file)\n",
        "    event_acc.Reload()\n",
        "    print(event_acc.Tags())\n",
        "    # Extract loss scalar events\n",
        "    if key in event_acc.Tags().get('scalars', []):\n",
        "        events = event_acc.Scalars(key)\n",
        "        value = events[-1].value  # Get the last logged value\n",
        "        return value\n",
        "    else:\n",
        "        print(f\"{key} not found in TensorBoard logs.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOvOVTxbKUWQ"
      },
      "outputs": [],
      "source": [
        "%pip install GitPython\n",
        "%pip install ipynbname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BCbRs-3KUWR"
      },
      "outputs": [],
      "source": [
        "from git import Repo\n",
        "\n",
        "repo = Repo(gpt_neox_colabDir)\n",
        "commit_id = repo.head.commit.hexsha\n",
        "branch_name = repo.active_branch.name\n",
        "repo_url = next(repo.remotes.origin.urls)\n",
        "print(f\"Commit ID: {commit_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaBFYofFytqP"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import subprocess\n",
        "import os\n",
        "from omegaconf import OmegaConf\n",
        "from hydra import initialize_config_dir, compose\n",
        "from hydra.core.global_hydra import GlobalHydra\n",
        "import mlflow\n",
        "import time\n",
        "import dagshub\n",
        "import ipynbname\n",
        "import torch\n",
        "\n",
        "%cd {GPTNeoXDir}\n",
        "\n",
        "dagshub.init(repo_owner='MarkNZed', repo_name='GPT-NeoX-Colab', mlflow=True)\n",
        "experiment_group = \"Log only experiment parameters\"\n",
        "mlflow.set_experiment(experiment_group)\n",
        "mlflow.enable_system_metrics_logging()\n",
        "\n",
        "def log_gpu_info():\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        mlflow.log_param(\"gpu_type\", gpu_name)\n",
        "    else:\n",
        "        mlflow.log_param(\"gpu_type\", \"CPU\")\n",
        "\n",
        "def load_and_merge_configs(base_conf_dir, experiment_name):\n",
        "    # Initialize Hydra with the base config directory\n",
        "    initialize_config_dir(config_dir=base_conf_dir, version_base=\"1.1\")\n",
        "\n",
        "    # Load the base configurations (shakespeare and shakespeare_deepy) and experiment overrides\n",
        "    base_cfg = compose(config_name=\"shakespeare.yml\")\n",
        "    OmegaConf.set_struct(base_cfg, False) # No struct checking for matching structure in merge\n",
        "    deepy_cfg = compose(config_name=\"shakespeare_deepy.yml\")\n",
        "    OmegaConf.set_struct(deepy_cfg, False) # No struct checking for matching structure in merge\n",
        "    experiment_cfg = compose(config_name=\"hydra\", overrides=[f\"experiments={experiment_name}\"])\n",
        "    OmegaConf.set_struct(experiment_cfg, False) # No struct checking for matching structure in merge\n",
        "\n",
        "    # Extract the 'experiments' section from the configuration dictionary\n",
        "    experiments_dict = OmegaConf.to_container(experiment_cfg.experiments, resolve=True)\n",
        "    # Log only the parameters within the 'experiments' section\n",
        "    mlflow.log_params(experiments_dict)\n",
        "\n",
        "    experiment_overrides = experiment_cfg.get(\"experiments\", {})\n",
        "    OmegaConf.set_struct(experiment_overrides, False) # No struct checking for matching structure in merge\n",
        "\n",
        "    print(experiment_overrides)\n",
        "\n",
        "    # Merge the configurations: base -> deepy -> experiment\n",
        "    cfg = OmegaConf.merge(base_cfg, deepy_cfg, experiment_overrides)\n",
        "\n",
        "    return cfg\n",
        "\n",
        "def run_experiment(cfg, experiment_name):\n",
        "    print(\"Running experiment:\", experiment_name)\n",
        "    experimentDir = f\"{GPTNeoXDir}/experiments/{experiment_name}\"\n",
        "    !rm -rf {experimentDir}\n",
        "    !mkdir -p {experimentDir}\n",
        "    !rm -f train_process.pid\n",
        "    file_position = 0\n",
        "    latest_log = None\n",
        "\n",
        "    # Create a temporary directory for configs\n",
        "    temp_config_dir = tempfile.mkdtemp()\n",
        "    temp_config_file = os.path.join(temp_config_dir, 'temp_config.yml')\n",
        "\n",
        "    # Save the modified config to the temporary file in JSON-like structure within a YAML file\n",
        "    with open(temp_config_file, 'w') as f:\n",
        "        # Dump the config as JSON but save it with a .yml extension\n",
        "        OmegaConf.save(OmegaConf.create(OmegaConf.to_container(cfg, resolve=True)), f)\n",
        "\n",
        "    # Start a detached background process using the temp config\n",
        "    cmd = f\"\"\"nohup bash -c \"source {workspaceDir}/my_env/bin/activate && \\\n",
        "        cd {GPTNeoXDir} && \\\n",
        "        python ./deepy.py train.py --conf_dir {temp_config_dir} \\\n",
        "        temp_config\" \"\"\"\n",
        "    print(\"Running command:\", cmd)\n",
        "    #cmd = \"nohup bash -c ls\" # Used to test without running on GPU\n",
        "\n",
        "    # Start the process and retrieve the PID directly\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        shell=True,\n",
        "        executable='/bin/bash',\n",
        "        preexec_fn=os.setsid  # Starts the process in a new session\n",
        "    )\n",
        "\n",
        "    pid = process.pid\n",
        "    print(f\"Started training with PID: {pid}\")\n",
        "\n",
        "    # Periodically check if the process has completed\n",
        "    while True:\n",
        "        # Poll the process to see if it has terminated\n",
        "        if process.poll() is not None:\n",
        "            # Process has completed\n",
        "            print(\"Training has completed.\")\n",
        "            break\n",
        "        else:\n",
        "            if latest_log:\n",
        "                read_new_iterations(latest_log)\n",
        "            elif os.path.exists(f\"{experimentDir}/logs\"):\n",
        "                latest_log = get_latest_file(f\"{experimentDir}/logs\", \"*_stdout.txt\")\n",
        "            print(\"Training is still running...\")\n",
        "            time.sleep(30)  # Check every X seconds\n",
        "\n",
        "    latest_events_file = get_latest_file(f\"{experimentDir}/tensorboard\", \"events.out.tfevents.*\")\n",
        "    if latest_events_file:\n",
        "        loss_key = \"test/lm_loss\"\n",
        "        loss = get_scalar_from_tensorboard(latest_events_file, loss_key)\n",
        "        print(f\"Logging metric {loss_key} {loss}\")\n",
        "        mlflow.log_metric(loss_key, loss)\n",
        "\n",
        "    # Clean up the temporary directory after training\n",
        "    # (Optional: You might want to keep it for debugging)\n",
        "    # shutil.rmtree(temp_config_dir)\n",
        "\n",
        "try:\n",
        "    notebook_path = ipynbname.path()\n",
        "except:\n",
        "    notebook_path = \"shakespeare_experiment.ipynb\"\n",
        "\n",
        "# List of experiment names\n",
        "experiments = [\"experiment1\", \"experiment2\", \"experiment3\"]\n",
        "\n",
        "# Parent Run\n",
        "with mlflow.start_run() as parent_run:\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "    parent_run_id = parent_run.info.run_id\n",
        "    client.set_tag(parent_run_id, \"experiment_group\", experiment_group)\n",
        "    log_gpu_info()\n",
        "\n",
        "    for experiment in experiments:\n",
        "        with mlflow.start_run(nested=True) as child_run:\n",
        "            run_id = child_run.info.run_id\n",
        "            client.set_tag(run_id, \"mlflow.source.git.commit\", commit_id)\n",
        "            client.set_tag(run_id, \"mlflow.source.git.branch\", branch_name)\n",
        "            client.set_tag(run_id, \"mlflow.source.git.repoURL\", repo_url)\n",
        "            client.set_tag(run_id, \"mlflow.source.type\", \"NOTEBOOK\")\n",
        "            client.set_tag(run_id, \"mlflow.source.name\", notebook_path)\n",
        "            log_gpu_info()\n",
        "\n",
        "            if GlobalHydra.instance().is_initialized():\n",
        "                GlobalHydra.instance().clear()\n",
        "\n",
        "            base_conf_dir = f\"{gpt_neox_colabDir}/configs\"\n",
        "            cfg = load_and_merge_configs(base_conf_dir, experiment)\n",
        "            run_experiment(cfg, experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luIf6lNkKUWT"
      },
      "outputs": [],
      "source": [
        "# Here we could disconnect from the GPU resource\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimental results\n",
        "- Because the SLM is so small a significant portion of the experiment runtime is taken up with the setup of the environment."
      ],
      "metadata": {
        "id": "6rKYo571OZpY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}