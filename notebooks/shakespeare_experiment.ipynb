{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/shakespeare_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flp2Dht6ytqE"
      },
      "source": [
        "# Experiment\n",
        "This is a demonstration of how experiments can be run using DagsHub and MLflow.\n",
        "We will train three different versions of the tiny LLM using different batch sizes and compare the results.\n",
        "\n",
        "## ToDo\n",
        "- Shorten the training time for testing\n",
        "- Run tests in parallel\n",
        "- Extract functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LECzxKWK8CyS"
      },
      "source": [
        "## Login to Dagshub\n",
        "To avoid requirest in the middle of the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wxd2wfpq1F9u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "%pip install -q dagshub\n",
        "import dagshub\n",
        "try:\n",
        "  from google.colab import userdata\n",
        "  #os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = userdata.get(\"MLFLOW_TRACKING_PASSWORD\")\n",
        "  #os.environ[\"MLFLOW_TRACKING_USERNAME\"] = userdata.get(\"MLFLOW_TRACKING_USERNAME\")\n",
        "  os.environ[\"DAGSHUB_USER_TOKEN\"] = userdata.get(\"DAGSHUB_USER_TOKEN\")\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  if os.environ[\"DAGSHUB_USER_TOKEN\"]:\n",
        "    pass\n",
        "except:\n",
        "  os.environ[\"DAGSHUB_USER_TOKEN\"] = dagshub.auth.get_token()\n",
        "dagshub.auth.add_app_token(token=os.environ[\"DAGSHUB_USER_TOKEN\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Iega62GjytqH"
      },
      "outputs": [],
      "source": [
        "#@title Setup paths\n",
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "# A file like .ipython/profile_default/startup/10-test.py could restore these vars\n",
        "workspaceDir = \"/content\"\n",
        "GPTNeoXDirName = \"gpt-neox\"\n",
        "GPTNeoXDir = f\"{workspaceDir}/{GPTNeoXDirName}\"\n",
        "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
        "GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUxVImVvytqL",
        "outputId": "8e3e11e2-b831-4e27-d903-6e1dcd3690a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'GPT-NeoX-Colab' already exists and is not an empty directory.\n",
            "/content/GPT-NeoX-Colab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for GPTNeoXColab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Data retrieval successful.\n",
            "Data retrieval successful.\n",
            "CPU times: user 134 ms, sys: 17 ms, total: 151 ms\n",
            "Wall time: 21.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX-Colab\n",
        "%cd {workspaceDir}\n",
        "# Don't use --depth 1 because that does not play nice with git-annex\n",
        "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "%cd {GPTNeoXColabDir}\n",
        "%pip install -q -r requirements_colab.txt\n",
        "%pip install -q .\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
        "import GPTNeoXColab\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.bin\")\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.idx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvpVaIbLytqM",
        "outputId": "e42e2f70-2763-4f4e-cec0-fbb8a2c4422c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'gpt-neox' already exists and is not an empty directory.\n",
            "CPU times: user 5.52 ms, sys: 440 Âµs, total: 5.96 ms\n",
            "Wall time: 104 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX\n",
        "%cd {workspaceDir}\n",
        "!git clone --depth 1 https://github.com/EleutherAI/gpt-neox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6EJBRBSpiSXF"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {GPTNeoXDir}/processed_data\n",
        "!cp {GPTNeoXColabDir}/data/shakespeare/shakespeare_text_document.* {GPTNeoXDir}/processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH8eetUBytqN",
        "outputId": "0680bc56-9515-407f-f8e8-aa71210c5d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "CPU times: user 7.86 ms, sys: 554 Âµs, total: 8.41 ms\n",
            "Wall time: 8.57 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Load prebuilt Python environment for Colab\n",
        "import GPTNeoXColab\n",
        "%cd {workspaceDir}\n",
        "GPTNeoXColab.utils.colab.download_my_env()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTyERIkj8elu"
      },
      "source": [
        "# Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "umfYbg3TytqJ"
      },
      "outputs": [],
      "source": [
        "import GPTNeoXColab\n",
        "import os\n",
        "from pathlib import Path\n",
        "ROOT_DIR = GPTNeoXColab.utils.colab.find_project_root()\n",
        "RELATIVE_ROOT_DIR = os.path.relpath(ROOT_DIR, Path.cwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oE-4NPwkytqO"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"experiment1\"  # Change this to dynamically load different experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "iaBFYofFytqP",
        "outputId": "3175f442-5e2c-47b6-8e78-985146f4cd3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as MarkNZed\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as MarkNZed\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"MarkNZed/GPT-NeoX-Colab\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"MarkNZed/GPT-NeoX-Colab\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository MarkNZed/GPT-NeoX-Colab initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository MarkNZed/GPT-NeoX-Colab initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment: base_experiment\n",
            "dagshub:\n",
            "  repo_owner: MarkNZed\n",
            "  repo_name: GPT-NeoX-Colab\n",
            "output_dir: ../outputs\n",
            "experiment_name: base_experiment\n",
            "seed: 42\n",
            "model:\n",
            "  hidden_dim: 64\n",
            "training:\n",
            "  batch_size: 32\n",
            "  learning_rate: 0.001\n",
            "  epochs: 2\n",
            "data: null\n",
            "experiments:\n",
            "  experiment_name: experiment_1\n",
            "  seed: 123\n",
            "\n",
            "/content/gpt-neox\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/11/11 14:49:16 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run polite-grub-460 at: https://dagshub.com/MarkNZed/GPT-NeoX-Colab.mlflow/#/experiments/3/runs/6f82f959de9d4f079a1fe7c87a0c05ce.\n",
            "2024/11/11 14:49:16 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: https://dagshub.com/MarkNZed/GPT-NeoX-Colab.mlflow/#/experiments/3.\n"
          ]
        }
      ],
      "source": [
        "#@title Run the training in a detached background process\n",
        "%cd {workspaceDir}\n",
        "import subprocess\n",
        "import os\n",
        "import dagshub\n",
        "import mlflow\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "from hydra.core.global_hydra import GlobalHydra\n",
        "from hydra import initialize, compose\n",
        "\n",
        "# Clear Hydra's global state if itâ€™s already initialized\n",
        "if GlobalHydra.instance().is_initialized():\n",
        "    GlobalHydra.instance().clear()\n",
        "\n",
        "initialize(config_path=f\"{RELATIVE_ROOT_DIR}/configs\", version_base=\"1.1\")\n",
        "\n",
        "cfg = compose(config_name=\"hydra\", overrides=[f\"experiments={experiment_name}\"])\n",
        "\n",
        "# Set MLflow tracking URI for DagsHub\n",
        "#MLFLOW_TRACKING_URI = f\"https://dagshub.com/{cfg.dagshub.repo_owner}/{cfg.dagshub.repo_name}.mlflow\"\n",
        "\n",
        "os.environ[\"DAGSHUB_USER\"] = cfg.dagshub.repo_owner\n",
        "\n",
        "# Initialize DagsHub logging\n",
        "try:\n",
        "    # Will setup MLFLOW_TRACKING_URI MLFLOW_TRACKING_USERNAME MLFLOW_TRACKING_PASSWORD\n",
        "    dagshub.init(repo_owner=cfg.dagshub.repo_owner, repo_name=cfg.dagshub.repo_name, mlflow=True)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize DagsHub logging: {e}\")\n",
        "\n",
        "def train_model(cfg: DictConfig):\n",
        "    print(\"Running experiment:\", cfg.experiment_name)\n",
        "    print(OmegaConf.to_yaml(cfg))\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_params(OmegaConf.to_container(cfg, resolve=True))\n",
        "\n",
        "    %cd {GPTNeoXDir}\n",
        "    # The deepy.py script assumes it is running in the root of GTP-NeoX repo\n",
        "    # Start a detached background process\n",
        "    process = subprocess.Popen(\n",
        "        f\"nohup bash -c \\\"source {workspaceDir}/my_env/bin/activate && python ./deepy.py train.py --conf_dir {GPTNeoXColabDir}/configs shakespeare shakespeare_deepy\\\" & echo $! > train_process.pid\",\n",
        "        shell=True,\n",
        "        executable='/bin/bash',\n",
        "        preexec_fn=subprocess.os.setsid  # Starts the process in a new session so interrupting Notebook does not kill the training\n",
        "    )\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "exp_id = GPTNeoXColab.utils.ml.get_or_create_experiment_id(\"tutorial\")\n",
        "\n",
        "# Set the experiment name and start the run\n",
        "# mlflow.set_experiment(experiment_name)\n",
        "with mlflow.start_run(experiment_id=exp_id):\n",
        "    train_model(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lXW4XGr-93pR",
        "outputId": "c1be3289-14fc-419a-e238-a674e9c7ae74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ogs directory found\n"
          ]
        }
      ],
      "source": [
        "#@title Wait until logs directory is created\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Path to the log directory\n",
        "logs_dir = f\"{GPTNeoXDir}/logs\"\n",
        "\n",
        "# Wait for the directory to be created\n",
        "while not os.path.exists(logs_dir):\n",
        "    print(\"Waiting for logs directory to be created...\")\n",
        "    time.sleep(10)  # Check every X seconds\n",
        "\n",
        "print(\"ogs directory found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCU9AaWO9sMb",
        "outputId": "4f037476-359d-46a3-bdea-a5e6ad8bb6ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest log file: /content/gpt-neox/logs/7883f81576c3_stdout.txt\n"
          ]
        }
      ],
      "source": [
        "#@title Find the latest log file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Define the log directory and pattern for log files\n",
        "log_dir = f\"{GPTNeoXDir}/logs\"\n",
        "log_pattern = os.path.join(log_dir, \"*_stdout.txt\")\n",
        "\n",
        "# Get the list of log files that match the pattern\n",
        "log_files = glob.glob(log_pattern)\n",
        "\n",
        "# Ensure there are log files in the directory\n",
        "if log_files:\n",
        "    # Find the latest log file based on modification time\n",
        "    latest_log = max(log_files, key=os.path.getmtime)\n",
        "    print(\"Latest log file:\", latest_log)\n",
        "else:\n",
        "    latest_log = None\n",
        "    print(\"No log files found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PTFQWG08tKv",
        "outputId": "6856389b-c12c-4f57-fdaf-7e4100b4d241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PID: 12077\n",
            "800 iterations\n",
            "Training is still running...\n",
            "Training has finished.\n"
          ]
        }
      ],
      "source": [
        "#@title Read the latest log file and extract the iteration count\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "\n",
        "# File to store the last read position (persistence between script runs)\n",
        "file_position = 0\n",
        "# Regular expression to match \"iteration <number> / <total>\"\n",
        "iteration_pattern = re.compile(r\"iteration\\s+(\\d+)\\s*/\\s*\\d+\")\n",
        "\n",
        "def read_new_iterations():\n",
        "    global file_position\n",
        "    # Open the log file and seek to the last position\n",
        "    with open(latest_log, \"r\") as file:\n",
        "        file.seek(file_position)\n",
        "        # Read new lines\n",
        "        new_lines = file.readlines()\n",
        "        file_position = file.tell()\n",
        "        # Process lines containing \"iteration\"\n",
        "        last_match = None\n",
        "        for line in new_lines:\n",
        "            match = iteration_pattern.search(line)\n",
        "            if match:\n",
        "                last_match = match\n",
        "        if last_match:\n",
        "            # Extract the iteration count from the regex match\n",
        "            iteration_count = int(last_match.group(1))\n",
        "            print(f\"{iteration_count} iterations\")\n",
        "\n",
        "# Read the PID from the file\n",
        "with open(\"train_process.pid\", \"r\") as f:\n",
        "    pid = int(f.read().strip())\n",
        "    print(\"PID:\", pid)\n",
        "\n",
        "# Function to check if the process is running\n",
        "def is_process_running(pid):\n",
        "    try:\n",
        "        os.kill(pid, 0)  # Sending signal 0 to check if the process exists\n",
        "        return True\n",
        "    except OSError:\n",
        "        return False\n",
        "\n",
        "# Monitor the training process\n",
        "while is_process_running(pid):\n",
        "    read_new_iterations()\n",
        "    print(\"Training is still running...\")\n",
        "    time.sleep(30)  # Check every X seconds\n",
        "\n",
        "print(\"Training has finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0JGqO-azytqR"
      },
      "outputs": [],
      "source": [
        "# Here we could disconnect from the GPU resource\n",
        "#from google.colab import runtime\n",
        "#runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}