{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/shakespeare_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lBrppZd_A0"
      },
      "source": [
        "# Inference of a tiny Shakespeare LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uYyxsLpOuQR1",
        "outputId": "b80ddb1b-94d9-4378-a7a7-cd4c09b35627",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Date and Time: 2024-11-28 12:45:17.046424\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(\"Current Date and Time:\", datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HBH4eN9Id_A7"
      },
      "outputs": [],
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "workspaceDir = \"/content\"\n",
        "gpt_neox_colabDirName = \"GPT-NeoX-Colab\"\n",
        "gpt_neox_colabDir = f\"{workspaceDir}/{gpt_neox_colabDirName}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!curl -LsSf https://astral.sh/uv/install.sh | sh"
      ],
      "metadata": {
        "id": "E0IJElXTzNtA",
        "outputId": "4fed168d-9154-4231-ad61-0fb450adc9d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading uv 0.5.5 x86_64-unknown-linux-gnu\n",
            "no checksums to verify\n",
            "installing to /root/.local/bin\n",
            "  uv\n",
            "  uvx\n",
            "everything's installed!\n",
            "\n",
            "To add $HOME/.local/bin to your PATH, either restart your shell or run:\n",
            "\n",
            "    source $HOME/.local/bin/env (sh, bash, zsh)\n",
            "    source $HOME/.local/bin/env.fish (fish)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e27VRVq07s"
      },
      "source": [
        "# Cloning Git Repos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "collapsed": true,
        "id": "j_hUsQxlhnou",
        "outputId": "0d58ad1e-5b5d-49d0-f7f4-428afe46b040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'GPT-NeoX-Colab' already exists and is not an empty directory.\n",
            "/content/GPT-NeoX-Colab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for gtp_neox_colab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "CPU times: user 242 ms, sys: 35.3 ms, total: 278 ms\n",
            "Wall time: 33.9 s\n"
          ]
        }
      ],
      "source": [
        "#@title Clone GPT-NeoX-Colab\n",
        "%%time\n",
        "%cd {workspaceDir}\n",
        "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "#%cd {gpt_neox_colabDir}\n",
        "#!uv sync -q --dev\n",
        "#!uv run pip install -q -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This runs python in the virtual environment\n",
        "#%%bash -s \"$gpt_neox_colabDir\"\n",
        "#source \"$1/.venv/bin/activate\"\n",
        "#python -c \"\n",
        "#from dotenv import load_dotenv\n",
        "#import os\n",
        "#load_dotenv('$1/.env')\n",
        "#import gpt_neox_colab\n",
        "#gpt_neox_colab.utils.colab.fetch_data('data/shakespeare/model.tar.gz')\n",
        "#\""
      ],
      "metadata": {
        "id": "XSgnc16q_1nx",
        "outputId": "df375218-dd06-47c7-ab25-d07140894647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data retrieval successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q dvc[s3]\n",
        "%cd {gpt_neox_colabDir}\n",
        "!dvc pull {'data/shakespeare/model.tar.gz'}\n",
        "%cd {gpt_neox_colabDir}/data/shakespeare\n",
        "!tar -xzf model.tar.gz"
      ],
      "metadata": {
        "id": "e8mTYol2Ble1",
        "outputId": "ccd5b34e-9901-4cc3-c500-b5f4fd1eef13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h/content/GPT-NeoX-Colab\n",
            "Collecting          |0.00 [00:00,    ?entry/s]\n",
            "Fetching\n",
            "!\u001b[A\n",
            "  0% |          |0/? [00:00<?,    ?files/s]\u001b[A\n",
            "Fetching\n",
            "Building workspace index          |3.00 [00:00,  968entry/s]\n",
            "Comparing indexes          |4.00 [00:00,  993entry/s]\n",
            "Applying changes          |0.00 [00:00,     ?file/s]\n",
            "Everything is up to date.\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7AOL1NJ43e"
      },
      "source": [
        "# Inference with Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "01ZRN2IceM8a",
        "outputId": "95c580c7-d561-4ce1-8004-c37890413c27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(512, 256)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=256, out_features=768, bias=True)\n",
              "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=256, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from transformers import GPTNeoXForCausalLM\n",
        "import torch\n",
        "\n",
        "# Move to model directory\n",
        "%cd {gpt_neox_colabDir}\n",
        "\n",
        "# Assuming CharLevelTokenizer is properly imported and instantiated\n",
        "from gpt_neox_colab import CharLevelTokenizer\n",
        "tokenizer = CharLevelTokenizer.CharLevelTokenizer(vocab_size=512)\n",
        "\n",
        "# Load your model\n",
        "model_path = \"data/shakespeare\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define a simple char-level tokenizer if not provided\n",
        "def char_level_tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def char_level_detokenize(tokens):\n",
        "    return tokenizer.detokenize(tokens)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "whJ1g0vJUXGc",
        "outputId": "47132321-fb4f-495b-91f7-f8f6ed8d7648",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your prompt: Mark is a \n",
            "Generated text: Mark is a begger. But to the counted looks. Than thou and my cous. Thereloved so office, And as you be the sea; whose that CLIFFORD: Of my should duke to speak. And more shall deed. And may man? while\n"
          ]
        }
      ],
      "source": [
        "# Prompt the user for input\n",
        "input_text = input(\"Enter your prompt: \")\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([tokenizer.tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}