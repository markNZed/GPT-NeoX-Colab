{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/venv/notebooks/shakespeare_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lBrppZd_A0"
      },
      "source": [
        "# Training a tiny LLM on a corpus of Shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYyxsLpOuQR1"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "print(\"Current Date and Time:\", datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBH4eN9Id_A7"
      },
      "outputs": [],
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "workspaceDir = \"/content\"\n",
        "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
        "GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e27VRVq07s"
      },
      "source": [
        "# Cloning Git Repos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j_hUsQxlhnou"
      },
      "outputs": [],
      "source": [
        "#@title Clone GPT-NeoX-Colab\n",
        "%%time\n",
        "%cd {workspaceDir}\n",
        "# Don't use --depth 1 because that does not play nice with git-annex\n",
        "!git clone -b venv https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "%cd {GPTNeoXColabDir}\n",
        "!pip install . > /dev/null 2>&1\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
        "import GPTNeoXColab\n",
        "GPTNeoXColab.utils.colab.install_git_annex()\n",
        "GPTNeoXColab.utils.colab.enable_remote()\n",
        "GPTNeoXColab.utils.colab.sync_annex()\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare.txt\")\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare.jsonl\")\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.bin\")\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare/shakespeare_text_document.idx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7AOL1NJ43e"
      },
      "source": [
        "# Inference with Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01ZRN2IceM8a"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoXForCausalLM\n",
        "import torch\n",
        "\n",
        "# Move to model directory\n",
        "%cd {GPTNeoXDir}\n",
        "\n",
        "# Assuming CharLevelTokenizer is properly imported and instantiated\n",
        "from GPTNeoXColab import CharLevelTokenizer\n",
        "tokenizer = CharLevelTokenizer.CharLevelTokenizer(vocab_size=512)\n",
        "\n",
        "# Load your model\n",
        "model_path = f\"{GPTNeoXColabDir}/data/shakespeare\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define a simple char-level tokenizer if not provided\n",
        "def char_level_tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def char_level_detokenize(tokens):\n",
        "    return tokenizer.detokenize(tokens)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user for input\n",
        "input_text = input(\"Enter your prompt: \")\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([char_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = char_level_detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
