{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/shakespeare_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lBrppZd_A0"
      },
      "source": [
        "# Inference of a tiny Shakespeare LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uYyxsLpOuQR1",
        "outputId": "1f8c7c7c-a382-4a2c-c1a6-0afb035d22a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Date and Time: 2024-11-28 14:08:56.902769\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(\"Current Date and Time:\", datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HBH4eN9Id_A7"
      },
      "outputs": [],
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "workspaceDir = \"/content\"\n",
        "gpt_neox_colabDirName = \"GPT-NeoX-Colab\"\n",
        "gpt_neox_colabDir = f\"{workspaceDir}/{gpt_neox_colabDirName}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!curl -LsSf https://astral.sh/uv/install.sh | sh"
      ],
      "metadata": {
        "id": "E0IJElXTzNtA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e27VRVq07s"
      },
      "source": [
        "# Cloning Git Repos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "j_hUsQxlhnou",
        "outputId": "c2800028-4e22-40e9-9ce1-07abcee649a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'GPT-NeoX-Colab'...\n",
            "remote: Enumerating objects: 1669, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 1669 (delta 8), reused 8 (delta 2), pack-reused 1637 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1669/1669), 18.38 MiB | 26.65 MiB/s, done.\n",
            "Resolving deltas: 100% (977/977), done.\n",
            "CPU times: user 63.1 ms, sys: 6.87 ms, total: 69.9 ms\n",
            "Wall time: 3.63 s\n"
          ]
        }
      ],
      "source": [
        "#@title Clone GPT-NeoX-Colab\n",
        "%%time\n",
        "%cd {workspaceDir}\n",
        "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "#%cd {gpt_neox_colabDir}\n",
        "#!uv sync -q --dev\n",
        "#!uv run pip install -q -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This runs python in the virtual environment\n",
        "#%%bash -s \"$gpt_neox_colabDir\"\n",
        "#source \"$1/.venv/bin/activate\"\n",
        "#python -c \"\n",
        "#from dotenv import load_dotenv\n",
        "#import os\n",
        "#load_dotenv('$1/.env')\n",
        "#import gpt_neox_colab\n",
        "#gpt_neox_colab.utils.colab.fetch_data('data/shakespeare/model.tar.gz')\n",
        "#\""
      ],
      "metadata": {
        "id": "XSgnc16q_1nx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q dvc[s3] python-dotenv\n",
        "%cd {gpt_neox_colabDir}\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv('.env')\n",
        "!dvc pull -q {'data/shakespeare/model.tar.gz'}\n",
        "%cd {gpt_neox_colabDir}/data/shakespeare\n",
        "!tar -xzf model.tar.gz"
      ],
      "metadata": {
        "id": "e8mTYol2Ble1",
        "outputId": "9606fb67-9b5b-4bb4-9ea8-9d157cea9511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n",
            "\u001b[0m/content/GPT-NeoX-Colab/data/shakespeare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7AOL1NJ43e"
      },
      "source": [
        "# Inference with Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "01ZRN2IceM8a",
        "outputId": "291b376e-89cc-4347-ccb2-601aa2be59bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(512, 256)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXSdpaAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=256, out_features=768, bias=True)\n",
              "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "  )\n",
              "  (embed_out): Linear(in_features=256, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from transformers import GPTNeoXForCausalLM\n",
        "import torch\n",
        "\n",
        "# Move to model directory\n",
        "%cd {gpt_neox_colabDir}\n",
        "\n",
        "# Assuming CharLevelTokenizer is properly imported and instantiated\n",
        "from src.gpt_neox_colab.CharLevelTokenizer import CharLevelTokenizer\n",
        "tokenizer = CharLevelTokenizer(vocab_size=512)\n",
        "\n",
        "# Load your model\n",
        "model_path = \"data/shakespeare\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define a simple char-level tokenizer if not provided\n",
        "def char_level_tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def char_level_detokenize(tokens):\n",
        "    return tokenizer.detokenize(tokens)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "whJ1g0vJUXGc",
        "outputId": "62c4c8ff-be84-40ce-ee45-2986258d32a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your prompt: test\n",
            "Generated text: test of my with her so. I have the most of the To ding of deaths. And bitted for man! I did to our heads. For what we have: what with you. BRAKENBURY: I can the books in the most in a That has morned \n"
          ]
        }
      ],
      "source": [
        "# Prompt the user for input\n",
        "input_text = input(\"Enter your prompt: \")\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([tokenizer.tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}