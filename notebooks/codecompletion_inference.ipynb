{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/codecompletion_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  DOCKER = False\n",
    "except:\n",
    "  DOCKER = True\n",
    "print(DOCKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8tGrS9KJu7QA"
   },
   "outputs": [],
   "source": [
    "# We could modify these paths to \"stub\" behavior for test/dev\n",
    "workspaceDir = \"/content\"\n",
    "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
    "if DOCKER:\n",
    "    GPTNeoXColabDir = f\"/workspace\"\n",
    "else:\n",
    "    GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOySwjeyktsH",
    "outputId": "ed85ff10-8381-4ff0-928d-b336b10f9a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n",
      "CPU times: user 440 μs, sys: 2.84 ms, total: 3.28 ms\n",
      "Wall time: 2.64 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#@title Clone GPT-NeoX-Colab\n",
    "if DOCKER:\n",
    "    %cd {GPTNeoXColabDir}\n",
    "else:\n",
    "    %cd {workspaceDir}\n",
    "    # Don't use --depth 1 because that does not play nice with git-annex\n",
    "    !git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
    "    %cd {GPTNeoXColabDir}\n",
    "    %pip install -q -r requirements_colab.txt\n",
    "    %pip install -q ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yu4yRpEzCyF6",
    "outputId": "16e01391-a108-4107-939d-f621eeec5101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data retrieval successful.\n",
      "/workspace/data/codecompletion\n",
      "/workspace\n",
      "Data retrieval successful.\n",
      "/workspace/models/codecompletion\n"
     ]
    }
   ],
   "source": [
    "%cd {GPTNeoXColabDir}\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
    "import GPTNeoXColab\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"data/codecompletion/token_completion.tar.gz\")\n",
    "%cd {GPTNeoXColabDir}/data/codecompletion\n",
    "if not os.path.exists(f\"data/codecompletion/token_completion\"):\n",
    "    !tar -xzf token_completion.tar.gz\n",
    "%cd {GPTNeoXColabDir}\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"models/codecompletion/global_step7000_HF.tar.gz\")\n",
    "%cd {GPTNeoXColabDir}/models/codecompletion\n",
    "if not os.path.exists(f\"latest\"):\n",
    "    !tar -xzf global_step7000_HF.tar.gz\n",
    "    !mv global_step7000_HF latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ7f8hTqipgF"
   },
   "source": [
    "# Using Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmOgA5alzT2A",
    "outputId": "27200673-21b0-4cfe-a913-4be9b27807f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/models/codecompletion/latest\n",
      "--2024-11-15 22:30:55--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.15.193.188, 52.217.129.248, 52.217.1.198, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.15.193.188|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: ‘gpt2-vocab.json’\n",
      "\n",
      "gpt2-vocab.json     100%[===================>]   1018K   791KB/s    in 1.3s    \n",
      "\n",
      "2024-11-15 22:30:57 (791 KB/s) - ‘gpt2-vocab.json’ saved [1042301/1042301]\n",
      "\n",
      "--2024-11-15 22:30:57--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.12.150, 52.217.91.182, 52.217.168.144, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.12.150|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [text/plain]\n",
      "Saving to: ‘gpt2-merges.txt’\n",
      "\n",
      "gpt2-merges.txt     100%[===================>] 445.62K   845KB/s    in 0.5s    \n",
      "\n",
      "2024-11-15 22:30:58 (845 KB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd {GPTNeoXColabDir}/models/codecompletion/latest\n",
    "if not os.path.exists(\"gpt2-vocab.json\"):\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
    "    !mv gpt2-vocab.json vocab.json\n",
    "if not os.path.exists(\"gpt2-merges.txt\"):\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
    "    !mv gpt2-merges.txt merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNB4sSsS-RN3"
   },
   "source": [
    "# HuggingFace Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztr-ItKf_G1M",
    "outputId": "e4038014-0efa-4112-e328-7e5cc0c92b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Generated text: <s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django . conf . urls import url <EOL> from django . conf import settings <EOL> from django . utils import six <EOL> from django . utils import six <EOL> from django . utils import six <EOL> from django . utils import six <EOL> from django . utils import six <EOL> from django . utils import six <EOL> from django . utils import six <EOL> from django . utils import six <EOL> from django . utils import six <EOL> from django . utils import six <EOL> from django . utils import six <EOL> from django .\n",
      "Final text: import sys , os \n",
      " import imp \n",
      " from optparse import make_option \n",
      " from django . conf import settings \n",
      " from django . conf . urls import url \n",
      " from django . conf import settings \n",
      " from django . utils import six \n",
      " from django . utils import six \n",
      " from django . utils import six \n",
      " from django . utils import six \n",
      " from django . utils import six \n",
      " from django . utils import six \n",
      " from django . utils import six \n",
      " from django . utils import six \n",
      " from django . utils import six \n",
      " from django . utils import six \n",
      " from django . utils import six \n",
      " from django .\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "%cd {workspaceDir}\n",
    "\n",
    "# Initialize the tokenizer with your vocabulary and merge files\n",
    "tokenizer = GPT2Tokenizer(vocab_file=f\"{GPTNeoXColabDir}/models/codecompletion/latest/vocab.json\", merges_file=f\"{GPTNeoXColabDir}/models/codecompletion/latest/merges.txt\")\n",
    "\n",
    "# Load your model\n",
    "model_path = f\"{GPTNeoXColabDir}/models/codecompletion/latest\"\n",
    "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Prompt the user for input\n",
    "input_text = \"\"\"<s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django\"\"\"\n",
    "\n",
    "# Tokenize and prepare input\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
    "\n",
    "# Generate text with specified pad_token_id and attention_mask\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=200,          # Adjust this for desired output length\n",
    "        temperature=0.7,        # Controls creativity\n",
    "        top_k=50,               # Controls diversity\n",
    "        top_p=0.9,              # Nucleus sampling\n",
    "        num_return_sequences=1, # Number of sequences to return\n",
    "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
    "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0].tolist())\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "# Function to replace special tokens with original representations\n",
    "def replace_special_tokens(text):\n",
    "    \"\"\"\n",
    "    Replaces special tokens in the generated text with more readable or context-appropriate representations.\n",
    "    \"\"\"\n",
    "    replacements = {\n",
    "        \"<EOL>\": \"\\n\",          # Replace with actual newline for code formatting\n",
    "        \"<s>\": \"\",              # Remove start token as it's not necessary in final output\n",
    "        \"</s>\": \"\",             # Remove end token as it's not necessary in final output\n",
    "        \"<pad>\": \"\",            # Remove padding tokens\n",
    "        \"<|UNKNOWN|>\": \"[UNK]\", # Represent unknown tokens in a readable way\n",
    "        \"<STR_LIT>\": \"\\\"STRING_LITERAL\\\"\",  # Placeholder for string literals\n",
    "        \"<NUM_LIT>\": \"0\",       # Placeholder for numeric literals\n",
    "        \"<BOOL_LIT>\": \"True\",   # Placeholder for boolean literals (e.g., True/False)\n",
    "        \"<COMMENT>\": \"# COMMENT\",  # Placeholder for comments in the code\n",
    "    }\n",
    "\n",
    "    # Replace each special token in text with its corresponding value in `replacements`\n",
    "    for token, replacement in replacements.items():\n",
    "        text = text.replace(token, replacement)\n",
    "\n",
    "    return text.strip()  # Strip leading/trailing whitespace for clean output\n",
    "\n",
    "# Replace special tokens in the generated text\n",
    "final_text = replace_special_tokens(generated_text)\n",
    "\n",
    "# Print the final output\n",
    "print(\"Final text:\", final_text)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
