{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "workspaceDir = \"/content\"\n",
        "GPTNeoXDirName = \"gpt-neox\"\n",
        "GPTNeoXDir = f\"{workspaceDir}/{GPTNeoXDirName}\"\n",
        "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
        "GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
      ],
      "metadata": {
        "id": "8tGrS9KJu7QA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpgI19mPrtvy"
      },
      "source": [
        "# Clone CodeXGLUE Repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/CodeXGLUE.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qBcQotWw1U0",
        "outputId": "4350c28f-1868-4647-dd17-b2b34ac83e9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CodeXGLUE'...\n",
            "remote: Enumerating objects: 3373, done.\u001b[K\n",
            "remote: Counting objects: 100% (3372/3372), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1534/1534), done.\u001b[K\n",
            "remote: Total 3373 (delta 1748), reused 3326 (delta 1733), pack-reused 1 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3373/3373), 213.15 MiB | 14.59 MiB/s, done.\n",
            "Resolving deltas: 100% (1748/1748), done.\n",
            "Updating files: 100% (400/400), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOySwjeyktsH",
        "outputId": "d3964b8e-8c0a-4788-98ac-bbade967f16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'GPT-NeoX-Colab'...\n",
            "remote: Enumerating objects: 1091, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 1091 (delta 16), reused 14 (delta 6), pack-reused 1052 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1091/1091), 13.32 MiB | 13.78 MiB/s, done.\n",
            "Resolving deltas: 100% (593/593), done.\n",
            "/content/GPT-NeoX-Colab\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.5/251.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.0/426.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.5/456.5 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.6/367.6 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.4/901.4 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.7/548.7 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.1/569.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.1/113.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.2/722.2 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for configobj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pysftp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for GPTNeoXColab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Data retrieval successful.\n",
            "CPU times: user 5.99 s, sys: 689 ms, total: 6.68 s\n",
            "Wall time: 2min 50s\n"
          ]
        }
      ],
      "source": [
        "#@title Clone GPT-NeoX-Colab\n",
        "%%time\n",
        "%cd {workspaceDir}\n",
        "# Don't use --depth 1 because that does not play nice with git-annex\n",
        "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "%cd {GPTNeoXColabDir}\n",
        "%pip install -q -r requirements_colab.txt\n",
        "%pip install -q .\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
        "import GPTNeoXColab\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/codecompletion/processed_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m7so6NvF6J5",
        "outputId": "9bd49d81-27de-4edf-cab3-cd4f05900070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/gpt-neox/processed_data\n",
        "!cp {GPTNeoXColabDir}/data/codecompletion/processed_data/* /content/gpt-neox/processed_data\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0FMSSnXzW0"
      },
      "source": [
        "# Cloning GPT-NeoX Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OguN3qpTQAQs",
        "outputId": "2ca41b68-81e2-49b4-8998-234a120463ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'gpt-neox'...\n",
            "remote: Enumerating objects: 19539, done.\u001b[K\n",
            "remote: Counting objects: 100% (1742/1742), done.\u001b[K\n",
            "remote: Compressing objects: 100% (844/844), done.\u001b[K\n",
            "remote: Total 19539 (delta 1281), reused 1240 (delta 890), pack-reused 17797 (from 1)\u001b[K\n",
            "Receiving objects: 100% (19539/19539), 113.66 MiB | 22.36 MiB/s, done.\n",
            "Resolving deltas: 100% (14138/14138), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/EleutherAI/gpt-neox.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UV1kBtFXkKo"
      },
      "source": [
        "# Downloading and Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fn0Wy7g7lGGe",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b044bab4-b449-40e6-8480-1c6731ab73cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists, skipping download and preprocessing.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the file path you want to check for existence\n",
        "token_completion_path = \"/content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150/token_completion\"  # Replace with the actual file name you want to check\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(token_completion_path):\n",
        "    # Change directory\n",
        "    %cd /content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150\n",
        "    # Run the shell script to download and extract\n",
        "    !bash /content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150/download_and_extract.sh\n",
        "    # Run the preprocessing Python script\n",
        "    !python preprocess.py --base_dir=py150_files --output_dir=token_completion\n",
        "else:\n",
        "    print(\"File already exists, skipping download and preprocessing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWTuy456X7Yl"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QH7iL-O-Qq3A",
        "outputId": "05b5d1b6-45c2-45ca-d2a1-3b968d01cc6e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n",
            "Collecting torch==2.3\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchaudio==2.3.0\n",
            "  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting torchvision==0.18.0\n",
            "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting transformers==4.41.0\n",
            "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (10.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3) (3.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3) (1.3.0)\n",
            "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=a683eb1a7879170e10e6588e988ea5f3a22548792795642a17674b34c3721f0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, transformers, torchvision, torchaudio, sentence-transformers\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.0+cu121\n",
            "    Uninstalling torchvision-0.20.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.0+cu121\n",
            "    Uninstalling torchaudio-2.5.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.0+cu121\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.2.1\n",
            "    Uninstalling sentence-transformers-3.2.1:\n",
            "      Successfully uninstalled sentence-transformers-3.2.1\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.2.2 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0 transformers-4.41.0 triton-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "transformers"
                ]
              },
              "id": "34ffa1c61dff48e8b3f64f1f2218e011"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed (from -r ./requirements/requirements.txt (line 1))\n",
            "  Cloning https://github.com/EleutherAI/DeeperSpeed.git (to revision 02e2ebf7dee6aaab3d89094ed470a4609763c742) to /tmp/pip-install-rzs0j60i/deepspeed_eda218d77d324a50b5e3c962cdf4ff1e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/EleutherAI/DeeperSpeed.git /tmp/pip-install-rzs0j60i/deepspeed_eda218d77d324a50b5e3c962cdf4ff1e\n",
            "  Running command git rev-parse -q --verify 'sha^02e2ebf7dee6aaab3d89094ed470a4609763c742'\n",
            "  Running command git fetch -q https://github.com/EleutherAI/DeeperSpeed.git 02e2ebf7dee6aaab3d89094ed470a4609763c742\n",
            "  Running command git checkout -q 02e2ebf7dee6aaab3d89094ed470a4609763c742\n",
            "  Resolved https://github.com/EleutherAI/DeeperSpeed.git to commit 02e2ebf7dee6aaab3d89094ed470a4609763c742\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lm_dataformat@ git+https://github.com/EleutherAI/lm_dataformat.git@4eec05349977071bf67fc072290b95e31c8dd836 (from -r ./requirements/requirements.txt (line 5))\n",
            "  Cloning https://github.com/EleutherAI/lm_dataformat.git (to revision 4eec05349977071bf67fc072290b95e31c8dd836) to /tmp/pip-install-rzs0j60i/lm-dataformat_465228bfa2d44e85872f848667b1f73a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/EleutherAI/lm_dataformat.git /tmp/pip-install-rzs0j60i/lm-dataformat_465228bfa2d44e85872f848667b1f73a\n",
            "  Running command git rev-parse -q --verify 'sha^4eec05349977071bf67fc072290b95e31c8dd836'\n",
            "  Running command git fetch -q https://github.com/EleutherAI/lm_dataformat.git 4eec05349977071bf67fc072290b95e31c8dd836\n",
            "  Resolved https://github.com/EleutherAI/lm_dataformat.git to commit 4eec05349977071bf67fc072290b95e31c8dd836\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy>=6.0.1 (from -r ./requirements/requirements.txt (line 2))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: huggingface_hub>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements.txt (line 3)) (0.24.7)\n",
            "Requirement already satisfied: jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements.txt (line 4)) (3.1.4)\n",
            "Collecting lm_eval<=0.4.1,>=0.4.0 (from -r ./requirements/requirements.txt (line 6))\n",
            "  Downloading lm_eval-0.4.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting mpi4py>=3.0.3 (from -r ./requirements/requirements.txt (line 7))\n",
            "  Downloading mpi4py-4.0.1.tar.gz (466 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.2/466.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements.txt (line 8)) (1.26.4)\n",
            "Collecting pybind11>=2.6.2 (from -r ./requirements/requirements.txt (line 9))\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements.txt (line 10)) (2024.9.11)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements.txt (line 11)) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements.txt (line 12)) (1.16.0)\n",
            "Collecting tiktoken>=0.1.2 (from -r ./requirements/requirements.txt (line 13))\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements.txt (line 14)) (0.19.1)\n",
            "Collecting transformers==4.38.0 (from -r ./requirements/requirements.txt (line 15))\n",
            "  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2==3.1.4->-r ./requirements/requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->-r ./requirements/requirements.txt (line 15)) (3.16.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->-r ./requirements/requirements.txt (line 15)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->-r ./requirements/requirements.txt (line 15)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->-r ./requirements/requirements.txt (line 15)) (2.32.3)\n",
            "Collecting tokenizers>=0.12.1 (from -r ./requirements/requirements.txt (line 14))\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->-r ./requirements/requirements.txt (line 15)) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->-r ./requirements/requirements.txt (line 15)) (4.66.6)\n",
            "Collecting hjson (from deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ninja (from deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (9.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (2.9.2)\n",
            "Collecting pynvml (from deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1))\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.0.1->-r ./requirements/requirements.txt (line 2)) (0.2.13)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.11.0->-r ./requirements/requirements.txt (line 3)) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.11.0->-r ./requirements/requirements.txt (line 3)) (4.12.2)\n",
            "Collecting zstandard (from lm_dataformat@ git+https://github.com/EleutherAI/lm_dataformat.git@4eec05349977071bf67fc072290b95e31c8dd836->-r ./requirements/requirements.txt (line 5))\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting jsonlines (from lm_dataformat@ git+https://github.com/EleutherAI/lm_dataformat.git@4eec05349977071bf67fc072290b95e31c8dd836->-r ./requirements/requirements.txt (line 5))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting ujson (from lm_dataformat@ git+https://github.com/EleutherAI/lm_dataformat.git@4eec05349977071bf67fc072290b95e31c8dd836->-r ./requirements/requirements.txt (line 5))\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (0.34.2)\n",
            "Collecting evaluate (from lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.14.0 (from lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (2.10.1)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (0.13.2)\n",
            "Collecting pytablewriter (from lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=1.5.0 (from lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (1.5.2)\n",
            "Collecting sqlitedict (from lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm-multiprocess (from lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (2.2.2)\n",
            "Collecting xxhash (from datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface_hub>=0.11.0->-r ./requirements/requirements.txt (line 3))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (3.10.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0->-r ./requirements/requirements.txt (line 15)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0->-r ./requirements/requirements.txt (line 15)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0->-r ./requirements/requirements.txt (line 15)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0->-r ./requirements/requirements.txt (line 15)) (2024.8.30)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (3.8.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (2.10.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (5.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (3.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->lm_dataformat@ git+https://github.com/EleutherAI/lm_dataformat.git@4eec05349977071bf67fc072290b95e31c8dd836->-r ./requirements/requirements.txt (line 5)) (24.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (2.23.4)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (75.1.0)\n",
            "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (3.2.1)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading tcolorpy-0.1.6-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6))\n",
            "  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (4.0.3)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (5.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed@ git+https://github.com/EleutherAI/DeeperSpeed.git@02e2ebf7dee6aaab3d89094ed470a4609763c742#egg=deepspeed->-r ./requirements/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.14.0->lm_eval<=0.4.1,>=0.4.0->-r ./requirements/requirements.txt (line 6)) (0.2.0)\n",
            "Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_eval-0.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.6-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed, lm_dataformat, mpi4py, rouge-score, sqlitedict\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.12.4+02e2ebf-py3-none-any.whl size=1286069 sha256=0fc13030d528096108f0b0ed346699c6da45567efc15eb98e5516636e8bad38c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/a3/16/4d52efba511c9490fb634597e2b4b6023819175fccfb7e4453\n",
            "  Building wheel for lm_dataformat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lm_dataformat: filename=lm_dataformat-0.0.20-py3-none-any.whl size=5833 sha256=6c6c6a174394c1e11073167578390b08c25e454e5267d0c43ff64b84edc396a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/5d/39/858622f394e968f5055c63d6023137b79341dfe415baf84098\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.1-cp310-cp310-linux_x86_64.whl size=4266343 sha256=740c8b7261a2bf7512f0c7f737883bbfdb0336e44a9baab2c160373a06108cc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/ca/13/13218a83854023ccec184e3af482f0f038b434aa32c19afee8\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=a9c206973c62ce6b93582d12220664fa80d7602730fda685a50102eb2d0a4984\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=6a551c66efbb0c2ff77db9b903c1e0b2eb36de1e7d4390dbc5997fb1b79112bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built deepspeed lm_dataformat mpi4py rouge-score sqlitedict\n",
            "Installing collected packages: sqlitedict, ninja, hjson, zstandard, xxhash, ujson, tqdm-multiprocess, tcolorpy, sacrebleu, pynvml, pybind11, mpi4py, mbstrdecoder, jsonlines, ftfy, fsspec, dill, typepy, tiktoken, rouge-score, multiprocess, lm_dataformat, tokenizers, transformers, deepspeed, DataProperty, tabledata, datasets, pytablewriter, evaluate, lm_eval\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.0\n",
            "    Uninstalling transformers-4.41.0:\n",
            "      Successfully uninstalled transformers-4.41.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "s3fs 2024.10.0 requires fsspec==2024.10.0.*, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed DataProperty-1.0.1 datasets-3.1.0 deepspeed-0.12.4+02e2ebf dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 ftfy-6.3.1 hjson-3.1.0 jsonlines-4.0.0 lm_dataformat-0.0.20 lm_eval-0.4.1 mbstrdecoder-1.1.3 mpi4py-4.0.1 multiprocess-0.70.16 ninja-1.11.1.1 pybind11-2.13.6 pynvml-11.5.3 pytablewriter-1.2.0 rouge-score-0.1.2 sacrebleu-2.4.3 sqlitedict-2.1.0 tabledata-1.3.3 tcolorpy-0.1.6 tiktoken-0.8.0 tokenizers-0.15.2 tqdm-multiprocess-0.0.11 transformers-4.38.0 typepy-1.3.2 ujson-5.10.0 xxhash-3.5.0 zstandard-0.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "35635b447a4543af912b64b20b37eeb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.38 s, sys: 505 ms, total: 3.89 s\n",
            "Wall time: 7min 34s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%cd /content/gpt-neox\n",
        "# Could not redirect to /dev/null in the standard Colab notebook (maybe no output for a particular time?)\n",
        "# Currently deepspeed from GTP-NeoX is not compatible with logging in torch >= 2.4\n",
        "!pip install torch==2.3 torchaudio==2.3.0 torchvision==0.18.0 transformers==4.41.0 sentence-transformers==2.2.2\n",
        "!pip install -r ./requirements/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwrfbvLpYXAC"
      },
      "source": [
        "# Preparing Custom Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDlSqS1iyU8k",
        "outputId": "81e8fb73-d62b-4c9c-c205-7244f7abe79f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%cd /content/gpt-neox\n",
        "!!mkdir -p data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "StUBNuLhHUPm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Define the file path you want to check for existence\n",
        "token_completion_path = \"/content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150/token_completion\"\n",
        "\n",
        "# Generate a list of dictionaries\n",
        "if not os.path.exists(token_completion_path):\n",
        "  lines = []\n",
        "  with open(\"/content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150/token_completion/train.txt\", encoding=\"utf8\") as f:\n",
        "      for line in f.read().splitlines():\n",
        "          if line:\n",
        "              lines.append({\"text\": line})\n",
        "\n",
        "  # Convert to a list of JSON strings\n",
        "  json_lines = [json.dumps(l) for l in lines]\n",
        "\n",
        "  # Join lines and save to .jsonl file\n",
        "  json_data = '\\n'.join(json_lines)\n",
        "  with open('/content/gpt-neox/data/py95K_train.jsonl', 'w') as f:\n",
        "      f.write(json_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ7f8hTqipgF"
      },
      "source": [
        "# Using Byte-Pair Encoding Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmOgA5alzT2A",
        "outputId": "ced2b129-3c3c-4aad-8db4-b757784fe6db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox/data\n",
            "--2024-11-10 17:34:18--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.129.16, 54.231.201.240, 3.5.2.46, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.129.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‘gpt2-vocab.json’\n",
            "\n",
            "gpt2-vocab.json     100%[===================>]   1018K  2.74MB/s    in 0.4s    \n",
            "\n",
            "2024-11-10 17:34:19 (2.74 MB/s) - ‘gpt2-vocab.json’ saved [1042301/1042301]\n",
            "\n",
            "--2024-11-10 17:34:19--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.129.16, 54.231.201.240, 3.5.2.46, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.129.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [text/plain]\n",
            "Saving to: ‘gpt2-merges.txt’\n",
            "\n",
            "gpt2-merges.txt     100%[===================>] 445.62K  1.81MB/s    in 0.2s    \n",
            "\n",
            "2024-11-10 17:34:19 (1.81 MB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd data\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHwG3CzJOfpy",
        "outputId": "5a033f5d-34b1-4928-c2cf-8a46778246e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab/data\n",
            "tar: py150_processed.tar.gz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "cp: cannot stat 'py150_processed/*': No such file or directory\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/GPT-NeoX-Colab/data\n",
        "!tar -xf py150_processed.tar.gz\n",
        "!mkdir -p /content/gpt-neox/processed_data\n",
        "!cp py150_processed/* /content/gpt-neox/processed_data\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xh49c3aQ1WW",
        "outputId": "426579d6-e5dd-48f0-cab2-63463e45a1fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists, skipping download and preprocessing.\n",
            "CPU times: user 72 µs, sys: 9 µs, total: 81 µs\n",
            "Wall time: 85.4 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import os\n",
        "\n",
        "# Define the file path you want to check for existence\n",
        "file_path = \"/content/gpt-neox/processed_data\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_path):\n",
        "  %cd /content/gpt-neox\n",
        "  !mkdir -p processed_data\n",
        "  !python tools/datasets/preprocess_data.py \\\n",
        "    --input ./data/py95K_train.jsonl \\\n",
        "    --vocab ./data/gpt2-vocab.json \\\n",
        "    --merge-file ./data/gpt2-merges.txt \\\n",
        "    --output-prefix ./processed_data/py150 \\\n",
        "    --tokenizer-type GPT2BPETokenizer \\\n",
        "    --dataset-impl mmap \\\n",
        "    --append-eod\n",
        "else:\n",
        "    print(\"File already exists, skipping download and preprocessing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2bv8UUYqRBp"
      },
      "source": [
        "# Tokens count in Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "aa4762c7bacb4dc1844916f3bfd8710c",
            "6cd3f021ba3842e0b9f4ff0c00874224",
            "8a89a879629a4e0bb330b2214721215a",
            "edbfcdd2466c40ffa00e1595cbe849ac",
            "a9eac3ece1c4438cb1aaf0c0e227ad49",
            "9914b000602c495db72858496eb694b6",
            "31a0249cd5c4469b85e3376ac42684de",
            "b5960de3624740668bc32dc4e7a4b01c",
            "af2ad671425d43c5a7ca1e6c631b9a55",
            "fe58e76db375459ba3e7a9a28ae1afa4",
            "4768c1c70f844ddea0ce49eb15cc6a60",
            "197713b919cc4c269776eedcec9c0ca9",
            "707ab5d9b89d468b915c3b21ad17aba7",
            "944d6f4e21d644eb94a0689def5c3c57",
            "2637a5a2465743efba83b5c63f1bbf80",
            "f9c7483621c04244a0146a1ae6d51725",
            "8854031067824f7c81076a524d225678",
            "6bd8b9c87f6e4cdda5693214f22fbc6e",
            "8f7ead849f68487ea1e74cd785442112",
            "96123ffd1df44949acddd0ddbac5af95",
            "af6a2a18d4914cc28710959adfe960ad",
            "3a1ad81812d1495dab13de93262b2f2e",
            "ade8650955d844418cf162899f3ccca5",
            "9a1561cb7665412ead4cfaaa23d7912f",
            "48ccf0535996412c80e8defc161561ae",
            "9f947f0fa65f4c049268fdb640fd7c87",
            "2b6c6a72577647afb3603a82a7b0ee05",
            "8509af4aa5bc4c0c9b0dc17c6704250e",
            "de89d28d6e2a4d76956f65c144c1a6aa",
            "ce73a68c9ffc46ca9aba0afee3ce3afe",
            "1b73d3378a484bbc98a8e5f4f600b592",
            "9627f935096d4c829aa24e8196127889",
            "1eec404629864d6dab0514845cee97e4",
            "15810cfda50044559763ffadc5459fa3",
            "885cc9a2d11b4e778d3dfae9deb25402",
            "6433e05d8c9047758cbdfe0830433423",
            "112bfd2dbb7940c0a524945676c49e79",
            "222cca687050497fa3a98390b327f258",
            "dce82ae405f74318884b4c6d457d93a1",
            "938b842be0fe41709c56813ae9ece40c",
            "c3f576cc337d41b39e260d9a0ddaa7e3",
            "aab64d421dbc44078cb779c2d912db43",
            "8ad17720fd1d4643a6d81cd89e8fc69f",
            "0621c7ae4b614e7395e1c92bd3535ed6",
            "9bcef93bfeaa4303a283a1fdf55532bb",
            "570c367cf297448188007453a84ebe7a",
            "05c79f1b886441cfa0e8d6845ffe2680",
            "6bf824739ef846d79d53349bb9e5d442",
            "f58bc3875ea24a92a14931d63eddad0b",
            "da0950d195dc4014b3a8c7591df1d4a8",
            "b6bffae6a34a48158cb56536224bab3d",
            "f35528d3781642ba9b0d0858e5c11141",
            "6d44435a75804e5ba5c8e83741186b72",
            "84be457457b54efeb1ccbcec3478be02",
            "09d7f86a07a14249b04bae3848b7e924"
          ]
        },
        "id": "YJzBk_-2etia",
        "outputId": "af1bbba1-1033-4236-ad2b-1250c8ffc99c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa4762c7bacb4dc1844916f3bfd8710c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "197713b919cc4c269776eedcec9c0ca9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ade8650955d844418cf162899f3ccca5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15810cfda50044559763ffadc5459fa3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bcef93bfeaa4303a283a1fdf55532bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total token count: 0\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Initialize the GPT-2 tokenizer (BPE-based)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Path to your text file\n",
        "file_path = \"/content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150/token_completion/train.txt\"\n",
        "\n",
        "# Initialize a token counter\n",
        "total_token_count = 0\n",
        "\n",
        "# Open the file and read line by line to count tokens\n",
        "#with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "#    for line in file:\n",
        "#        tokens = tokenizer.encode(line)\n",
        "#        total_token_count += len(tokens)\n",
        "        #print(total_token_count)\n",
        "\n",
        "print(f\"Total token count: {total_token_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miolaIFSa8sS"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a_t7Dpd3Wn0Y"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQknkkSEbMXa",
        "outputId": "bff50138-7d1a-4dc5-af89-668256d6c17c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n",
            "[2024-11-10 17:45:50,087] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "NeoXArgs.from_ymls() ['/content/GPT-NeoX-Colab/configs/CC19M.yml', '/content/GPT-NeoX-Colab/configs/cc_setup.yml']\n",
            "INFO:root:NeoXArgs.calculate_derived() Total number of GPUs determined to be: 1\n",
            "-------------------- arguments --------------------\n",
            "  attention_config ................ ['global', 'global', 'global', 'global', 'global', 'global']updated\n",
            "  batch_size ...................... 4...........................updated\n",
            "  checkpoint_activations .......... True........................updated\n",
            "  checkpoint_factor ............... 1000........................updated\n",
            "  config_files .................... {'CC19M.yml': '{\\n  \"pipe_parallel_size\": 0,\\n  \"model_parallel_size\": 1,\\n  \"tokenizer_type\": \"GPT2BPETokenizer\",\\n  \"vocab_file\": \"data/gpt2-vocab.json\",\\n  \"merge_file\": \"data/gpt2-merges.txt\",\\n  \"save\": \"checkpoints\",\\n  \"load\": \"checkpoints\",\\n  \"checkpoint_validation_with_forward_pass\": False,\\n\\n\\n  # model settings\\n  \"num_layers\": 6,\\n  \"hidden_size\": 512,\\n  \"num_attention_heads\": 8,\\n  \"seq_length\": 2048,\\n  \"max_position_embeddings\": 2048,\\n  \"pos_emb\": \"rotary\",\\n  \"no_weight_tying\": true,\\n  \"gpt_j_residual\": false,\\n  \"output_layer_parallelism\": \"column\",\\n\\n  \"scaled_upper_triang_masked_softmax_fusion\": false,\\n  \"bias_gelu_fusion\": false,\\n  \"rope_fusion\": false,\\n  \"layernorm_fusion\": false,\\n\\n  # init methods\\n  \"init_method\": \"small_init\",\\n  \"output_layer_init_method\": \"wang_init\",\\n\\n  \"optimizer\": {\\n    \"type\": \"Adam\",\\n    \"params\": {\\n      \"lr\": 0.001,\\n      \"betas\": [0.9, 0.95],\\n      \"eps\": 1.0e-8,\\n    }\\n  },\\n  \"min_lr\": 0.0001,\\n\\n  # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\\n  \"zero_optimization\": {\\n    \"stage\": 1,\\n    \"allgather_partitions\": True,\\n    \"allgather_bucket_size\": 500000000,\\n    \"overlap_comm\": True,\\n    \"reduce_scatter\": True,\\n    \"reduce_bucket_size\": 500000000,\\n    \"contiguous_gradients\": True,\\n  },\\n\\n  \"train_micro_batch_size_per_gpu\": 4, #32,\\n  \"gradient_accumulation_steps\": 1,\\n  \"data_impl\": \"mmap\",\\n  \"num_workers\": 1,\\n\\n  # activation checkpointing\\n  \"checkpoint_activations\": true,\\n  \"checkpoint_num_layers\": 1,\\n  \"partition_activations\": true,\\n  \"synchronize_each_layer\": true,\\n\\n  # regularization\\n  \"gradient_clipping\": 1.0,\\n  \"weight_decay\": 0.1,\\n  \"hidden_dropout\": 0,\\n  \"attention_dropout\": 0,\\n\\n  # precision settings\\n  \"fp16\": {\\n    \"fp16\": true,\\n    \"enabled\": true,\\n    \"loss_scale\": 0,\\n    \"loss_scale_window\": 1000,\\n    \"initial_scale_power\": 12,\\n    \"hysteresis\": 2,\\n    \"min_loss_scale\": 1,\\n  },\\n\\n  \"train_iters\": 100,\\n  \"lr_decay_iters\": 100,\\n  \"distributed_backend\": \"nccl\",\\n  \"lr_decay_style\": \"cosine\",\\n  \"warmup\": 0.01,\\n  \"checkpoint_factor\": 1000,\\n  \"eval_interval\": 40,\\n  \"eval_iters\": 10,\\n\\n  \"log_interval\": 10,\\n  \"steps_per_print\": 10,\\n  \"wall_clock_breakdown\": true,\\n\\n  # additional deepspeed args not specified above\\n  \"deepspeed_extra_args\": {\\n    \"comms_logger\": {\\n        \"enabled\": true,\\n        \"verbose\": true,\\n        \"prof_all\": true,\\n        \"debug\": false\\n    },\\n  }\\n\\n}\\n', 'cc_setup.yml': '# Suggested data paths when using GPT-NeoX locally\\n{\\n  \"data_path\": \"processed_data/py150_text_document\",\\n\\n  # or for weighted datasets:\\n  # \"train-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\\n  # \"test-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\\n  # \"valid-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\\n  # \"train-data-weights\": [1., 2.],\\n  # \"test-data-weights\": [2., 1.],\\n  # \"valid-data-weights\": [0.5, 0.4],\\n\\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\\n  # WARNING: setting this to True will override any user provided weights\\n  # \"weight_by_num_documents\": false,\\n  # \"weighted_sampler_alpha\": 0.3,\\n\\n#  \"vocab_file\": \"data/gpt2-vocab.json\",\\n#  \"merge_file\": \"data/gpt2-merges.txt\",\\n\\n  \"tensorboard_dir\": \"tensorboard\",\\n  \"log_dir\": \"logs\",\\n}\\n'}updated\n",
            "  data_impl ....................... mmap........................updated\n",
            "  data_path ....................... processed_data/py150_text_documentupdated\n",
            "  deepspeed_extra_args ............ {'comms_logger': {'enabled': True, 'verbose': True, 'prof_all': True, 'debug': False}}updated\n",
            "  dynamic_loss_scale .............. True........................updated\n",
            "  eval_interval ................... 40..........................updated\n",
            "  eval_iters ...................... 10..........................updated\n",
            "  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated\n",
            "  global_num_gpus ................. 1...........................updated\n",
            "  hidden_size ..................... 512.........................updated\n",
            "  init_method ..................... small_init..................updated\n",
            "  load ............................ checkpoints.................updated\n",
            "  log_dir ......................... logs........................updated\n",
            "  log_interval .................... 10..........................updated\n",
            "  lr .............................. 0.001.......................updated\n",
            "  lr_decay_iters .................. 100.........................updated\n",
            "  lr_decay_style .................. cosine......................updated\n",
            "  max_position_embeddings ......... 2048........................updated\n",
            "  merge_file ...................... data/gpt2-merges.txt........updated\n",
            "  min_lr .......................... 0.0001......................updated\n",
            "  no_weight_tying ................. True........................updated\n",
            "  num_attention_heads ............. 8...........................updated\n",
            "  num_layers ...................... 6...........................updated\n",
            "  num_workers ..................... 1...........................updated\n",
            "  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.001, 'betas': [0.9, 0.95], 'eps': 1e-08}}updated\n",
            "  optimizer_type .................. Adam........................updated\n",
            "  output_layer_init_method ........ wang_init...................updated\n",
            "  partition_activations ........... True........................updated\n",
            "  pos_emb ......................... rotary......................updated\n",
            "  precision ....................... fp16........................updated\n",
            "  save ............................ checkpoints.................updated\n",
            "  seq_length ...................... 2048........................updated\n",
            "  sparsity_config ................. {}..........................updated\n",
            "  synchronize_each_layer .......... True........................updated\n",
            "  tensorboard_dir ................. tensorboard.................updated\n",
            "  text_gen_type ................... unconditional...............updated\n",
            "  train_batch_size ................ 4...........................updated\n",
            "  train_iters ..................... 100.........................updated\n",
            "  train_micro_batch_size_per_gpu .. 4...........................updated\n",
            "  user_script ..................... train.py....................updated\n",
            "  vocab_file ...................... data/gpt2-vocab.json........updated\n",
            "  wall_clock_breakdown ............ True........................updated\n",
            "  zero_allgather_bucket_size ...... 500000000...................updated\n",
            "  zero_contiguous_gradients ....... True........................updated\n",
            "  zero_optimization ............... {'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True}updated\n",
            "  zero_reduce_bucket_size ......... 500000000...................updated\n",
            "  zero_reduce_scatter ............. True........................updated\n",
            "  zero_stage ...................... 1...........................updated\n",
            "  account ......................... None........................default\n",
            "  activation ...................... gelu........................default\n",
            "  activation_checkpointing ........ None........................default\n",
            "  adlr_autoresume ................. False.......................default\n",
            "  adlr_autoresume_interval ........ 1000........................default\n",
            "  allow_chopped ................... True........................default\n",
            "  amp ............................. None........................default\n",
            "  apply_query_key_layer_scaling ... False.......................default\n",
            "  attention_dropout ............... 0...........................default\n",
            "  attention_softmax_in_fp32 ....... False.......................default\n",
            "  autotuning ...................... None........................default\n",
            "  autotuning_run .................. None........................default\n",
            "  base_shapes_file ................ None........................default\n",
            "  bf16 ............................ None........................default\n",
            "  bias_dropout_fusion ............. False.......................default\n",
            "  bias_gelu_fusion ................ False.......................default\n",
            "  char_level_ppl .................. False.......................default\n",
            "  checkpoint ...................... None........................default\n",
            "  checkpoint_in_cpu ............... False.......................default\n",
            "  checkpoint_num_layers ........... 1...........................default\n",
            "  checkpoint_scale ................ linear......................default\n",
            "  checkpoint_validation_with_forward_pass  False................default\n",
            "  clip_grad ....................... 1.0.........................default\n",
            "  comet_experiment ................ None........................default\n",
            "  comet_experiment_name ........... None........................default\n",
            "  comet_others .................... None........................default\n",
            "  comet_project ................... None........................default\n",
            "  comet_tags ...................... None........................default\n",
            "  comet_workspace ................. None........................default\n",
            "  comment ......................... None........................default\n",
            "  comms_logger .................... None........................default\n",
            "  communication_data_type ......... None........................default\n",
            "  compression_training ............ None........................default\n",
            "  contiguous_checkpointing ........ False.......................default\n",
            "  coord_check ..................... False.......................default\n",
            "  create_moe_param_group .......... True........................default\n",
            "  csv_monitor ..................... None........................default\n",
            "  curriculum_learning ............. None........................default\n",
            "  curriculum_seqlen ............... 0...........................default\n",
            "  data_efficiency ................. None........................default\n",
            "  data_types ...................... None........................default\n",
            "  dataset_impl .................... gpt2........................default\n",
            "  deepscale ....................... False.......................default\n",
            "  deepscale_config ................ None........................default\n",
            "  deepspeed ....................... True........................default\n",
            "  deepspeed_activation_checkpointing  True......................default\n",
            "  deepspeed_mpi ................... False.......................default\n",
            "  deepspeed_slurm ................. False.......................default\n",
            "  detect_nvlink_pairs ............. False.......................default\n",
            "  dim_att ......................... None........................default\n",
            "  distributed_backend ............. nccl........................default\n",
            "  do_test ......................... None........................default\n",
            "  do_train ........................ None........................default\n",
            "  do_valid ........................ None........................default\n",
            "  dpo_beta ........................ 0.1.........................default\n",
            "  dpo_fp32 ........................ True........................default\n",
            "  dpo_reference_free .............. False.......................default\n",
            "  dump_state ...................... False.......................default\n",
            "  elasticity ...................... None........................default\n",
            "  enable_expert_tensor_parallelism  False.......................default\n",
            "  eod_mask_loss ................... False.......................default\n",
            "  eval_results_prefix ............. ............................default\n",
            "  eval_tasks ...................... None........................default\n",
            "  exclude ......................... None........................default\n",
            "  exit_interval ................... None........................default\n",
            "  expansion_factor ................ None........................default\n",
            "  expert_interval ................. 2...........................default\n",
            "  extra_save_iters ................ None........................default\n",
            "  ffn_dim ......................... None........................default\n",
            "  finetune ........................ False.......................default\n",
            "  flops_profiler .................. None........................default\n",
            "  force_multi ..................... False.......................default\n",
            "  fp16_lm_cross_entropy ........... False.......................default\n",
            "  fp32_allreduce .................. False.......................default\n",
            "  git_hash ........................ 59a5236d....................default\n",
            "  gmlp_attn_dim ................... 64..........................default\n",
            "  gpt_j_residual .................. False.......................default\n",
            "  gpt_j_tied ...................... False.......................default\n",
            "  gradient_accumulation_steps ..... 1...........................default\n",
            "  gradient_clipping ............... 1.0.........................default\n",
            "  gradient_noise_scale_cpu_offload  False.......................default\n",
            "  gradient_noise_scale_n_batches .. 5...........................default\n",
            "  gradient_predivide_factor ....... 1.0.........................default\n",
            "  head_size ....................... None........................default\n",
            "  hidden_dropout .................. 0...........................default\n",
            "  hostfile ........................ None........................default\n",
            "  hysteresis ...................... 2...........................default\n",
            "  include ......................... None........................default\n",
            "  init_method_std ................. 0.02........................default\n",
            "  intermediate_size ............... None........................default\n",
            "  is_pipe_parallel ................ False.......................default\n",
            "  iteration ....................... None........................default\n",
            "  keep_last_n_checkpoints ......... None........................default\n",
            "  kto_beta ........................ 0.1.........................default\n",
            "  kto_desirable_weight ............ 1.0.........................default\n",
            "  kto_fp32 ........................ True........................default\n",
            "  kto_undesirable_weight .......... 1.0.........................default\n",
            "  launcher ........................ pdsh........................default\n",
            "  layernorm_epsilon ............... 1e-05.......................default\n",
            "  layernorm_fusion ................ False.......................default\n",
            "  lazy_mpu_init ................... False.......................default\n",
            "  local_rank ...................... None........................default\n",
            "  log_grad_norm ................... False.......................default\n",
            "  log_grad_pct_zeros .............. False.......................default\n",
            "  log_gradient_noise_scale ........ False.......................default\n",
            "  log_optimizer_states ............ False.......................default\n",
            "  log_param_norm .................. False.......................default\n",
            "  loss_scale ...................... None........................default\n",
            "  loss_scale_window ............... 1000.0......................default\n",
            "  lr_decay_fraction ............... None........................default\n",
            "  make_vocab_size_divisible_by .... 128.........................default\n",
            "  mamba_causal_conv_fusion ........ False.......................default\n",
            "  mamba_inner_func_fusion ......... False.......................default\n",
            "  mamba_selective_fp32_params ..... True........................default\n",
            "  mamba_selective_scan_fusion ..... False.......................default\n",
            "  mamba_use_bias_in_conv .......... True........................default\n",
            "  mamba_use_bias_in_linears ....... False.......................default\n",
            "  master_addr ..................... None........................default\n",
            "  master_port ..................... 29500.......................default\n",
            "  maximum_tokens .................. 64..........................default\n",
            "  memory_profiling ................ False.......................default\n",
            "  memory_profiling_path ........... None........................default\n",
            "  min_scale ....................... 1.0.........................default\n",
            "  mlp_multiple_of ................. 1...........................default\n",
            "  mmap_warmup ..................... False.......................default\n",
            "  model_parallel_size ............. 1...........................default\n",
            "  moe_eval_capacity_factor ........ 1.0.........................default\n",
            "  moe_expert_parallel_size ........ 1...........................default\n",
            "  moe_glu ......................... False.......................default\n",
            "  moe_jitter_eps .................. None........................default\n",
            "  moe_lbl_in_fp32 ................. False.......................default\n",
            "  moe_loss_coeff .................. 0.1.........................default\n",
            "  moe_min_capacity ................ 4...........................default\n",
            "  moe_num_experts ................. 1...........................default\n",
            "  moe_token_dropping .............. False.......................default\n",
            "  moe_top_k ....................... 1...........................default\n",
            "  moe_train_capacity_factor ....... 1.0.........................default\n",
            "  moe_type ........................ megablocks..................default\n",
            "  moe_use_residual ................ True........................default\n",
            "  mup_attn_temp ................... 1.0.........................default\n",
            "  mup_embedding_mult .............. 1.0.........................default\n",
            "  mup_init_scale .................. 1.0.........................default\n",
            "  mup_output_temp ................. 1.0.........................default\n",
            "  mup_rp_embedding_mult ........... 1.0.........................default\n",
            "  mup_width_scale ................. 2...........................default\n",
            "  neg_test_data_paths ............. None........................default\n",
            "  neg_test_label_data_paths ....... None........................default\n",
            "  neg_train_data_paths ............ None........................default\n",
            "  neg_train_label_data_paths ...... None........................default\n",
            "  neg_valid_data_paths ............ None........................default\n",
            "  neg_valid_label_data_paths ...... None........................default\n",
            "  no_load_optim ................... False.......................default\n",
            "  no_load_rng ..................... False.......................default\n",
            "  no_save_optim ................... False.......................default\n",
            "  no_save_rng ..................... False.......................default\n",
            "  no_ssh_check .................... False.......................default\n",
            "  norm ............................ layernorm...................default\n",
            "  num_gpus ........................ None........................default\n",
            "  num_kv_heads .................... None........................default\n",
            "  num_nodes ....................... -1..........................default\n",
            "  num_samples ..................... 1...........................default\n",
            "  num_unique_layers ............... None........................default\n",
            "  onnx_safe ....................... False.......................default\n",
            "  opt_pos_emb_offset .............. 0...........................default\n",
            "  output_layer_parallelism ........ column......................default\n",
            "  override_lr_scheduler ........... False.......................default\n",
            "  pack_impl ....................... packed......................default\n",
            "  padded_vocab_size ............... None........................default\n",
            "  param_sharing_style ............. grouped.....................default\n",
            "  pipe_parallel_size .............. 0...........................default\n",
            "  pipe_partition_method ........... type:transformer|mlp........default\n",
            "  pos_test_data_paths ............. None........................default\n",
            "  pos_test_label_data_paths ....... None........................default\n",
            "  pos_train_data_paths ............ None........................default\n",
            "  pos_train_label_data_paths ...... None........................default\n",
            "  pos_valid_data_paths ............ None........................default\n",
            "  pos_valid_label_data_paths ...... None........................default\n",
            "  precompute_model_name ........... None........................default\n",
            "  prescale_gradients .............. False.......................default\n",
            "  profile ......................... False.......................default\n",
            "  profile_backward ................ False.......................default\n",
            "  profile_step_start .............. 10..........................default\n",
            "  profile_step_stop ............... 12..........................default\n",
            "  prompt_end ...................... \n",
            "...........................default\n",
            "  rank ............................ None........................default\n",
            "  recompute ....................... False.......................default\n",
            "  return_logits ................... False.......................default\n",
            "  rms_norm_epsilon ................ 1e-08.......................default\n",
            "  rmsnorm_fusion .................. False.......................default\n",
            "  rope_fusion ..................... False.......................default\n",
            "  rotary_emb_base ................. 10000.......................default\n",
            "  rotary_pct ...................... 1.0.........................default\n",
            "  rotary_save_freqs_buffer ........ False.......................default\n",
            "  rpe_max_distance ................ 128.........................default\n",
            "  rpe_num_buckets ................. 32..........................default\n",
            "  s3_chunk_size ................... 104857600...................default\n",
            "  s3_path ......................... None........................default\n",
            "  sample_input_file ............... None........................default\n",
            "  sample_output_file .............. samples.txt.................default\n",
            "  save_base_shapes ................ False.......................default\n",
            "  scaled_masked_softmax_fusion .... False.......................default\n",
            "  scaled_upper_triang_masked_softmax_fusion  False..............default\n",
            "  scalenorm_epsilon ............... 1e-08.......................default\n",
            "  scheduler ....................... None........................default\n",
            "  seed ............................ 1234........................default\n",
            "  sequence_parallel ............... False.......................default\n",
            "  short_seq_prob .................. 0.1.........................default\n",
            "  sliding_window_width ............ None........................default\n",
            "  soft_prompt_tuning .............. None........................default\n",
            "  sparse_attention ................ None........................default\n",
            "  sparse_gradients ................ False.......................default\n",
            "  split ........................... 969, 30, 1..................default\n",
            "  steps_per_print ................. 10..........................default\n",
            "  temperature ..................... 0.0.........................default\n",
            "  tensorboard ..................... None........................default\n",
            "  test_data_paths ................. None........................default\n",
            "  test_data_weights ............... None........................default\n",
            "  test_label_data_paths ........... None........................default\n",
            "  test_reward_data_paths .......... None........................default\n",
            "  tokenizer_type .................. GPT2BPETokenizer............default\n",
            "  top_k ........................... 0...........................default\n",
            "  top_p ........................... 0.0.........................default\n",
            "  train_data_paths ................ None........................default\n",
            "  train_data_weights .............. None........................default\n",
            "  train_epochs .................... None........................default\n",
            "  train_impl ...................... normal......................default\n",
            "  train_label_data_paths .......... None........................default\n",
            "  train_reward_data_paths ......... None........................default\n",
            "  use_bias_in_attn_linear ......... True........................default\n",
            "  use_bias_in_mlp ................. True........................default\n",
            "  use_bias_in_norms ............... True........................default\n",
            "  use_bnb_optimizer ............... False.......................default\n",
            "  use_checkpoint_lr_scheduler ..... False.......................default\n",
            "  use_comet ....................... None........................default\n",
            "  use_cpu_initialization .......... False.......................default\n",
            "  use_flashattn_swiglu ............ False.......................default\n",
            "  use_mup ......................... False.......................default\n",
            "  use_qk_layernorm ................ False.......................default\n",
            "  use_shared_fs ................... True........................default\n",
            "  use_tutel ....................... False.......................default\n",
            "  use_wandb ....................... None........................default\n",
            "  valid_data_paths ................ None........................default\n",
            "  valid_data_weights .............. None........................default\n",
            "  valid_label_data_paths .......... None........................default\n",
            "  valid_reward_data_paths ......... None........................default\n",
            "  wandb ........................... None........................default\n",
            "  wandb_group ..................... None........................default\n",
            "  wandb_host ...................... https://api.wandb.ai........default\n",
            "  wandb_init_all_ranks ............ False.......................default\n",
            "  wandb_project ................... neox........................default\n",
            "  wandb_team ...................... None........................default\n",
            "  warmup .......................... 0.01........................default\n",
            "  weight_by_num_documents ......... False.......................default\n",
            "  weight_decay .................... 0.1.........................default\n",
            "  weighted_sampler_alpha .......... 1.0.........................default\n",
            "  world_size ...................... None........................default\n",
            "  z_loss .......................... 0.0.........................default\n",
            "---------------- end of arguments ----------------\n",
            "NeoXArgs.configure_distributed_args() using world size: 1 and model-parallel size: 1 \n",
            "[2024-11-10 17:45:53,730] [WARNING] [runner.py:217:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2024-11-10 17:45:53,730] [INFO] [runner.py:586:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed_config eyJ0cmFpbl9iYXRjaF9zaXplIjogNCwgInRyYWluX21pY3JvX2JhdGNoX3NpemVfcGVyX2dwdSI6IDQsICJvcHRpbWl6ZXIiOiB7InR5cGUiOiAiQWRhbSIsICJwYXJhbXMiOiB7ImxyIjogMC4wMDEsICJiZXRhcyI6IFswLjksIDAuOTVdLCAiZXBzIjogMWUtMDh9fSwgImZwMTYiOiB7ImZwMTYiOiB0cnVlLCAiZW5hYmxlZCI6IHRydWUsICJsb3NzX3NjYWxlIjogMCwgImxvc3Nfc2NhbGVfd2luZG93IjogMTAwMCwgImluaXRpYWxfc2NhbGVfcG93ZXIiOiAxMiwgImh5c3RlcmVzaXMiOiAyLCAibWluX2xvc3Nfc2NhbGUiOiAxfSwgInplcm9fb3B0aW1pemF0aW9uIjogeyJzdGFnZSI6IDEsICJhbGxnYXRoZXJfcGFydGl0aW9ucyI6IHRydWUsICJhbGxnYXRoZXJfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJvdmVybGFwX2NvbW0iOiB0cnVlLCAicmVkdWNlX3NjYXR0ZXIiOiB0cnVlLCAicmVkdWNlX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAiY29udGlndW91c19ncmFkaWVudHMiOiB0cnVlfSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZSwgImNvbW1zX2xvZ2dlciI6IHsiZW5hYmxlZCI6IHRydWUsICJ2ZXJib3NlIjogdHJ1ZSwgInByb2ZfYWxsIjogdHJ1ZSwgImRlYnVnIjogZmFsc2V9fQ== --megatron_config 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\n",
            "[2024-11-10 17:45:55,296] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-11-10 17:45:58,249] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.19.3-1+cuda12.2\n",
            "[2024-11-10 17:45:58,250] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.19.3-1\n",
            "[2024-11-10 17:45:58,250] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.19.3-1\n",
            "[2024-11-10 17:45:58,250] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2024-11-10 17:45:58,250] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.19.3-1+cuda12.2\n",
            "[2024-11-10 17:45:58,250] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2024-11-10 17:45:58,250] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.19.3-1\n",
            "[2024-11-10 17:45:58,250] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2024-11-10 17:45:58,250] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2024-11-10 17:45:58,250] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2024-11-10 17:45:58,250] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2024-11-10 17:45:58,250] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2024-11-10 17:46:00,535] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba\n",
            "NeoXArgs.configure_distributed_args() using world size: 1 and model-parallel size: 1 \n",
            "> building GPT2BPETokenizer tokenizer ...\n",
            " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
            "2024-11-10 17:46:06.184703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-10 17:46:06.204733: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-10 17:46:06.211002: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-10 17:46:07.354219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "> setting up tensorboard ...\n",
            "> initializing torch distributed ...\n",
            "[2024-11-10 17:46:07,854] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-11-10 17:46:07,854] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "> initializing model parallel with size 1\n",
            "MPU DP: [0]\n",
            "MPU PP: [0]\n",
            "MPU MP: [0]\n",
            "> setting random seeds to 1234 ...\n",
            "[2024-11-10 17:46:07,859] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
            "make: Entering directory '/content/gpt-neox/megatron/data'\n",
            "make: Nothing to be done for 'default'.\n",
            "make: Leaving directory '/content/gpt-neox/megatron/data'\n",
            "> building train, validation, and test datasets ...\n",
            "    reading sizes...\n",
            "    reading pointers...\n",
            "    reading document index...\n",
            "    creating numpy buffer of mmap...\n",
            "    creating memory view of numpy buffer...\n",
            " > dataset split:\n",
            "    train:\n",
            "     document indices in [0, 92055) total of 92055 documents\n",
            "    validation:\n",
            "     document indices in [92055, 94905) total of 2850 documents\n",
            "    test:\n",
            "     document indices in [94905, 95000) total of 95 documents\n",
            "/content/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  counts = torch.cuda.LongTensor([1])\n",
            " > loading doc-idx mapping from processed_data/py150_text_document_train_indexmap_400ns_2048sl_1234s_packedpi_ac_doc_idx.npy\n",
            " > loading sample-idx mapping from processed_data/py150_text_document_train_indexmap_400ns_2048sl_1234s_packedpi_ac_sample_idx.npy\n",
            " > loading shuffle-idx mapping from processed_data/py150_text_document_train_indexmap_400ns_2048sl_1234s_packedpi_ac_shuffle_idx.npy\n",
            "    loaded indexed file in 0.001 seconds\n",
            "    total number of samples: 86775\n",
            "    total number of epochs: 1\n",
            " > loading doc-idx mapping from processed_data/py150_text_document_valid_indexmap_120ns_2048sl_1234s_packedpi_ac_doc_idx.npy\n",
            " > loading sample-idx mapping from processed_data/py150_text_document_valid_indexmap_120ns_2048sl_1234s_packedpi_ac_sample_idx.npy\n",
            " > loading shuffle-idx mapping from processed_data/py150_text_document_valid_indexmap_120ns_2048sl_1234s_packedpi_ac_shuffle_idx.npy\n",
            "    loaded indexed file in 0.001 seconds\n",
            "    total number of samples: 2371\n",
            "    total number of epochs: 1\n",
            " > loading doc-idx mapping from processed_data/py150_text_document_test_indexmap_40ns_2048sl_1234s_packedpi_ac_doc_idx.npy\n",
            " > loading sample-idx mapping from processed_data/py150_text_document_test_indexmap_40ns_2048sl_1234s_packedpi_ac_sample_idx.npy\n",
            " > loading shuffle-idx mapping from processed_data/py150_text_document_test_indexmap_40ns_2048sl_1234s_packedpi_ac_shuffle_idx.npy\n",
            "    loaded indexed file in 0.007 seconds\n",
            "    total number of samples: 68\n",
            "    total number of epochs: 1\n",
            "building GPT2 model ...\n",
            "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
            "Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0}\n",
            "[2024-11-10 17:46:08,455] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer|mlp\n",
            "stage=0 layers=11\n",
            "     0: EmbeddingPipe\n",
            "     1: _pre_transformer_block\n",
            "     2: ParallelTransformerLayerPipe\n",
            "     3: ParallelTransformerLayerPipe\n",
            "     4: ParallelTransformerLayerPipe\n",
            "     5: ParallelTransformerLayerPipe\n",
            "     6: ParallelTransformerLayerPipe\n",
            "     7: ParallelTransformerLayerPipe\n",
            "     8: _post_transformer_block\n",
            "     9: NormPipe\n",
            "    10: ParallelLinearPipe\n",
            "  loss: partial\n",
            "Configuring Optimizer type: Adam with params: {'lr': 0.001, 'betas': [0.9, 0.95], 'eps': 1e-08}\n",
            "WARNING: APEX not installed - defaulting to deepspeed's fused adam\n",
            "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 0.1061091423034668 seconds\n",
            "> learning rate decay style: cosine\n",
            "DeepSpeed is enabled.\n",
            "[2024-11-10 17:46:09,422] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4+02e2ebf, git-hash=02e2ebf, git-branch=HEAD\n",
            "[2024-11-10 17:46:09,429] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.36 | msg size: 49.12 MB | algbw (Gbps): 1129.53 | busbw (Gbps): 1129.53\n",
            "[2024-11-10 17:46:09,430] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.17 | msg size: 1.0 KB | algbw (Gbps): 0.05 | busbw (Gbps): 0.05\n",
            "[2024-11-10 17:46:09,430] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.16 | msg size: 1.0 KB | algbw (Gbps): 0.05 | busbw (Gbps): 0.05\n",
            "[2024-11-10 17:46:09,431] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.5 MB | algbw (Gbps): 86.21 | busbw (Gbps): 86.21\n",
            "[2024-11-10 17:46:09,431] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.16 | msg size: 3.0 KB | algbw (Gbps): 0.15 | busbw (Gbps): 0.15\n",
            "[2024-11-10 17:46:09,432] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 512.0 KB | algbw (Gbps): 28.05 | busbw (Gbps): 28.05\n",
            "[2024-11-10 17:46:09,432] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.0 KB | algbw (Gbps): 0.05 | busbw (Gbps): 0.05\n",
            "[2024-11-10 17:46:09,433] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,433] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,434] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 2.0 MB | algbw (Gbps): 115.74 | busbw (Gbps): 115.74\n",
            "[2024-11-10 17:46:09,434] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 4.0 KB | algbw (Gbps): 0.24 | busbw (Gbps): 0.24\n",
            "[2024-11-10 17:46:09,435] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.16 | msg size: 2.0 MB | algbw (Gbps): 103.70 | busbw (Gbps): 103.70\n",
            "[2024-11-10 17:46:09,435] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.17 | msg size: 1.0 KB | algbw (Gbps): 0.05 | busbw (Gbps): 0.05\n",
            "[2024-11-10 17:46:09,436] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,436] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.18 | msg size: 1.0 KB | algbw (Gbps): 0.04 | busbw (Gbps): 0.04\n",
            "[2024-11-10 17:46:09,437] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 1.5 MB | algbw (Gbps): 91.96 | busbw (Gbps): 91.96\n",
            "[2024-11-10 17:46:09,437] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 3.0 KB | algbw (Gbps): 0.17 | busbw (Gbps): 0.17\n",
            "[2024-11-10 17:46:09,438] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 512.0 KB | algbw (Gbps): 28.76 | busbw (Gbps): 28.76\n",
            "[2024-11-10 17:46:09,438] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,439] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,439] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,440] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 2.0 MB | algbw (Gbps): 114.60 | busbw (Gbps): 114.60\n",
            "[2024-11-10 17:46:09,440] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 4.0 KB | algbw (Gbps): 0.23 | busbw (Gbps): 0.23\n",
            "[2024-11-10 17:46:09,440] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 2.0 MB | algbw (Gbps): 115.23 | busbw (Gbps): 115.23\n",
            "[2024-11-10 17:46:09,441] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,441] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,442] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,442] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.17 | msg size: 1.5 MB | algbw (Gbps): 74.91 | busbw (Gbps): 74.91\n",
            "[2024-11-10 17:46:09,443] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 3.0 KB | algbw (Gbps): 0.17 | busbw (Gbps): 0.17\n",
            "[2024-11-10 17:46:09,443] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.19 | msg size: 512.0 KB | algbw (Gbps): 22.26 | busbw (Gbps): 22.26\n",
            "[2024-11-10 17:46:09,444] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,444] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.18 | msg size: 1.0 KB | algbw (Gbps): 0.05 | busbw (Gbps): 0.05\n",
            "[2024-11-10 17:46:09,445] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,445] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.18 | msg size: 2.0 MB | algbw (Gbps): 91.47 | busbw (Gbps): 91.47\n",
            "[2024-11-10 17:46:09,446] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 4.0 KB | algbw (Gbps): 0.23 | busbw (Gbps): 0.23\n",
            "[2024-11-10 17:46:09,446] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.20 | msg size: 2.0 MB | algbw (Gbps): 84.37 | busbw (Gbps): 84.37\n",
            "[2024-11-10 17:46:09,447] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.16 | msg size: 1.0 KB | algbw (Gbps): 0.05 | busbw (Gbps): 0.05\n",
            "[2024-11-10 17:46:09,447] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.0 KB | algbw (Gbps): 0.05 | busbw (Gbps): 0.05\n",
            "[2024-11-10 17:46:09,448] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,448] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.5 MB | algbw (Gbps): 84.91 | busbw (Gbps): 84.91\n",
            "[2024-11-10 17:46:09,449] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 3.0 KB | algbw (Gbps): 0.17 | busbw (Gbps): 0.17\n",
            "[2024-11-10 17:46:09,449] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 512.0 KB | algbw (Gbps): 32.05 | busbw (Gbps): 32.05\n",
            "[2024-11-10 17:46:09,450] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,450] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,450] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,451] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 128.19 | busbw (Gbps): 128.19\n",
            "[2024-11-10 17:46:09,451] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.16 | msg size: 4.0 KB | algbw (Gbps): 0.20 | busbw (Gbps): 0.20\n",
            "[2024-11-10 17:46:09,452] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 130.06 | busbw (Gbps): 130.06\n",
            "[2024-11-10 17:46:09,452] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,453] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,453] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,453] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 1.5 MB | algbw (Gbps): 89.53 | busbw (Gbps): 89.53\n",
            "[2024-11-10 17:46:09,454] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 3.0 KB | algbw (Gbps): 0.19 | busbw (Gbps): 0.19\n",
            "[2024-11-10 17:46:09,454] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.16 | msg size: 512.0 KB | algbw (Gbps): 25.45 | busbw (Gbps): 25.45\n",
            "[2024-11-10 17:46:09,455] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,455] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.19 | msg size: 1.0 KB | algbw (Gbps): 0.04 | busbw (Gbps): 0.04\n",
            "[2024-11-10 17:46:09,456] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,456] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 130.26 | busbw (Gbps): 130.26\n",
            "[2024-11-10 17:46:09,457] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.16 | msg size: 4.0 KB | algbw (Gbps): 0.21 | busbw (Gbps): 0.21\n",
            "[2024-11-10 17:46:09,457] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 129.13 | busbw (Gbps): 129.13\n",
            "[2024-11-10 17:46:09,457] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,458] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,458] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,459] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.5 MB | algbw (Gbps): 97.45 | busbw (Gbps): 97.45\n",
            "[2024-11-10 17:46:09,459] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.17 | msg size: 3.0 KB | algbw (Gbps): 0.15 | busbw (Gbps): 0.15\n",
            "[2024-11-10 17:46:09,460] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 512.0 KB | algbw (Gbps): 32.00 | busbw (Gbps): 32.00\n",
            "[2024-11-10 17:46:09,460] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.20 | msg size: 1.0 KB | algbw (Gbps): 0.04 | busbw (Gbps): 0.04\n",
            "[2024-11-10 17:46:09,461] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.15 | msg size: 1.0 KB | algbw (Gbps): 0.05 | busbw (Gbps): 0.05\n",
            "[2024-11-10 17:46:09,461] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.18 | msg size: 1.0 KB | algbw (Gbps): 0.04 | busbw (Gbps): 0.04\n",
            "[2024-11-10 17:46:09,462] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 126.73 | busbw (Gbps): 126.73\n",
            "[2024-11-10 17:46:09,462] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 4.0 KB | algbw (Gbps): 0.25 | busbw (Gbps): 0.25\n",
            "[2024-11-10 17:46:09,462] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 128.16 | busbw (Gbps): 128.16\n",
            "[2024-11-10 17:46:09,463] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,463] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.14 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,464] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06 | busbw (Gbps): 0.06\n",
            "[2024-11-10 17:46:09,464] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: broadcast | time (ms): 0.18 | msg size: 49.12 MB | algbw (Gbps): 2352.11 | busbw (Gbps): 2352.11\n",
            "[2024-11-10 17:46:09,464] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2024-11-10 17:46:09,465] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
            "[2024-11-10 17:46:09,465] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "[2024-11-10 17:46:09,466] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
            "[2024-11-10 17:46:09,466] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
            "[2024-11-10 17:46:09,466] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer\n",
            "[2024-11-10 17:46:09,466] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000\n",
            "[2024-11-10 17:46:09,466] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000\n",
            "[2024-11-10 17:46:09,466] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
            "[2024-11-10 17:46:09,467] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
            "[2024-11-10 17:46:10,065] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states\n",
            "[2024-11-10 17:46:10,066] [INFO] [utils.py:803:see_memory_usage] MA 0.39 GB         Max_MA 0.39 GB         CA 0.4 GB         Max_CA 0 GB \n",
            "[2024-11-10 17:46:10,066] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 3.7 GB, percent = 29.2%\n",
            "[2024-11-10 17:46:10,393] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states\n",
            "[2024-11-10 17:46:10,394] [INFO] [utils.py:803:see_memory_usage] MA 0.92 GB         Max_MA 1.18 GB         CA 1.19 GB         Max_CA 1 GB \n",
            "[2024-11-10 17:46:10,394] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 3.7 GB, percent = 29.2%\n",
            "[2024-11-10 17:46:10,394] [INFO] [stage_1_and_2.py:517:__init__] optimizer state initialized\n",
            "[2024-11-10 17:46:10,710] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2024-11-10 17:46:10,711] [INFO] [utils.py:803:see_memory_usage] MA 0.92 GB         Max_MA 0.92 GB         CA 1.19 GB         Max_CA 1 GB \n",
            "[2024-11-10 17:46:10,711] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 3.7 GB, percent = 29.2%\n",
            "[2024-11-10 17:46:10,714] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam\n",
            "[2024-11-10 17:46:10,714] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
            "[2024-11-10 17:46:10,714] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7a454f56fe20>\n",
            "[2024-11-10 17:46:10,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[[0.9, 0.95], [0.9, 0.95]]\n",
            "[2024-11-10 17:46:10,714] [INFO] [config.py:979:print] DeepSpeedEngine configuration:\n",
            "[2024-11-10 17:46:10,715] [INFO] [config.py:983:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2024-11-10 17:46:10,715] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2024-11-10 17:46:10,715] [INFO] [config.py:983:print]   amp_enabled .................. False\n",
            "[2024-11-10 17:46:10,715] [INFO] [config.py:983:print]   amp_params ................... False\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   bfloat16_enabled ............. False\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7a45217630a0>\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   communication_data_type ...... None\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   dataloader_drop_last ......... False\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   disable_allgather ............ False\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   dump_state ................... False\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100\n",
            "[2024-11-10 17:46:10,716] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   elasticity_enabled ........... False\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   fp16_auto_cast ............... False\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   fp16_enabled ................. True\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   global_rank .................. 0\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   grad_accum_dtype ............. None\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 1\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   gradient_clipping ............ 0.0\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 4096\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   load_universal_checkpoint .... False\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   loss_scale ................... 0\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   memory_breakdown ............. False\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   mics_shard_size .............. -1\n",
            "[2024-11-10 17:46:10,717] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   optimizer_name ............... adam\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.9, 0.95], 'eps': 1e-08}\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   pld_enabled .................. False\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   pld_params ................... False\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   prescale_gradients ........... False\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   scheduler_name ............... None\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   scheduler_params ............. None\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   sparse_attention ............. None\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   steps_per_print .............. 10\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   train_batch_size ............. 4\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  4\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   use_node_local_storage ....... False\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   wall_clock_breakdown ......... True\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   weight_quantization_config ... None\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   world_size ................... 1\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
            "[2024-11-10 17:46:10,718] [INFO] [config.py:983:print]   zero_enabled ................. True\n",
            "[2024-11-10 17:46:10,719] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2024-11-10 17:46:10,719] [INFO] [config.py:983:print]   zero_optimization_stage ...... 1\n",
            "[2024-11-10 17:46:10,719] [INFO] [config.py:969:print_user_config]   json = {\n",
            "    \"train_batch_size\": 4, \n",
            "    \"train_micro_batch_size_per_gpu\": 4, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"Adam\", \n",
            "        \"params\": {\n",
            "            \"lr\": 0.001, \n",
            "            \"betas\": [0.9, 0.95], \n",
            "            \"eps\": 1e-08\n",
            "        }\n",
            "    }, \n",
            "    \"fp16\": {\n",
            "        \"fp16\": true, \n",
            "        \"enabled\": true, \n",
            "        \"loss_scale\": 0, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"initial_scale_power\": 12, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 1, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 5.000000e+08, \n",
            "        \"overlap_comm\": true, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 5.000000e+08, \n",
            "        \"contiguous_gradients\": true\n",
            "    }, \n",
            "    \"wall_clock_breakdown\": true, \n",
            "    \"comms_logger\": {\n",
            "        \"enabled\": true, \n",
            "        \"verbose\": true, \n",
            "        \"prof_all\": true, \n",
            "        \"debug\": false\n",
            "    }\n",
            "}\n",
            " > number of parameters on model parallel rank 0: 70426624\n",
            " > total params: 70,426,624\n",
            "[2024-11-10 17:46:10,770] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoints/global_step100/mp_rank_00_model_states.pt...\n",
            "[2024-11-10 17:46:10,895] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoints/global_step100/mp_rank_00_model_states.pt.\n",
            "[2024-11-10 17:46:10,913] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoints/global_step100/mp_rank_00_model_states.pt...\n",
            "[2024-11-10 17:46:11,038] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoints/global_step100/mp_rank_00_model_states.pt.\n",
            " > using checkpoint value 0.001 for learning rate\n",
            " > using checkpoint value 0.0001 for minimum learning rate\n",
            " > using checkpoint value 1.0 for warmup iterations\n",
            " > using checkpoint value 100 for total number of iterations\n",
            " > using checkpoint value cosine for decay style\n",
            "[2024-11-10 17:46:11,085] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoints/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
            "[2024-11-10 17:46:11,669] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoints/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
            "[2024-11-10 17:46:11,669] [INFO] [engine.py:2982:_get_all_zero_checkpoint_state_dicts] successfully read 1 ZeRO state_dicts for rank 0\n",
            "[2024-11-10 17:46:11,854] [INFO] [engine.py:2914:_load_zero_checkpoint] loading 1 zero partition checkpoints for rank 0\n",
            " > validated currently set args with arguments in the checkpoint ...\n",
            "  successfully loaded checkpoints/global_step100/mp_rank_00_model_states.pt\n",
            "Loading checkpoint and starting from iteration 100\n",
            "setting training data start iteration to 100\n",
            "setting validation data start iteration to 20\n",
            "done with setups ...\n",
            "time (ms) | train/valid/test data loaders: 429.60 | model and optimizer: 3461.98 | train/valid/test data iterators: 94.49\n",
            "training ...\n",
            "[2024-11-10 17:46:12,531] [INFO] [checkpointing.py:540:forward] Activation Checkpointing Information\n",
            "[2024-11-10 17:46:12,531] [INFO] [checkpointing.py:541:forward] ----Partition Activations True, CPU CHECKPOINTING False\n",
            "[2024-11-10 17:46:12,531] [INFO] [checkpointing.py:542:forward] ----contiguous Memory Checkpointing False with 6 total layers\n",
            "[2024-11-10 17:46:12,531] [INFO] [checkpointing.py:544:forward] ----Synchronization True\n",
            "[2024-11-10 17:46:12,531] [INFO] [checkpointing.py:545:forward] ----Profiling time in checkpointing False\n",
            "---------------------------------------------------------------------------------------------------------------------------\n",
            " validation results at the end of training for val data | lm_loss value: 2.810510E+00 | lm_loss_ppl value: 1.661839E+01 | \n",
            "---------------------------------------------------------------------------------------------------------------------------\n",
            "[2024-11-10 17:46:14,751] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: barrier | time (ms): 0.42 | msg size: 0B | algbw (Gbps): 0.00 | busbw (Gbps): 0.00\n",
            "[2024-11-10 17:46:14,751] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!\n",
            "[2024-11-10 17:46:14,752] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: all_reduce | time (ms): 0.29 | msg size: 20.0 B | algbw (Gbps): 0.00 | busbw (Gbps): 0.00\n",
            "[2024-11-10 17:46:14,753] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: all_reduce | time (ms): 0.18 | msg size: 20.0 B | algbw (Gbps): 0.00 | busbw (Gbps): 0.00\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
            "  warnings.warn(\n",
            "[2024-11-10 17:46:14,783] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: checkpoints/global_step100/mp_rank_00_model_states.pt\n",
            "[2024-11-10 17:46:14,783] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step100/mp_rank_00_model_states.pt...\n",
            "[2024-11-10 17:46:15,339] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step100/mp_rank_00_model_states.pt.\n",
            "[2024-11-10 17:46:15,340] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: barrier | time (ms): 0.62 | msg size: 0B | algbw (Gbps): 0.00 | busbw (Gbps): 0.00\n",
            "[2024-11-10 17:46:15,341] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
            "[2024-11-10 17:47:05,333] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
            "[2024-11-10 17:47:05,333] [INFO] [engine.py:3426:_save_zero_checkpoint] zero checkpoint saved checkpoints/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
            "[2024-11-10 17:47:05,334] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!\n",
            "[2024-11-10 17:47:05,335] [INFO] [logging.py:96:log_dist] [Rank 0] comm op: barrier | time (ms): 0.60 | msg size: 0B | algbw (Gbps): 0.00 | busbw (Gbps): 0.00\n",
            "Evaluating iter 10/10\n",
            "----------------------------------------------------------------------------------------------------------------------\n",
            " test results at the end of training for test data | lm_loss value: 2.808052E+00 | lm_loss_ppl value: 1.657759E+01 | \n",
            "----------------------------------------------------------------------------------------------------------------------\n",
            "[2024-11-10 17:47:11,327] [INFO] [launch.py:347:main] Process 15993 exits successfully.\n",
            "CPU times: user 570 ms, sys: 73.1 ms, total: 643 ms\n",
            "Wall time: 1min 26s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%cd /content/gpt-neox\n",
        "!python ./deepy.py train.py --conf_dir /content/GPT-NeoX-Colab/configs CC19M.yml cc_setup.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsoKIdFh99O_",
        "outputId": "1ee8a1a6-33f9-445d-8ac7-d687715fe8eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: datasets\n",
            "Version: 3.1.0\n",
            "Summary: HuggingFace community-driven open-source library of datasets\n",
            "Home-page: https://github.com/huggingface/datasets\n",
            "Author: HuggingFace Inc.\n",
            "Author-email: thomas@huggingface.co\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
            "Required-by: evaluate, lm_eval\n",
            "Collecting datasets==1.18.0\n",
            "  Using cached datasets-1.18.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.0) (1.26.4)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.0) (17.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.0) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->datasets==1.18.0) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.0) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.0) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.0) (24.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.0) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.0) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.0) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.0) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.0) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.0) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==1.18.0) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==1.18.0) (0.2.0)\n",
            "Using cached datasets-1.18.0-py3-none-any.whl (311 kB)\n",
            "Installing collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 3.1.0\n",
            "    Uninstalling datasets-3.1.0:\n",
            "      Successfully uninstalled datasets-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "evaluate 0.4.3 requires datasets>=2.0.0, but you have datasets 1.18.0 which is incompatible.\n",
            "lm-eval 0.4.5 requires datasets>=2.16.0, but you have datasets 1.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-1.18.0\n",
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.10/dist-packages (0.1.8)\n",
            "Requirement already satisfied: lm-eval in /usr/local/lib/python3.10/dist-packages (0.4.5)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (0.34.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (from lm-eval) (0.4.3)\n",
            "Collecting datasets>=2.16.0 (from lm-eval)\n",
            "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from lm-eval) (4.0.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm-eval) (2.10.1)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (0.13.2)\n",
            "Requirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (2.13.6)\n",
            "Requirement already satisfied: pytablewriter in /usr/local/lib/python3.10/dist-packages (from lm-eval) (1.2.0)\n",
            "Requirement already satisfied: rouge-score>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (0.1.2)\n",
            "Requirement already satisfied: sacrebleu>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (2.4.3)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (1.5.2)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.10/dist-packages (from lm-eval) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (2.3.0)\n",
            "Requirement already satisfied: tqdm-multiprocess in /usr/local/lib/python3.10/dist-packages (from lm-eval) (0.0.11)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (4.38.0)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (from lm-eval) (0.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from lm-eval) (0.3.8)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.10/dist-packages (from lm-eval) (1.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from lm-eval) (10.5.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm-eval) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm-eval) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm-eval) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm-eval) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm-eval) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm-eval) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (17.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->lm-eval) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (3.10.10)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval) (1.16.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval) (2.10.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval) (5.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->lm-eval) (12.6.77)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1->lm-eval) (0.15.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->lm-eval) (24.2.0)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval) (75.1.0)\n",
            "Requirement already satisfied: DataProperty<2,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval) (1.0.1)\n",
            "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval) (1.1.3)\n",
            "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval) (3.2.1)\n",
            "Requirement already satisfied: tabledata<2,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval) (1.3.3)\n",
            "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval) (0.1.6)\n",
            "Requirement already satisfied: typepy<2,>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval) (1.3.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval) (4.0.3)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval) (2024.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->lm-eval) (3.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score>=0.0.4->lm-eval) (8.1.7)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->lm-eval) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8->lm-eval) (1.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->lm-eval) (0.2.0)\n",
            "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "Installing collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 1.18.0\n",
            "    Uninstalling datasets-1.18.0:\n",
            "      Successfully uninstalled datasets-1.18.0\n",
            "Successfully installed datasets-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip show datasets\n",
        "!pip install datasets==1.18.0\n",
        "!pip install hf-transfer\n",
        "!pip install lm-eval --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3Wp-zQC-EXW"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "rYKP6ya8Iqej",
        "outputId": "447e2558-aa50-4bdb-bad4-319824270811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n",
            "Available scalar keys: ['validation/lm_loss', 'validation/lm_loss_ppl', 'test/lm_loss', 'test/lm_loss_ppl']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Key train/lm_loss was not found in Reservoir'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-72183c139b56>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Extract training and validation losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train/lm_loss'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust for actual name if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation/lm_loss'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust for actual name if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorboard/backend/event_processing/event_accumulator.py\u001b[0m in \u001b[0;36mScalars\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    601\u001b[0m           \u001b[0mAn\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScalarEvent\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorboard/backend/event_processing/reservoir.py\u001b[0m in \u001b[0;36mItems\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buckets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Key %s was not found in Reservoir\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buckets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Key train/lm_loss was not found in Reservoir'"
          ]
        }
      ],
      "source": [
        "%cd /content/gpt-neox\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "import os\n",
        "import numpy as np\n",
        "# Path to the latest log file\n",
        "log_dir = \"tensorboard\"\n",
        "log_files = [os.path.join(log_dir, d) for d in os.listdir(log_dir)]\n",
        "latest_log_dir = max(log_files, key=os.path.getmtime)\n",
        "\n",
        "# Initialize EventAccumulator to load scalar data\n",
        "ea = event_accumulator.EventAccumulator(latest_log_dir)\n",
        "ea.Reload()  # Load all logs\n",
        "\n",
        "# List all scalar keys available in the logs\n",
        "scalar_keys = ea.Tags()['scalars']\n",
        "print(\"Available scalar keys:\", scalar_keys)\n",
        "\n",
        "# Extract training and validation losses\n",
        "train_loss = ea.Scalars('train/lm_loss')  # Adjust for actual name if necessary\n",
        "val_loss = ea.Scalars('validation/lm_loss')  # Adjust for actual name if necessary\n",
        "\n",
        "# Convert to lists for plotting\n",
        "train_loss_values = [x.value for x in train_loss]\n",
        "val_loss_values = [x.value for x in val_loss]\n",
        "\n",
        "# Find the lengths of both arrays\n",
        "len_train = len(train_loss_values)\n",
        "len_val = len(val_loss_values)\n",
        "\n",
        "iterations = None\n",
        "# Interpolate the shorter array\n",
        "if len_train != len_val:\n",
        "    if len_train > len_val:\n",
        "        # Interpolate validation loss to match the training loss length\n",
        "        iterations = np.linspace(1, len_train, len_train)\n",
        "        val_iterations = np.linspace(1, len_train, len_val)\n",
        "        val_loss_values = np.interp(iterations, val_iterations, val_loss_values)\n",
        "    else:\n",
        "        # Interpolate training loss to match the validation loss length\n",
        "        iterations = np.linspace(1, len_val, len_val)\n",
        "        train_iterations = np.linspace(1, len_val, len_train)\n",
        "        train_loss_values = np.interp(iterations, train_iterations, train_loss_values)\n",
        "else:\n",
        "    iterations = range(1, len_train + 1)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(iterations, train_loss_values, label='Training Loss')\n",
        "plt.plot(iterations, val_loss_values, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNB4sSsS-RN3"
      },
      "source": [
        "# HuggingFace Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IOlAoGw-jZ7"
      },
      "source": [
        "# Convert Our Model to HuggingFace Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfKL-k5k-DQj",
        "outputId": "5c1cfe76-9bc6-4a1b-8558-85c69d53b41d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to the latest checkpoint: /content/gpt-neox/checkpoints/global_step100\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the checkpoints directory\n",
        "checkpoints_dir = \"/content/gpt-neox/checkpoints\"\n",
        "\n",
        "# Read the 'latest' file to get the latest checkpoint name\n",
        "with open(os.path.join(checkpoints_dir, \"latest\"), \"r\") as f:\n",
        "    latest_checkpoint_name = f.read().strip()\n",
        "\n",
        "# Construct the full path to the latest checkpoint directory\n",
        "latest_checkpoint_path = os.path.join(checkpoints_dir, latest_checkpoint_name)\n",
        "print(\"Path to the latest checkpoint:\", latest_checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPbSUBro-25L",
        "outputId": "88f0988c-b084-4433-de80-8804b91a2d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-10 17:49:23,625] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "Detected 'pipe-parallel-size' of 0, assuming model is saved as Sequential...\n",
            "> building GPT2BPETokenizer tokenizer ...\n",
            " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
            "Auto-detecting precision to save model into...\n",
            "Saving weights in fp16 precision...\n",
            "['sequential.0.word_embeddings.weight', 'sequential.2.input_layernorm.weight', 'sequential.2.input_layernorm.bias', 'sequential.2.attention.query_key_value.weight', 'sequential.2.attention.query_key_value.bias', 'sequential.2.attention.dense.weight', 'sequential.2.attention.dense.bias', 'sequential.2.post_attention_layernorm.weight', 'sequential.2.post_attention_layernorm.bias', 'sequential.2.mlp.linear1.weight', 'sequential.2.mlp.linear1.bias', 'sequential.2.mlp.linear2.weight', 'sequential.2.mlp.linear2.bias', 'sequential.3.input_layernorm.weight', 'sequential.3.input_layernorm.bias', 'sequential.3.attention.query_key_value.weight', 'sequential.3.attention.query_key_value.bias', 'sequential.3.attention.dense.weight', 'sequential.3.attention.dense.bias', 'sequential.3.post_attention_layernorm.weight', 'sequential.3.post_attention_layernorm.bias', 'sequential.3.mlp.linear1.weight', 'sequential.3.mlp.linear1.bias', 'sequential.3.mlp.linear2.weight', 'sequential.3.mlp.linear2.bias', 'sequential.4.input_layernorm.weight', 'sequential.4.input_layernorm.bias', 'sequential.4.attention.query_key_value.weight', 'sequential.4.attention.query_key_value.bias', 'sequential.4.attention.dense.weight', 'sequential.4.attention.dense.bias', 'sequential.4.post_attention_layernorm.weight', 'sequential.4.post_attention_layernorm.bias', 'sequential.4.mlp.linear1.weight', 'sequential.4.mlp.linear1.bias', 'sequential.4.mlp.linear2.weight', 'sequential.4.mlp.linear2.bias', 'sequential.5.input_layernorm.weight', 'sequential.5.input_layernorm.bias', 'sequential.5.attention.query_key_value.weight', 'sequential.5.attention.query_key_value.bias', 'sequential.5.attention.dense.weight', 'sequential.5.attention.dense.bias', 'sequential.5.post_attention_layernorm.weight', 'sequential.5.post_attention_layernorm.bias', 'sequential.5.mlp.linear1.weight', 'sequential.5.mlp.linear1.bias', 'sequential.5.mlp.linear2.weight', 'sequential.5.mlp.linear2.bias', 'sequential.6.input_layernorm.weight', 'sequential.6.input_layernorm.bias', 'sequential.6.attention.query_key_value.weight', 'sequential.6.attention.query_key_value.bias', 'sequential.6.attention.dense.weight', 'sequential.6.attention.dense.bias', 'sequential.6.post_attention_layernorm.weight', 'sequential.6.post_attention_layernorm.bias', 'sequential.6.mlp.linear1.weight', 'sequential.6.mlp.linear1.bias', 'sequential.6.mlp.linear2.weight', 'sequential.6.mlp.linear2.bias', 'sequential.7.input_layernorm.weight', 'sequential.7.input_layernorm.bias', 'sequential.7.attention.query_key_value.weight', 'sequential.7.attention.query_key_value.bias', 'sequential.7.attention.dense.weight', 'sequential.7.attention.dense.bias', 'sequential.7.post_attention_layernorm.weight', 'sequential.7.post_attention_layernorm.bias', 'sequential.7.mlp.linear1.weight', 'sequential.7.mlp.linear1.bias', 'sequential.7.mlp.linear2.weight', 'sequential.7.mlp.linear2.bias', 'sequential.9.norm.weight', 'sequential.9.norm.bias', 'sequential.10.final_linear.weight']\n",
            "Detected MLP naming convention: new\n",
            "100% 6/6 [00:00<00:00, 170.33it/s]\n"
          ]
        }
      ],
      "source": [
        "!python ./tools/ckpts/convert_neox_to_hf.py --input_dir {latest_checkpoint_path} --config_file /content/GPT-NeoX-Colab/configs/CC19M.yml --output_dir hf_model/save/location --precision auto --architecture neox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVuBSMByImpm"
      },
      "source": [
        "# Code Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztr-ItKf_G1M",
        "outputId": "cd05ccc0-ada2-4afb-cf0c-30158b5d8fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n",
            "[2024-11-10 17:49:35,371] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "Generated text: import sys, os \n",
            " import imp \n",
            " from optparse import make_option \n",
            " from django. conf import settings \n",
            " from django. utils. importlib import import_module \n",
            " from django. core. management import call_command \n",
            " from django. core. management import BaseCommand \n",
            " from django. db import connections \n",
            " def import_app ( app_label, verbosity ) : \n",
            " try : \n",
            " app_path = __import__ ( app_label, { }, { }, [ app_label. split ( '<STR_LIT:.>' ) [ - <NUM_LIT:1> ] ] ). __path__ \n",
            " if '<STR_LIT:1>' : \n",
            " def __ = '<STR_L\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPTNeoXForCausalLM\n",
        "import torch\n",
        "\n",
        "# Move to model directory\n",
        "%cd /content/gpt-neox\n",
        "\n",
        "# Assuming CharLevelTokenizer is properly imported and instantiated\n",
        "from megatron.tokenizer.tokenizer import _GPT2BPETokenizer\n",
        "tokenizer = _GPT2BPETokenizer(vocab_file=\"data/gpt2-vocab.json\", merge_file=\"data/gpt2-merges.txt\")\n",
        "\n",
        "# Load your model\n",
        "model_path = \"/content/gpt-neox/hf_model/save/location\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define a simple char-level tokenizer if not provided\n",
        "def token_level_tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def token_level_detokenize(tokens):\n",
        "    return tokenizer.detokenize(tokens)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user for input\n",
        "input_text = \"\"\"<s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django . utils . importlib import import_module <EOL> from django . core . management import call_command <EOL> from django . core . management import BaseCommand <EOL> from django . db import connections <EOL> def import_app ( app_label , verbosity ) : <EOL> try : <EOL> app_path = __import__ ( app_label , { } , { } , [ app_label . split ( '<STR_LIT:.>' ) [ - <NUM_LIT:1> ] ] ) . __path__ <EOL>\"\"\"\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([token_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = token_level_detokenize(output[0].tolist())\n",
        "\n",
        "# Function to replace special tokens with original representations\n",
        "def replace_special_tokens(text):\n",
        "    replacements = {\n",
        "        \"<EOL>\": \"\\n\",  # Replace with actual newline\n",
        "        \"<s>\": \"\",\n",
        "        \"</s>\": \"\",     # Remove end token\n",
        "        \"<STR_LIT>\": \"STR_LITERAL\",  # Example replacement, adjust as necessary\n",
        "        \"<NUM_LIT>\": \"NUM_LITERAL\",   # Example replacement, adjust as necessary\n",
        "    }\n",
        "\n",
        "    for token, replacement in replacements.items():\n",
        "        text = text.replace(token, replacement)\n",
        "\n",
        "    return text.strip()  # Strip leading/trailing whitespace\n",
        "\n",
        "# Replace special tokens in the generated text\n",
        "final_text = replace_special_tokens(generated_text)\n",
        "\n",
        "# Print the final output\n",
        "print(\"Generated text:\", final_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmLwXfGIc8uJ"
      },
      "outputs": [],
      "source": [
        "!cp /content/gpt-neox/data/gpt2-vocab.json /content/gpt-neox/hf_model/save/location/vocab.json\n",
        "!cp /content/gpt-neox/data/gpt2-merges.txt /content/gpt-neox/hf_model/save/location/merges.txt\n",
        "%cd /content/CodeXGLUE/Code-Code/CodeCompletion-token/code\n",
        "!python -u run_lm.py \\\n",
        "        --data_dir=../dataset/py150/token_completion \\\n",
        "        --lit_file=../dataset/py150/literals.json \\\n",
        "        --langs=$LANG \\\n",
        "        --output_dir=../dataset/py150 \\\n",
        "        --pretrain_dir=/content/gpt-neox/hf_model/save/location \\\n",
        "        --log_file=../completion_python_eval.log \\\n",
        "        --model_type=gpt2neox \\\n",
        "        --block_size=2048 \\\n",
        "        --do_eval \\\n",
        "        --per_gpu_eval_batch_size=4 \\\n",
        "        --logging_steps=100 \\\n",
        "        --seed=42"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aa4762c7bacb4dc1844916f3bfd8710c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cd3f021ba3842e0b9f4ff0c00874224",
              "IPY_MODEL_8a89a879629a4e0bb330b2214721215a",
              "IPY_MODEL_edbfcdd2466c40ffa00e1595cbe849ac"
            ],
            "layout": "IPY_MODEL_a9eac3ece1c4438cb1aaf0c0e227ad49"
          }
        },
        "6cd3f021ba3842e0b9f4ff0c00874224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9914b000602c495db72858496eb694b6",
            "placeholder": "​",
            "style": "IPY_MODEL_31a0249cd5c4469b85e3376ac42684de",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8a89a879629a4e0bb330b2214721215a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5960de3624740668bc32dc4e7a4b01c",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af2ad671425d43c5a7ca1e6c631b9a55",
            "value": 26
          }
        },
        "edbfcdd2466c40ffa00e1595cbe849ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe58e76db375459ba3e7a9a28ae1afa4",
            "placeholder": "​",
            "style": "IPY_MODEL_4768c1c70f844ddea0ce49eb15cc6a60",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.52kB/s]"
          }
        },
        "a9eac3ece1c4438cb1aaf0c0e227ad49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9914b000602c495db72858496eb694b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31a0249cd5c4469b85e3376ac42684de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5960de3624740668bc32dc4e7a4b01c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af2ad671425d43c5a7ca1e6c631b9a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe58e76db375459ba3e7a9a28ae1afa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4768c1c70f844ddea0ce49eb15cc6a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "197713b919cc4c269776eedcec9c0ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_707ab5d9b89d468b915c3b21ad17aba7",
              "IPY_MODEL_944d6f4e21d644eb94a0689def5c3c57",
              "IPY_MODEL_2637a5a2465743efba83b5c63f1bbf80"
            ],
            "layout": "IPY_MODEL_f9c7483621c04244a0146a1ae6d51725"
          }
        },
        "707ab5d9b89d468b915c3b21ad17aba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8854031067824f7c81076a524d225678",
            "placeholder": "​",
            "style": "IPY_MODEL_6bd8b9c87f6e4cdda5693214f22fbc6e",
            "value": "vocab.json: 100%"
          }
        },
        "944d6f4e21d644eb94a0689def5c3c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f7ead849f68487ea1e74cd785442112",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96123ffd1df44949acddd0ddbac5af95",
            "value": 1042301
          }
        },
        "2637a5a2465743efba83b5c63f1bbf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af6a2a18d4914cc28710959adfe960ad",
            "placeholder": "​",
            "style": "IPY_MODEL_3a1ad81812d1495dab13de93262b2f2e",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 5.27MB/s]"
          }
        },
        "f9c7483621c04244a0146a1ae6d51725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8854031067824f7c81076a524d225678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bd8b9c87f6e4cdda5693214f22fbc6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f7ead849f68487ea1e74cd785442112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96123ffd1df44949acddd0ddbac5af95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af6a2a18d4914cc28710959adfe960ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a1ad81812d1495dab13de93262b2f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ade8650955d844418cf162899f3ccca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a1561cb7665412ead4cfaaa23d7912f",
              "IPY_MODEL_48ccf0535996412c80e8defc161561ae",
              "IPY_MODEL_9f947f0fa65f4c049268fdb640fd7c87"
            ],
            "layout": "IPY_MODEL_2b6c6a72577647afb3603a82a7b0ee05"
          }
        },
        "9a1561cb7665412ead4cfaaa23d7912f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8509af4aa5bc4c0c9b0dc17c6704250e",
            "placeholder": "​",
            "style": "IPY_MODEL_de89d28d6e2a4d76956f65c144c1a6aa",
            "value": "merges.txt: 100%"
          }
        },
        "48ccf0535996412c80e8defc161561ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce73a68c9ffc46ca9aba0afee3ce3afe",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b73d3378a484bbc98a8e5f4f600b592",
            "value": 456318
          }
        },
        "9f947f0fa65f4c049268fdb640fd7c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9627f935096d4c829aa24e8196127889",
            "placeholder": "​",
            "style": "IPY_MODEL_1eec404629864d6dab0514845cee97e4",
            "value": " 456k/456k [00:00&lt;00:00, 6.80MB/s]"
          }
        },
        "2b6c6a72577647afb3603a82a7b0ee05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8509af4aa5bc4c0c9b0dc17c6704250e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de89d28d6e2a4d76956f65c144c1a6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce73a68c9ffc46ca9aba0afee3ce3afe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b73d3378a484bbc98a8e5f4f600b592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9627f935096d4c829aa24e8196127889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eec404629864d6dab0514845cee97e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15810cfda50044559763ffadc5459fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_885cc9a2d11b4e778d3dfae9deb25402",
              "IPY_MODEL_6433e05d8c9047758cbdfe0830433423",
              "IPY_MODEL_112bfd2dbb7940c0a524945676c49e79"
            ],
            "layout": "IPY_MODEL_222cca687050497fa3a98390b327f258"
          }
        },
        "885cc9a2d11b4e778d3dfae9deb25402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dce82ae405f74318884b4c6d457d93a1",
            "placeholder": "​",
            "style": "IPY_MODEL_938b842be0fe41709c56813ae9ece40c",
            "value": "tokenizer.json: 100%"
          }
        },
        "6433e05d8c9047758cbdfe0830433423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3f576cc337d41b39e260d9a0ddaa7e3",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aab64d421dbc44078cb779c2d912db43",
            "value": 1355256
          }
        },
        "112bfd2dbb7940c0a524945676c49e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ad17720fd1d4643a6d81cd89e8fc69f",
            "placeholder": "​",
            "style": "IPY_MODEL_0621c7ae4b614e7395e1c92bd3535ed6",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 17.5MB/s]"
          }
        },
        "222cca687050497fa3a98390b327f258": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dce82ae405f74318884b4c6d457d93a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "938b842be0fe41709c56813ae9ece40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3f576cc337d41b39e260d9a0ddaa7e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aab64d421dbc44078cb779c2d912db43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ad17720fd1d4643a6d81cd89e8fc69f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0621c7ae4b614e7395e1c92bd3535ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bcef93bfeaa4303a283a1fdf55532bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_570c367cf297448188007453a84ebe7a",
              "IPY_MODEL_05c79f1b886441cfa0e8d6845ffe2680",
              "IPY_MODEL_6bf824739ef846d79d53349bb9e5d442"
            ],
            "layout": "IPY_MODEL_f58bc3875ea24a92a14931d63eddad0b"
          }
        },
        "570c367cf297448188007453a84ebe7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da0950d195dc4014b3a8c7591df1d4a8",
            "placeholder": "​",
            "style": "IPY_MODEL_b6bffae6a34a48158cb56536224bab3d",
            "value": "config.json: 100%"
          }
        },
        "05c79f1b886441cfa0e8d6845ffe2680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f35528d3781642ba9b0d0858e5c11141",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d44435a75804e5ba5c8e83741186b72",
            "value": 665
          }
        },
        "6bf824739ef846d79d53349bb9e5d442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84be457457b54efeb1ccbcec3478be02",
            "placeholder": "​",
            "style": "IPY_MODEL_09d7f86a07a14249b04bae3848b7e924",
            "value": " 665/665 [00:00&lt;00:00, 42.4kB/s]"
          }
        },
        "f58bc3875ea24a92a14931d63eddad0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da0950d195dc4014b3a8c7591df1d4a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6bffae6a34a48158cb56536224bab3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f35528d3781642ba9b0d0858e5c11141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d44435a75804e5ba5c8e83741186b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84be457457b54efeb1ccbcec3478be02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d7f86a07a14249b04bae3848b7e924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}