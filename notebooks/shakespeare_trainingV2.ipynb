{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/venv/notebooks/shakespeare_trainingV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYyxsLpOuQR1",
        "outputId": "11d5a250-f3c4-4623-8495-b8fa31aab142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Date and Time: 2024-11-09 09:36:27.058039\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(\"Current Date and Time:\", datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j_hUsQxlhnou",
        "outputId": "f744ac61-ebf8-42aa-af58-91f06956b41e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'GPT-NeoX-Colab'...\n",
            "remote: Enumerating objects: 839, done.\u001b[K\n",
            "remote: Counting objects: 100% (199/199), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 839 (delta 85), reused 157 (delta 61), pack-reused 640 (from 1)\u001b[K\n",
            "Receiving objects: 100% (839/839), 15.56 MiB | 1.69 MiB/s, done.\n",
            "Resolving deltas: 100% (423/423), done.\n",
            "/content/GPT-NeoX-Colab\n",
            "git-annex is already installed.\n",
            "Starting git annex sync...\n",
            "Sync successful. Fetching data from backblaze...\n",
            "Data retrieval successful.\n",
            "Data retrieval successful.\n",
            "Data retrieval successful.\n",
            "Data retrieval successful.\n",
            "CPU times: user 10.4 s, sys: 1.26 s, total: 11.6 s\n",
            "Wall time: 41 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%cd /content\n",
        "# Don't use --depth 1 because that does not play nice with git-annex\n",
        "!git clone -b venv https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "%cd /content/GPT-NeoX-Colab\n",
        "!pip install . > /dev/null 2>&1\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv('/content/GPT-NeoX-Colab/.env')\n",
        "import GPTNeoXColab\n",
        "GPTNeoXColab.utils.colab.install_git_annex()\n",
        "GPTNeoXColab.utils.colab.enable_remote()\n",
        "GPTNeoXColab.utils.colab.sync_annex()\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare.txt\")\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare.jsonl\")\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare_text_document.bin\")\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/shakespeare_text_document.idx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellView": "form",
        "id": "Afe83IZTrTrL"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "GPTNeoXColab.utils.colab.mount_google_drive(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "3fGdvLyYcjTO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This function requires Google Colab and won't work in other environments.\n"
          ]
        }
      ],
      "source": [
        "#@title SSH Connection for VS Code\n",
        "GPTNeoXColab.utils.colab.setup_ssh_connection(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e27VRVq07s"
      },
      "source": [
        "# Cloning GPT-NeoX Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "id": "FjLEIFCR6d8m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'gpt-neox'...\n",
            "remote: Enumerating objects: 296, done.\u001b[K\n",
            "remote: Counting objects: 100% (296/296), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 296 (delta 74), reused 136 (delta 43), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (296/296), 2.50 MiB | 2.44 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n",
            "CPU times: user 60.8 ms, sys: 21.8 ms, total: 82.5 ms\n",
            "Wall time: 2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%cd /content\n",
        "!git clone --depth 1 https://github.com/EleutherAI/gpt-neox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Jp8wncmZDAew"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading my_env.tar.gz\n",
            "Unzipping my_env.tar.gz\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "Command failed with return code 127 CompletedProcess(args='pigz -d -p 4 /content/my_env.tar.gz', returncode=127, stdout=b'', stderr=b'/bin/sh: 1: pigz: not found\\n')",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
            "File \u001b[0;32m/workspace/src/GPTNeoXColab/utils/colab.py:227\u001b[0m, in \u001b[0;36mdownload_my_env\u001b[0;34m(upload_env)\u001b[0m\n\u001b[1;32m    224\u001b[0m # Compress the tar archive with pigz, showing a progress indicator\n\u001b[1;32m    225\u001b[0m # Compresses 5.5G to 3.1G\n\u001b[1;32m    226\u001b[0m run(\"time tar cf - my_env | pv -p -e -r -b | pigz -p 4 -1 > my_env.tar.gz\")\n\u001b[0;32m--> 227\u001b[0m # Upload to BackBlaze\n\u001b[1;32m    228\u001b[0m if userdata.get(\"B2_APP_KEY_RW\"):\n\u001b[1;32m    229\u001b[0m     response = upload_file(os.getenv(\"BB_BUCKET\"), \"/content\", \"my_env.tar.gz\", b2_rw)\n",
            "File \u001b[0;32m/workspace/src/GPTNeoXColab/utils/colab.py:146\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cmd, check)\u001b[0m\n\u001b[1;32m    142\u001b[0m         run(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapt-get update\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m         run(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapt-get install -y git-annex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menable_remote\u001b[39m():\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Enable git annex backblaze remote.\"\"\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     run(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit annex enableremote backblaze\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mException\u001b[0m: Command failed with return code 127 CompletedProcess(args='pigz -d -p 4 /content/my_env.tar.gz', returncode=127, stdout=b'', stderr=b'/bin/sh: 1: pigz: not found\\n')"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%cd /content\n",
        "BACKBLAZE_SAVE = False\n",
        "GPTNeoXColab.utils.colab.download_my_env(BACKBLAZE_SAVE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OBHMRDRap0G"
      },
      "source": [
        "# Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "8T7JsHvusiyy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Reading package lists... 0%\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
            "E: Unable to lock directory /var/lib/apt/lists/\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: virtualenv in /home/vscode/.local/lib/python3.10/site-packages (20.27.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /home/vscode/.local/lib/python3.10/site-packages (from virtualenv) (0.3.9)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/site-packages (from virtualenv) (3.16.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/site-packages (from virtualenv) (4.3.6)\n",
            "/content/gpt-neox\n",
            "/bin/sh: 1: source: not found\n",
            "/bin/sh: 1: source: not found\n",
            "/bin/sh: 1: source: not found\n",
            "CPU times: user 349 ms, sys: 85.5 ms, total: 434 ms\n",
            "Wall time: 15.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%cd /content\n",
        "import os\n",
        "# Check if the directory does not exist\n",
        "if not os.path.isdir(\"/content/my_env\"):\n",
        "    # Install venv package for Python 3.10\n",
        "    !apt-get update && apt-get install -y python3.10-venv\n",
        "    !pip install virtualenv\n",
        "    # Create the virtual environment\n",
        "    !python3 -m venv /content/my_env\n",
        "    %cd /content/gpt-neox\n",
        "    # Install specific versions of torch and other packages to avoid compatibility issues\n",
        "    !source /content/my_env/bin/activate && pip install torch==2.3.0 torchaudio==2.3.0 torchvision==0.18.0 transformers==4.41.0 sentence-transformers==2.2.2\n",
        "    # Install dependencies\n",
        "    !source /content/my_env/bin/activate && pip install -r ./requirements/requirements.txt\n",
        "    !source /content/my_env/bin/activate && pip install -r ./requirements/requirements-tensorboard.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zfI_5Iz_-t3b"
      },
      "outputs": [],
      "source": [
        "GPTNeoXColab.utils.colab.upload_my_env(BACKBLAZE_SAVE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jzX5ohGax6p"
      },
      "source": [
        "# Preparing Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "N-hmZjCc-WnV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gpt-neox\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'GPTNeoXColab.utils' has no attribute 'ml'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Check if the file exists\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/GPT-NeoX-Colab/data/shakespeare.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mGPTNeoXColab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mml\u001b[49m\u001b[38;5;241m.\u001b[39mtext2jsonl(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/GPT-NeoX-Colab/data/shakespeare.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/GPT-NeoX-Colab/data/shakespeare.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp /content/GPT-NeoX-Colab/data/shakespeare.jsonl /content/GPT-NeoX-Colab/data/shakespeare.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'GPTNeoXColab.utils' has no attribute 'ml'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "%cd /content/gpt-neox\n",
        "!mkdir -p data\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.isfile(\"/content/GPT-NeoX-Colab/data/shakespeare.jsonl\"):\n",
        "    GPTNeoXColab.utils.ml.text2jsonl(\"/content/GPT-NeoX-Colab/data/shakespeare.jsonl\", '/content/GPT-NeoX-Colab/data/shakespeare.jsonl')\n",
        "\n",
        "\n",
        "!cp /content/GPT-NeoX-Colab/data/shakespeare.jsonl /content/GPT-NeoX-Colab/data/shakespeare.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZzye-15a5aO"
      },
      "source": [
        "# TinyShakespeare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x57thNaLa-yN"
      },
      "source": [
        "# Tokenizing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JyD8RujkEUsr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/gpt-neox/tools/datasets/preprocess_data.py\", line 25, in <module>\n",
            "    import lm_dataformat as lmd\n",
            "ModuleNotFoundError: No module named 'lm_dataformat'\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command 'b'source /content/my_env/bin/activate\\ncd /content/gpt-neox\\nmkdir -p processed_data\\npython tools/datasets/preprocess_data.py \\\\\\n    --input ./data/shakespeare.jsonl \\\\\\n    --output-prefix ./processed_data/shakespeare \\\\\\n    --tokenizer-type CharLevelTokenizer \\\\\\n    --dataset-impl mmap \\\\\\n    --append-eod\\n'' returned non-zero exit status 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'source /content/my_env/bin/activate\\ncd /content/gpt-neox\\nmkdir -p processed_data\\npython tools/datasets/preprocess_data.py \\\\\\n    --input ./data/shakespeare.jsonl \\\\\\n    --output-prefix ./processed_data/shakespeare \\\\\\n    --tokenizer-type CharLevelTokenizer \\\\\\n    --dataset-impl mmap \\\\\\n    --append-eod\\n'' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%%bash\n",
        "source /content/my_env/bin/activate\n",
        "cd /content/gpt-neox\n",
        "mkdir -p processed_data\n",
        "python tools/datasets/preprocess_data.py \\\n",
        "    --input ./data/shakespeare.jsonl \\\n",
        "    --output-prefix ./processed_data/shakespeare \\\n",
        "    --tokenizer-type CharLevelTokenizer \\\n",
        "    --dataset-impl mmap \\\n",
        "    --append-eod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QszVj7_vSP7L"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNM2gpADtjM9"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mfqqEw_lnbh"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "# Start a detached background process\n",
        "process = subprocess.Popen(\n",
        "    \"nohup bash -c \\\"source /content/my_env/bin/activate && python ./deepy.py train.py --conf_dir /content/GPT-NeoX-Colab/configs shakespeare.yml shakespeare_deepy.yml\\\" & echo $! > train_process.pid\",\n",
        "    shell=True,\n",
        "    executable='/bin/bash',\n",
        "    preexec_fn=subprocess.os.setsid  # Starts the process in a new session so interrupting Notebook does not kill the training\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVCgdZ1Ma9TI"
      },
      "outputs": [],
      "source": [
        "# Wait until a tensorboard is created\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Path to the TensorBoard log directory\n",
        "tensorboard_log_dir = '/content/gpt-neox/tensorboard'\n",
        "\n",
        "# Wait for the directory to be created\n",
        "while not os.path.exists(tensorboard_log_dir):\n",
        "    print(\"Waiting for TensorBoard log directory to be created...\")\n",
        "    time.sleep(10)  # Check every X seconds\n",
        "\n",
        "print(\"TensorBoard log directory found. You can now launch TensorBoard.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfefGgc-ln-J"
      },
      "outputs": [],
      "source": [
        "# Need to remove everything in checkpoints and tensorboard dir for a fresh run\n",
        "%tensorboard --logdir tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j3_fG5vV7AB"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Define the log directory and pattern for log files\n",
        "log_dir = \"/content/gpt-neox/logs\"\n",
        "log_pattern = os.path.join(log_dir, \"*_stdout.txt\")\n",
        "\n",
        "# Get the list of log files that match the pattern\n",
        "log_files = glob.glob(log_pattern)\n",
        "\n",
        "# Ensure there are log files in the directory\n",
        "if log_files:\n",
        "    # Find the latest log file based on modification time\n",
        "    latest_log = max(log_files, key=os.path.getmtime)\n",
        "    print(\"Latest log file:\", latest_log)\n",
        "else:\n",
        "    latest_log = None\n",
        "    print(\"No log files found.\")\n",
        "\n",
        "# Now `latest_log` holds the path to the latest log file or None if not found\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn13kNSRacnA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import re\n",
        "\n",
        "# File to store the last read position (persistence between script runs)\n",
        "file_position = 0\n",
        "# Regular expression to match \"iteration <number> / <total>\"\n",
        "iteration_pattern = re.compile(r\"iteration\\s+(\\d+)\\s*/\\s*\\d+\")\n",
        "\n",
        "def read_new_iterations():\n",
        "    global file_position\n",
        "    # Open the log file and seek to the last position\n",
        "    with open(latest_log, \"r\") as file:\n",
        "        file.seek(file_position)\n",
        "        # Read new lines\n",
        "        new_lines = file.readlines()\n",
        "        file_position = file.tell()\n",
        "        # Process lines containing \"iteration\"\n",
        "        last_match = None\n",
        "        for line in new_lines:\n",
        "            match = iteration_pattern.search(line)\n",
        "            if match:\n",
        "                last_match = match\n",
        "        if last_match:\n",
        "            # Extract the iteration count from the regex match\n",
        "            iteration_count = int(last_match.group(1))\n",
        "            print(f\"{iteration_count} iterations\")\n",
        "\n",
        "# Read the PID from the file\n",
        "with open(\"train_process.pid\", \"r\") as f:\n",
        "    pid = int(f.read().strip())\n",
        "    print(\"PID:\", pid)\n",
        "\n",
        "# Function to check if the process is running\n",
        "def is_process_running(pid):\n",
        "    try:\n",
        "        os.kill(pid, 0)  # Sending signal 0 to check if the process exists\n",
        "        return True\n",
        "    except OSError:\n",
        "        return False\n",
        "\n",
        "# Monitor the process\n",
        "while is_process_running(pid):\n",
        "    read_new_iterations()\n",
        "    print(\"Training is still running...\")\n",
        "    time.sleep(30)  # Check every X seconds\n",
        "\n",
        "print(\"Training has finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66bhedFQcNB8"
      },
      "outputs": [],
      "source": [
        "# Wait until a checkpoint is created\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Path to the checkpoints directory\n",
        "checkpoints_dir = '/content/gpt-neox/checkpoints'\n",
        "\n",
        "# Wait for the directory to be created\n",
        "while not os.path.exists(checkpoints_dir):\n",
        "    print(\"Waiting for checkpoints directory to be created...\")\n",
        "    time.sleep(10)  # Check every X seconds\n",
        "\n",
        "print(\"Checkpoints directory found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJb7TUqi4vVg"
      },
      "outputs": [],
      "source": [
        "#@title Training and Validation Loss\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "import os\n",
        "import numpy as np\n",
        "# Path to the latest log file\n",
        "log_dir = \"tensorboard\"\n",
        "log_files = [os.path.join(log_dir, d) for d in os.listdir(log_dir)]\n",
        "latest_log_dir = max(log_files, key=os.path.getmtime)\n",
        "\n",
        "# Initialize EventAccumulator to load scalar data\n",
        "ea = event_accumulator.EventAccumulator(latest_log_dir)\n",
        "ea.Reload()  # Load all logs\n",
        "\n",
        "# List all scalar keys available in the logs\n",
        "scalar_keys = ea.Tags()['scalars']\n",
        "print(\"Available scalar keys:\", scalar_keys)\n",
        "\n",
        "# Extract training and validation losses\n",
        "train_loss = ea.Scalars('train/lm_loss')  # Adjust for actual name if necessary\n",
        "val_loss = ea.Scalars('validation/lm_loss')  # Adjust for actual name if necessary\n",
        "\n",
        "# Convert to lists for plotting\n",
        "train_loss_values = [x.value for x in train_loss]\n",
        "val_loss_values = [x.value for x in val_loss]\n",
        "\n",
        "# Find the lengths of both arrays\n",
        "len_train = len(train_loss_values)\n",
        "len_val = len(val_loss_values)\n",
        "\n",
        "iterations = None\n",
        "# Interpolate the shorter array\n",
        "if len_train != len_val:\n",
        "    if len_train > len_val:\n",
        "        # Interpolate validation loss to match the training loss length\n",
        "        iterations = np.linspace(1, len_train, len_train)\n",
        "        val_iterations = np.linspace(1, len_train, len_val)\n",
        "        val_loss_values = np.interp(iterations, val_iterations, val_loss_values)\n",
        "    else:\n",
        "        # Interpolate training loss to match the validation loss length\n",
        "        iterations = np.linspace(1, len_val, len_val)\n",
        "        train_iterations = np.linspace(1, len_val, len_train)\n",
        "        train_loss_values = np.interp(iterations, train_iterations, train_loss_values)\n",
        "else:\n",
        "    iterations = range(1, len_train + 1)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(iterations, train_loss_values, label='Training Loss')\n",
        "plt.plot(iterations, val_loss_values, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VpdD5ZzfGTf"
      },
      "outputs": [],
      "source": [
        "# If we want to monitor the logs\n",
        "#!tail -f /content/gpt-neox/logs/*stdout.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk8DhmmEZFyz"
      },
      "source": [
        "# Inference with GPT-NeoX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKb0Ar6NZFyz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# This has issues if used during training -  The server socket has failed to bind to [::]:29500 (errno: 98 - Address already\n",
        "# This will write over the logs\n",
        "!source /content/my_env/bin/activate && python ./deepy.py generate.py -d configs /content/GPT-NeoX-Colab/configs/shakespeare.yml /content/GPT-NeoX-Colab/configs/shakespeare_gen.yml > /dev/null\n",
        "!cat sample_output.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7AOL1NJ43e"
      },
      "source": [
        "# Inference with Hugging Face\n",
        "\n",
        "## Convert Our Model to HuggingFace Format\n",
        "Here we are converting our model to `HuggingFace Format`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNdWrrkWfyT-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the checkpoints directory\n",
        "checkpoints_dir = \"/content/gpt-neox/checkpoints\"\n",
        "\n",
        "# Read the 'latest' file to get the latest checkpoint name\n",
        "with open(os.path.join(checkpoints_dir, \"latest\"), \"r\") as f:\n",
        "    latest_checkpoint_name = f.read().strip()\n",
        "\n",
        "# Construct the full path to the latest checkpoint directory\n",
        "latest_checkpoint_path = os.path.join(checkpoints_dir, latest_checkpoint_name)\n",
        "print(\"Path to the latest checkpoint:\", latest_checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rplkNclPXgkH"
      },
      "outputs": [],
      "source": [
        "!source /content/my_env/bin/activate && python ./tools/ckpts/convert_neox_to_hf.py --input_dir {latest_checkpoint_path} --config_file /content/GPT-NeoX-Colab/configs/shakespeare.yml --output_dir hf_model/save/location --precision auto --architecture neox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qra1qQtC2oiI"
      },
      "source": [
        "## Generating Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01ZRN2IceM8a"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoXForCausalLM\n",
        "import torch\n",
        "\n",
        "# Move to model directory\n",
        "%cd /content/gpt-neox\n",
        "\n",
        "# Assuming CharLevelTokenizer is properly imported and instantiated\n",
        "from GPTNeoXColab import CharLevelTokenizer\n",
        "tokenizer = CharLevelTokenizer.CharLevelTokenizer(vocab_size=512)\n",
        "\n",
        "# Load your model\n",
        "model_path = \"/content/gpt-neox/hf_model/save/location\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define a simple char-level tokenizer if not provided\n",
        "def char_level_tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def char_level_detokenize(tokens):\n",
        "    return tokenizer.detokenize(tokens)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user for input\n",
        "#input_text = input(\"Enter your prompt: \")\n",
        "input_text = \"Thou art\"\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([char_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = char_level_detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
