{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/venv/notebooks/shakespeare_trainingV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYyxsLpOuQR1",
        "outputId": "11d5a250-f3c4-4623-8495-b8fa31aab142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Date and Time: 2024-11-08 13:41:39.051959\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(\"Current Date and Time:\", datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j_hUsQxlhnou",
        "outputId": "f744ac61-ebf8-42aa-af58-91f06956b41e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'GPT-NeoX-Colab' already exists and is not an empty directory.\n",
            "/content/GPT-NeoX-Colab\n",
            "git-annex is already installed.\n",
            "Starting git annex sync...\n",
            "Sync successful. Fetching data from backblaze...\n",
            "Data retrieval successful.\n",
            "CPU times: user 211 ms, sys: 42.5 ms, total: 253 ms\n",
            "Wall time: 29.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%cd /content\n",
        "# Don't use --depth 1 because that does not play nice with git-annex\n",
        "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "%cd /content/GPT-NeoX-Colab\n",
        "!pip install . >& /dev/null\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv('/content/GPT-NeoX-Colab/.env')\n",
        "from GPTNeoXColab import utils\n",
        "utils.install_git_annex()\n",
        "utils.enable_remote()\n",
        "utils.sync_and_get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Afe83IZTrTrL"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "SAVE_TO_DRIVE = False\n",
        "CACHE_PIP = False\n",
        "if SAVE_TO_DRIVE or CACHE_PIP:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  !mkdir -p /content/drive/MyDrive/pip_cache\n",
        "  !mkdir -p /content/drive/MyDrive/pip_wheels\n",
        "  %env PIP_CACHE_DIR=/content/drive/MyDrive/pip_cache\n",
        "  %env PIP_FIND_LINKS=/content/drive/MyDrive/pip_wheels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "3fGdvLyYcjTO"
      },
      "outputs": [],
      "source": [
        "#@title SSH Connection for VS Code\n",
        "SSH_CONNECT = False\n",
        "if SSH_CONNECT:\n",
        "  from google.colab import userdata\n",
        "  if \"SSH_CONNECTED\" not in globals():\n",
        "      SSH_CONNECTED = None\n",
        "  # This is using the Colab \"Secrets\" feature\n",
        "  if userdata.get('REMOTE_SSH') and not SSH_CONNECTED:\n",
        "    SSH_CONNECTED = True\n",
        "    !apt-get install -y openssh-server\n",
        "    !mkdir -p /var/run/sshd\n",
        "    !echo 'root:root' | chpasswd  # Set the root password to 'root'\n",
        "    !echo 'PermitRootLogin yes' >> /etc/ssh/sshd_config\n",
        "    !service ssh restart\n",
        "    !apt-get install -y screen\n",
        "    import pexpect\n",
        "    import getpass\n",
        "    password = getpass.getpass(\"Enter your SSH password: \")\n",
        "    ssh_command = f\"ssh -N -R 2223:localhost:22 -o StrictHostKeyChecking=no -o ServerAliveInterval=60 -o ServerAliveCountMax=5 {userdata.get('REMOTE_SSH')}\"\n",
        "    child = pexpect.spawn(ssh_command, encoding='utf-8')\n",
        "    child.expect([\"password:\", \"Password:\"], timeout=60)\n",
        "    child.sendline(password)\n",
        "    child.sendline(\"\") # Send an empty command to prevent immediate exit\n",
        "    print(\"SSH session is running in the background.\")\n",
        "    # AFter this the client should also connect to the notebook SSH via the forwarded port\n",
        "    # ssh -N -L 9999:localhost:2223 -o ServerAliveInterval=60 -o ServerAliveCountMax=5 REMOTE_SSH\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e27VRVq07s"
      },
      "source": [
        "# Cloning GPT-NeoX Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FjLEIFCR6d8m"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%cd /content\n",
        "# Fix for text_generation\n",
        "#!git clone https://github.com/EleutherAI/gpt-neox\n",
        "!git clone -b text_gen_not_parallel https://github.com/markNZed/gpt-neox.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp8wncmZDAew"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Experimenting with BackBlaze\n",
        "BACKBLAZE_SAVE = False\n",
        "%cd /content\n",
        "!pip install boto3\n",
        "# Install pigz and pv for multi-threaded compression and progress tracking\n",
        "!apt-get install -y pigz pv\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "from botocore.config import Config\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "# Upload specified file into the specified bucket\n",
        "def upload_file(bucket, directory, file, b2, b2path=None):\n",
        "    file_path = directory + '/' + file\n",
        "    remote_path = b2path\n",
        "    if remote_path is None:\n",
        "        remote_path = file\n",
        "    try:\n",
        "        response = b2.Bucket(bucket).upload_file(file_path, remote_path)\n",
        "    except ClientError as ce:\n",
        "        print('error', ce)\n",
        "        traceback.print_exc()  # Print the full stack trace\n",
        "    return response\n",
        "\n",
        "# Download the specified object from B2 and write to local file system\n",
        "def download_file(bucket, directory, file, key_name, b2):\n",
        "    file_path = directory + '/' + file\n",
        "    try:\n",
        "        b2.Bucket(bucket).download_file(key_name, file_path)\n",
        "    except ClientError as ce:\n",
        "        print('error', ce)\n",
        "        traceback.print_exc()  # Print the full stack trace\n",
        "\n",
        "# Return a boto3 resource object for B2 service\n",
        "def get_b2_resource(endpoint, key_id, application_key):\n",
        "    b2 = boto3.resource(\n",
        "        service_name='s3',\n",
        "        endpoint_url=endpoint,                # Backblaze endpoint\n",
        "        aws_access_key_id=key_id,              # Backblaze keyID\n",
        "        aws_secret_access_key=application_key, # Backblaze applicationKey\n",
        "        config = Config(\n",
        "            signature_version='s3v4',\n",
        "        )\n",
        "    )\n",
        "    return b2\n",
        "\n",
        "key_id_r = \"003cb130fbeaa800000000001\"\n",
        "app_id_r = \"K00347hujVVLL/TFqml/lXyhtkxB/C0\"\n",
        "b2_endpoint = \"https://s3.eu-central-003.backblazeb2.com\"\n",
        "b2_r = get_b2_resource(b2_endpoint, key_id_r, app_id_r)\n",
        "if BACKBLAZE_SAVE:\n",
        "  from google.colab import userdata\n",
        "  key_id_rw = \"003cb130fbeaa800000000002\"\n",
        "  b2_rw = get_b2_resource(b2_endpoint, key_id_rw, userdata.get('B2_APP_KEY_RW'))\n",
        "bucket = \"GPT-NeoX-Colab\"\n",
        "directory = \"/content\"\n",
        "if not os.path.exists(\"/content/my_env.tar\") and not BACKBLAZE_SAVE:\n",
        "  # Downloading takes about 40sec\n",
        "  print(\"Downloading my_env.tar.gz\")\n",
        "  download_file(bucket, directory, \"my_env.tar.gz\", \"my_env.tar.gz\", b2_r)\n",
        "  print(\"Unzipping my_env.tar.gz\")\n",
        "  !pigz -d -p 4 /content/my_env.tar.gz  # Decompress using 4 cores (adjust as needed)\n",
        "  print(\"Untarring my_env.tar.gz\")\n",
        "  !pv /content/my_env.tar | tar -xf - -C /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OBHMRDRap0G"
      },
      "source": [
        "# Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8T7JsHvusiyy"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Measure time with `time` command\n",
        "time {\n",
        "  # Check if the directory does not exist\n",
        "  if [ ! -d \"/content/my_env\" ]; then\n",
        "    # Install venv package for Python 3.10\n",
        "    apt-get update && apt-get install -y python3.10-venv\n",
        "\n",
        "    # Install virtualenv\n",
        "    pip install virtualenv\n",
        "\n",
        "    # Create and activate the virtual environment\n",
        "    python3 -m venv /content/my_env\n",
        "    source /content/my_env/bin/activate\n",
        "\n",
        "    # Change to the GPT-NeoX directory\n",
        "    cd /content/gpt-neox\n",
        "\n",
        "    # Install specific versions of torch and other packages to avoid compatibility issues\n",
        "    pip install torch==2.3.0 torchaudio==2.3.0 torchvision==0.18.0 transformers==4.41.0 sentence-transformers==2.2.2\n",
        "\n",
        "    # Install dependencies with parallel jobs to speed up\n",
        "    pip install -r ./requirements/requirements.txt\n",
        "    pip install -r ./requirements/requirements-tensorboard.txt\n",
        "  fi\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfI_5Iz_-t3b"
      },
      "outputs": [],
      "source": [
        "# tar and gzip the venv, takes about 2.5mins\n",
        "# Check if the compressed file already exists\n",
        "if not os.path.exists(\"/content/my_env.tar\") and BACKBLAZE_SAVE:\n",
        "    # Only run if my_env.tar.gz does not exist\n",
        "    print(\"my_env.tar not found, proceeding with compression.\")\n",
        "    # Change directory (if needed) to ensure paths are correct\n",
        "    %cd /content\n",
        "    # Compress the tar archive with pigz, showing a progress indicator\n",
        "    # Compresses 5.5G to 3.1G\n",
        "    !time tar cf - my_env | pv -p -e -r -b | pigz -p 4 -1 > my_env.tar.gz\n",
        "    # Upload to BackBlaze\n",
        "    if userdata.get('B2_APP_KEY_RW'):\n",
        "      response = upload_file(bucket, directory, \"my_env.tar.gz\", b2_rw)\n",
        "      print(\"Upload response:\", response)\n",
        "    else:\n",
        "      print(\"Skipping upload to BackBlaze.\")\n",
        "else:\n",
        "    print(\"Skipping upload to BackBlaze.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jzX5ohGax6p"
      },
      "source": [
        "# Preparing Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-hmZjCc-WnV"
      },
      "outputs": [],
      "source": [
        "%cd /content/gpt-neox\n",
        "!mkdir -p data\n",
        "\n",
        "import json\n",
        "\n",
        "# Generate a list of dictionaries\n",
        "lines = []\n",
        "with open(\"/content/GPT-NeoX-Colab/data/shakespeare.txt\", encoding=\"utf8\") as f:\n",
        "    for line in f.read().splitlines():\n",
        "        if line:\n",
        "            lines.append({\"text\": line})\n",
        "\n",
        "# Convert to a list of JSON strings\n",
        "json_lines = [json.dumps(l) for l in lines]\n",
        "\n",
        "# Join lines and save to .jsonl file\n",
        "json_data = '\\n'.join(json_lines)\n",
        "with open('/content/gpt-neox/data/tinyshakespeare.jsonl', 'w') as f:\n",
        "    f.write(json_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZzye-15a5aO"
      },
      "source": [
        "# TinyShakespeare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x57thNaLa-yN"
      },
      "source": [
        "# Tokenizing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyD8RujkEUsr"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%%bash\n",
        "source /content/my_env/bin/activate\n",
        "cd /content/gpt-neox\n",
        "mkdir -p processed_data\n",
        "python tools/datasets/preprocess_data.py \\\n",
        "    --input ./data/tinyshakespeare.jsonl \\\n",
        "    --output-prefix ./processed_data/tinyshakespeare \\\n",
        "    --tokenizer-type CharLevelTokenizer \\\n",
        "    --dataset-impl mmap \\\n",
        "    --append-eod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QszVj7_vSP7L"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNM2gpADtjM9"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mfqqEw_lnbh"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "# Start a detached background process\n",
        "process = subprocess.Popen(\n",
        "    \"nohup bash -c \\\"source /content/my_env/bin/activate && python ./deepy.py train.py --conf_dir /content/GPT-NeoX-Colab/configs shakespeare.yml shakespeare_deepy.yml\\\" > /dev/null 2>&1 & echo $! > train_process.pid\",\n",
        "    shell=True,\n",
        "    executable='/bin/bash',\n",
        "    preexec_fn=subprocess.os.setsid  # Starts the process in a new session\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVCgdZ1Ma9TI"
      },
      "outputs": [],
      "source": [
        "# Wait until a tensorboard is created\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Path to the TensorBoard log directory\n",
        "tensorboard_log_dir = '/content/gpt-neox/tensorboard'\n",
        "\n",
        "# Wait for the directory to be created\n",
        "while not os.path.exists(tensorboard_log_dir):\n",
        "    print(\"Waiting for TensorBoard log directory to be created...\")\n",
        "    time.sleep(10)  # Check every X seconds\n",
        "\n",
        "print(\"TensorBoard log directory found. You can now launch TensorBoard.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfefGgc-ln-J"
      },
      "outputs": [],
      "source": [
        "# Need to remove everything in checkpoints and tensorboard dir for a fresh run\n",
        "%tensorboard --logdir tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j3_fG5vV7AB"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Define the log directory and pattern for log files\n",
        "log_dir = \"/content/gpt-neox/logs\"\n",
        "log_pattern = os.path.join(log_dir, \"*_stdout.txt\")\n",
        "\n",
        "# Get the list of log files that match the pattern\n",
        "log_files = glob.glob(log_pattern)\n",
        "\n",
        "# Ensure there are log files in the directory\n",
        "if log_files:\n",
        "    # Find the latest log file based on modification time\n",
        "    latest_log = max(log_files, key=os.path.getmtime)\n",
        "    print(\"Latest log file:\", latest_log)\n",
        "else:\n",
        "    latest_log = None\n",
        "    print(\"No log files found.\")\n",
        "\n",
        "# Now `latest_log` holds the path to the latest log file or None if not found\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn13kNSRacnA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import re\n",
        "\n",
        "# File to store the last read position (persistence between script runs)\n",
        "file_position = 0\n",
        "# Regular expression to match \"iteration <number> / <total>\"\n",
        "iteration_pattern = re.compile(r\"iteration\\s+(\\d+)\\s*/\\s*\\d+\")\n",
        "\n",
        "def read_new_iterations():\n",
        "    global file_position\n",
        "    # Open the log file and seek to the last position\n",
        "    with open(latest_log, \"r\") as file:\n",
        "        file.seek(file_position)\n",
        "        # Read new lines\n",
        "        new_lines = file.readlines()\n",
        "        file_position = file.tell()\n",
        "        # Process lines containing \"iteration\"\n",
        "        last_match = None\n",
        "        for line in new_lines:\n",
        "            match = iteration_pattern.search(line)\n",
        "            if match:\n",
        "                last_match = match\n",
        "        if last_match:\n",
        "            # Extract the iteration count from the regex match\n",
        "            iteration_count = int(last_match.group(1))\n",
        "            print(f\"{iteration_count} iterations\")\n",
        "\n",
        "# Read the PID from the file\n",
        "with open(\"train_process.pid\", \"r\") as f:\n",
        "    pid = int(f.read().strip())\n",
        "    print(\"PID:\", pid)\n",
        "\n",
        "# Function to check if the process is running\n",
        "def is_process_running(pid):\n",
        "    try:\n",
        "        os.kill(pid, 0)  # Sending signal 0 to check if the process exists\n",
        "        return True\n",
        "    except OSError:\n",
        "        return False\n",
        "\n",
        "# Monitor the process\n",
        "while is_process_running(pid):\n",
        "    read_new_iterations()\n",
        "    print(\"Training is still running...\")\n",
        "    time.sleep(30)  # Check every X seconds\n",
        "\n",
        "print(\"Training has finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66bhedFQcNB8"
      },
      "outputs": [],
      "source": [
        "# Wait until a checkpoint is created\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Path to the checkpoints directory\n",
        "checkpoints_dir = '/content/gpt-neox/checkpoints'\n",
        "\n",
        "# Wait for the directory to be created\n",
        "while not os.path.exists(checkpoints_dir):\n",
        "    print(\"Waiting for checkpoints directory to be created...\")\n",
        "    time.sleep(10)  # Check every X seconds\n",
        "\n",
        "print(\"Checkpoints directory found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJb7TUqi4vVg"
      },
      "outputs": [],
      "source": [
        "#@title Training and Validation Loss\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "import os\n",
        "import numpy as np\n",
        "# Path to the latest log file\n",
        "log_dir = \"tensorboard\"\n",
        "log_files = [os.path.join(log_dir, d) for d in os.listdir(log_dir)]\n",
        "latest_log_dir = max(log_files, key=os.path.getmtime)\n",
        "\n",
        "# Initialize EventAccumulator to load scalar data\n",
        "ea = event_accumulator.EventAccumulator(latest_log_dir)\n",
        "ea.Reload()  # Load all logs\n",
        "\n",
        "# List all scalar keys available in the logs\n",
        "scalar_keys = ea.Tags()['scalars']\n",
        "print(\"Available scalar keys:\", scalar_keys)\n",
        "\n",
        "# Extract training and validation losses\n",
        "train_loss = ea.Scalars('train/lm_loss')  # Adjust for actual name if necessary\n",
        "val_loss = ea.Scalars('validation/lm_loss')  # Adjust for actual name if necessary\n",
        "\n",
        "# Convert to lists for plotting\n",
        "train_loss_values = [x.value for x in train_loss]\n",
        "val_loss_values = [x.value for x in val_loss]\n",
        "\n",
        "# Find the lengths of both arrays\n",
        "len_train = len(train_loss_values)\n",
        "len_val = len(val_loss_values)\n",
        "\n",
        "iterations = None\n",
        "# Interpolate the shorter array\n",
        "if len_train != len_val:\n",
        "    if len_train > len_val:\n",
        "        # Interpolate validation loss to match the training loss length\n",
        "        iterations = np.linspace(1, len_train, len_train)\n",
        "        val_iterations = np.linspace(1, len_train, len_val)\n",
        "        val_loss_values = np.interp(iterations, val_iterations, val_loss_values)\n",
        "    else:\n",
        "        # Interpolate training loss to match the validation loss length\n",
        "        iterations = np.linspace(1, len_val, len_val)\n",
        "        train_iterations = np.linspace(1, len_val, len_train)\n",
        "        train_loss_values = np.interp(iterations, train_iterations, train_loss_values)\n",
        "else:\n",
        "    iterations = range(1, len_train + 1)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(iterations, train_loss_values, label='Training Loss')\n",
        "plt.plot(iterations, val_loss_values, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VpdD5ZzfGTf"
      },
      "outputs": [],
      "source": [
        "# If we want to monitor the logs\n",
        "#!tail -f /content/gpt-neox/logs/*stdout.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk8DhmmEZFyz"
      },
      "source": [
        "# Inference with GPT-NeoX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKb0Ar6NZFyz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# This has issues if used during training -  The server socket has failed to bind to [::]:29500 (errno: 98 - Address already\n",
        "# This will write over the logs\n",
        "!source /content/my_env/bin/activate && python ./deepy.py generate.py -d configs /content/GPT-NeoX-Colab/configs/shakespeare.yml /content/GPT-NeoX-Colab/configs/shakespeare_gen.yml > /dev/null\n",
        "!cat sample_output.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7AOL1NJ43e"
      },
      "source": [
        "# Inference with Hugging Face\n",
        "\n",
        "## Convert Our Model to HuggingFace Format\n",
        "Here we are converting our model to `HuggingFace Format`. Follow the below instruction:\n",
        "1. Add the `tokenizer_type` parameter to your .yml file:\n",
        "(Example `\"tokenizer_type\": \"CharLevelTokenizer\",`)\n",
        "In my use case I ll add above parameter to my `shakespeare.yml` file.\n",
        "2. Select your best model from `checkpoints` directory. In my use case I select `global_step400`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNdWrrkWfyT-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the checkpoints directory\n",
        "checkpoints_dir = \"/content/gpt-neox/checkpoints\"\n",
        "\n",
        "# Read the 'latest' file to get the latest checkpoint name\n",
        "with open(os.path.join(checkpoints_dir, \"latest\"), \"r\") as f:\n",
        "    latest_checkpoint_name = f.read().strip()\n",
        "\n",
        "# Construct the full path to the latest checkpoint directory\n",
        "latest_checkpoint_path = os.path.join(checkpoints_dir, latest_checkpoint_name)\n",
        "print(\"Path to the latest checkpoint:\", latest_checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rplkNclPXgkH"
      },
      "outputs": [],
      "source": [
        "!source /content/my_env/bin/activate && python ./tools/ckpts/convert_neox_to_hf.py --input_dir {latest_checkpoint_path} --config_file /content/GPT-NeoX-Colab/configs/shakespeare.yml --output_dir hf_model/save/location --precision auto --architecture neox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qra1qQtC2oiI"
      },
      "source": [
        "## Generating Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01ZRN2IceM8a"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoXForCausalLM\n",
        "import torch\n",
        "\n",
        "# Move to model directory\n",
        "%cd /content/gpt-neox\n",
        "\n",
        "# Assuming CharLevelTokenizer is properly imported and instantiated\n",
        "from GPTNeoXColab import CharLevelTokenizer\n",
        "tokenizer = CharLevelTokenizer()\n",
        "\n",
        "# Load your model\n",
        "model_path = \"/content/gpt-neox/hf_model/save/location\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define a simple char-level tokenizer if not provided\n",
        "def char_level_tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def char_level_detokenize(tokens):\n",
        "    return tokenizer.detokenize(tokens)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user for input\n",
        "#input_text = input(\"Enter your prompt: \")\n",
        "input_text = \"Thou art\"\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([char_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = char_level_detokenize(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5ejxD--8z99"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk5KFdtSUZ3f"
      },
      "outputs": [],
      "source": [
        "#@title Storing Checkpoints in Google Drive\n",
        "if SAVE_TO_DRIVE:\n",
        "\n",
        "  import shutil\n",
        "  import os\n",
        "\n",
        "  # Define source and destination base paths\n",
        "  source_folder = '/content/gpt-neox/checkpoints'\n",
        "  destination_folder = '/content/drive/MyDrive/gpt-neox-checkpoints'\n",
        "\n",
        "  # Function to add version suffix if destination folder exists\n",
        "  def get_versioned_folder_path(base_path):\n",
        "      version = 1\n",
        "      new_path = base_path\n",
        "      while os.path.exists(new_path):\n",
        "          new_path = f\"{base_path}_v{version}\"\n",
        "          version += 1\n",
        "      return new_path\n",
        "\n",
        "  # Get the versioned destination folder path\n",
        "  destination_folder_versioned = get_versioned_folder_path(destination_folder)\n",
        "\n",
        "  # Copy the folder\n",
        "  shutil.copytree(source_folder, destination_folder_versioned)\n",
        "\n",
        "  print(f\"Folder copied successfully to Google Drive as '{destination_folder_versioned}'!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
