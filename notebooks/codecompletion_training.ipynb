{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/codecompletion_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Modify torch and transformers which requires manual notebook restart\n",
        "# Could not redirect to /dev/null in the standard Colab notebook (maybe no output for a particular time?)\n",
        "# Currently deepspeed from GTP-NeoX is not compatible with logging in torch >= 2.4\n",
        "%pip install --use-feature=fast-deps -q torch==2.3 &\n",
        "%pip install --use-feature=fast-deps -q torchaudio==2.3.0 &\n",
        "%pip install --use-feature=fast-deps -q torchvision==0.18.0 &\n",
        "%pip install --use-feature=fast-deps -q transformers==4.38.0 &\n",
        "%pip install --use-feature=fast-deps -q sentence-transformers==2.2.2 &"
      ],
      "metadata": {
        "id": "ygioWffQoa-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install -y sysbench\n",
        "# The Colab vCPU should give around 200 events per second. The high RAM instance type is around 350 (at twice the price)\n",
        "# Given the price of T4 GPU and the percentage of time we spendin in setup is it worth using a high RAM instance\n",
        "# The high RAM instance has 8 threads instead of 2 and can reach around 1600 events per second with all 8 threads\n",
        "!sysbench cpu --cpu-max-prime=20000 run"
      ],
      "metadata": {
        "id": "eLsDz2dWo6bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "workspaceDir = \"/content\"\n",
        "GPTNeoXDirName = \"gpt-neox\"\n",
        "GPTNeoXDir = f\"{workspaceDir}/{GPTNeoXDirName}\"\n",
        "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
        "GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
      ],
      "metadata": {
        "id": "8tGrS9KJu7QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpgI19mPrtvy"
      },
      "source": [
        "# Clone CodeXGLUE Repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/CodeXGLUE.git &"
      ],
      "metadata": {
        "id": "0qBcQotWw1U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOySwjeyktsH"
      },
      "outputs": [],
      "source": [
        "#@title Clone GPT-NeoX-Colab\n",
        "%%time\n",
        "%cd {workspaceDir}\n",
        "# Don't use --depth 1 because that does not play nice with git-annex\n",
        "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "%cd {GPTNeoXColabDir}\n",
        "#%pip install --use-feature=fast-deps -q -r requirements_colab.txt\n",
        "!cat requirements_colab.txt | xargs -n 1 -P 8 pip install --use-feature=fast-deps -q\n",
        "%pip install --use-feature=fast-deps -q .\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
        "import GPTNeoXColab\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/codecompletion/processed_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OguN3qpTQAQs"
      },
      "outputs": [],
      "source": [
        "%cd {workspaceDir}\n",
        "#!git clone https://github.com/EleutherAI/gpt-neox.git\n",
        "!git clone -b pipe_parallel_size_1 --depth 1 https://github.com/markNZed/gpt-neox.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m7so6NvF6J5"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/gpt-neox/processed_data\n",
        "!cp {GPTNeoXColabDir}/data/codecompletion/processed_data/* /content/gpt-neox/processed_data\n",
        "processed_data_path = \"/content/gpt-neox/processed_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0FMSSnXzW0"
      },
      "source": [
        "# Cloning GPT-NeoX Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UV1kBtFXkKo"
      },
      "source": [
        "# Downloading and Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn0Wy7g7lGGe",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(processed_data_path):\n",
        "    # Change directory\n",
        "    %cd /content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150\n",
        "    # Run the shell script to download and extract\n",
        "    !bash /content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150/download_and_extract.sh\n",
        "    # Run the preprocessing Python script\n",
        "    !python preprocess.py --base_dir=py150_files --output_dir=token_completion\n",
        "else:\n",
        "    print(\"File already exists, skipping download and preprocessing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWTuy456X7Yl"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH7iL-O-Qq3A",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%cd /content/gpt-neox\n",
        "%pip install --use-feature=fast-deps -q -r ./requirements/requirements.txt\n",
        "#!cat ./requirements/requirements.txt | xargs -n 1 -P 8 pip install --use-feature=fast-deps -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwrfbvLpYXAC"
      },
      "source": [
        "# Preparing Custom Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDlSqS1iyU8k"
      },
      "outputs": [],
      "source": [
        "%cd /content/gpt-neox\n",
        "!mkdir -p data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StUBNuLhHUPm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Generate a list of dictionaries\n",
        "if not os.path.exists(processed_data_path):\n",
        "  lines = []\n",
        "  with open(\"/content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150/token_completion/train.txt\", encoding=\"utf8\") as f:\n",
        "      for line in f.read().splitlines():\n",
        "          if line:\n",
        "              lines.append({\"text\": line})\n",
        "\n",
        "  # Convert to a list of JSON strings\n",
        "  json_lines = [json.dumps(l) for l in lines]\n",
        "\n",
        "  # Join lines and save to .jsonl file\n",
        "  json_data = '\\n'.join(json_lines)\n",
        "  with open('/content/gpt-neox/data/py95K_train.jsonl', 'w') as f:\n",
        "      f.write(json_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ7f8hTqipgF"
      },
      "source": [
        "# Using Byte-Pair Encoding Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmOgA5alzT2A"
      },
      "outputs": [],
      "source": [
        "%cd data\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json &\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Xh49c3aQ1WW"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import os\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(processed_data_path):\n",
        "  %cd /content/gpt-neox\n",
        "  !mkdir -p processed_data\n",
        "  !python tools/datasets/preprocess_data.py \\\n",
        "    --input ./data/py95K_train.jsonl \\\n",
        "    --vocab ./data/gpt2-vocab.json \\\n",
        "    --merge-file ./data/gpt2-merges.txt \\\n",
        "    --output-prefix ./processed_data/py150 \\\n",
        "    --tokenizer-type GPT2BPETokenizer \\\n",
        "    --dataset-impl mmap \\\n",
        "    --append-eod\n",
        "else:\n",
        "    print(\"File already exists, skipping download and preprocessing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2bv8UUYqRBp"
      },
      "source": [
        "# Tokens count in Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJzBk_-2etia"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Initialize the GPT-2 tokenizer (BPE-based)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Path to your text file\n",
        "file_path = \"/content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150/token_completion/train.txt\"\n",
        "\n",
        "# Initialize a token counter\n",
        "total_token_count = 0\n",
        "\n",
        "# Open the file and read line by line to count tokens\n",
        "#with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "#    for line in file:\n",
        "#        tokens = tokenizer.encode(line)\n",
        "#        total_token_count += len(tokens)\n",
        "        #print(total_token_count)\n",
        "\n",
        "print(f\"Total token count: {total_token_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miolaIFSa8sS"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_t7Dpd3Wn0Y"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQknkkSEbMXa"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%cd /content/gpt-neox\n",
        "!nohup python ./deepy.py train.py --conf_dir /content/GPT-NeoX-Colab/configs codecompletion codecompletion_train &"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {GPTNeoXDir}\n",
        "%tensorboard --logdir tensorboard"
      ],
      "metadata": {
        "id": "-dVOPGKMXH75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsoKIdFh99O_"
      },
      "outputs": [],
      "source": [
        "!pip show datasets\n",
        "!pip install datasets==1.18.0\n",
        "!pip install hf-transfer\n",
        "!pip install lm-eval --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3Wp-zQC-EXW"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYKP6ya8Iqej"
      },
      "outputs": [],
      "source": [
        "%cd /content/gpt-neox\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "import os\n",
        "import numpy as np\n",
        "# Path to the latest log file\n",
        "log_dir = \"tensorboard\"\n",
        "log_files = [os.path.join(log_dir, d) for d in os.listdir(log_dir)]\n",
        "latest_log_dir = max(log_files, key=os.path.getmtime)\n",
        "\n",
        "# Initialize EventAccumulator to load scalar data\n",
        "ea = event_accumulator.EventAccumulator(latest_log_dir)\n",
        "ea.Reload()  # Load all logs\n",
        "\n",
        "# List all scalar keys available in the logs\n",
        "scalar_keys = ea.Tags()['scalars']\n",
        "print(\"Available scalar keys:\", scalar_keys)\n",
        "\n",
        "# Extract training and validation losses\n",
        "train_loss = ea.Scalars('train/lm_loss')  # Adjust for actual name if necessary\n",
        "val_loss = ea.Scalars('validation/lm_loss')  # Adjust for actual name if necessary\n",
        "\n",
        "# Convert to lists for plotting\n",
        "train_loss_values = [x.value for x in train_loss]\n",
        "val_loss_values = [x.value for x in val_loss]\n",
        "\n",
        "# Find the lengths of both arrays\n",
        "len_train = len(train_loss_values)\n",
        "len_val = len(val_loss_values)\n",
        "\n",
        "iterations = None\n",
        "# Interpolate the shorter array\n",
        "if len_train != len_val:\n",
        "    if len_train > len_val:\n",
        "        # Interpolate validation loss to match the training loss length\n",
        "        iterations = np.linspace(1, len_train, len_train)\n",
        "        val_iterations = np.linspace(1, len_train, len_val)\n",
        "        val_loss_values = np.interp(iterations, val_iterations, val_loss_values)\n",
        "    else:\n",
        "        # Interpolate training loss to match the validation loss length\n",
        "        iterations = np.linspace(1, len_val, len_val)\n",
        "        train_iterations = np.linspace(1, len_val, len_train)\n",
        "        train_loss_values = np.interp(iterations, train_iterations, train_loss_values)\n",
        "else:\n",
        "    iterations = range(1, len_train + 1)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(iterations, train_loss_values, label='Training Loss')\n",
        "plt.plot(iterations, val_loss_values, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNB4sSsS-RN3"
      },
      "source": [
        "# HuggingFace Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IOlAoGw-jZ7"
      },
      "source": [
        "# Convert Our Model to HuggingFace Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfKL-k5k-DQj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the checkpoints directory\n",
        "checkpoints_dir = \"/content/gpt-neox/checkpoints\"\n",
        "\n",
        "# Read the 'latest' file to get the latest checkpoint name\n",
        "with open(os.path.join(checkpoints_dir, \"latest\"), \"r\") as f:\n",
        "    latest_checkpoint_name = f.read().strip()\n",
        "\n",
        "# Construct the full path to the latest checkpoint directory\n",
        "latest_checkpoint_path = os.path.join(checkpoints_dir, latest_checkpoint_name)\n",
        "print(\"Path to the latest checkpoint:\", latest_checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPbSUBro-25L"
      },
      "outputs": [],
      "source": [
        "!python ./tools/ckpts/convert_neox_to_hf.py --input_dir {latest_checkpoint_path} --config_file /content/GPT-NeoX-Colab/configs/codecompletion.yml --output_dir hf_model/save/location --precision auto --architecture neox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVuBSMByImpm"
      },
      "source": [
        "# Code Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztr-ItKf_G1M"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoXForCausalLM\n",
        "import torch\n",
        "\n",
        "# Move to model directory\n",
        "%cd /content/gpt-neox\n",
        "\n",
        "# Assuming CharLevelTokenizer is properly imported and instantiated\n",
        "from megatron.tokenizer.tokenizer import _GPT2BPETokenizer\n",
        "tokenizer = _GPT2BPETokenizer(vocab_file=\"data/gpt2-vocab.json\", merge_file=\"data/gpt2-merges.txt\")\n",
        "\n",
        "# Load your model\n",
        "model_path = \"/content/gpt-neox/hf_model/save/location\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define a simple char-level tokenizer if not provided\n",
        "def token_level_tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def token_level_detokenize(tokens):\n",
        "    return tokenizer.detokenize(tokens)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user for input\n",
        "input_text = \"\"\"<s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django . utils . importlib import import_module <EOL> from django . core . management import call_command <EOL> from django . core . management import BaseCommand <EOL> from django . db import connections <EOL> def import_app ( app_label , verbosity ) : <EOL> try : <EOL> app_path = __import__ ( app_label , { } , { } , [ app_label . split ( '<STR_LIT:.>' ) [ - <NUM_LIT:1> ] ] ) . __path__ <EOL>\"\"\"\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([token_level_tokenize(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = token_level_detokenize(output[0].tolist())\n",
        "\n",
        "# Function to replace special tokens with original representations\n",
        "def replace_special_tokens(text):\n",
        "    replacements = {\n",
        "        \"<EOL>\": \"\\n\",  # Replace with actual newline\n",
        "        \"<s>\": \"\",\n",
        "        \"</s>\": \"\",     # Remove end token\n",
        "        \"<STR_LIT>\": \"STR_LITERAL\",  # Example replacement, adjust as necessary\n",
        "        \"<NUM_LIT>\": \"NUM_LITERAL\",   # Example replacement, adjust as necessary\n",
        "    }\n",
        "\n",
        "    for token, replacement in replacements.items():\n",
        "        text = text.replace(token, replacement)\n",
        "\n",
        "    return text.strip()  # Strip leading/trailing whitespace\n",
        "\n",
        "# Replace special tokens in the generated text\n",
        "final_text = replace_special_tokens(generated_text)\n",
        "\n",
        "# Print the final output\n",
        "print(\"Generated text:\", final_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(\"/content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150/token_completion\"):\n",
        "    # Change directory\n",
        "    %cd /content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150\n",
        "    # Run the shell script to download and extract\n",
        "    !bash /content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150/download_and_extract.sh\n",
        "    # Run the preprocessing Python script\n",
        "    !python preprocess.py --base_dir=py150_files --output_dir=token_completion\n",
        "else:\n",
        "    print(\"File already exists, skipping download and preprocessing.\")"
      ],
      "metadata": {
        "id": "I_K4YdYfv2V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmLwXfGIc8uJ"
      },
      "outputs": [],
      "source": [
        "!cp /content/gpt-neox/data/gpt2-vocab.json /content/gpt-neox/hf_model/save/location/vocab.json\n",
        "!cp /content/gpt-neox/data/gpt2-merges.txt /content/gpt-neox/hf_model/save/location/merges.txt\n",
        "%cd /content/CodeXGLUE/Code-Code/CodeCompletion-token/code\n",
        "!python -u run_lm.py \\\n",
        "        --data_dir=../dataset/py150/token_completion \\\n",
        "        --lit_file=../dataset/py150/literals.json \\\n",
        "        --langs=$LANG \\\n",
        "        --output_dir=../dataset/py150 \\\n",
        "        --pretrain_dir=/content/gpt-neox/hf_model/save/location \\\n",
        "        --log_file=../completion_python_eval.log \\\n",
        "        --model_type=gpt2 \\\n",
        "        --block_size=2048 \\\n",
        "        --do_eval \\\n",
        "        --per_gpu_eval_batch_size=4 \\\n",
        "        --logging_steps=100 \\\n",
        "        --seed=42"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using git on VM\n",
        "- Create PAT on GitHub with content permission for repo\n",
        "- To store the details on first entry: `git config --global credential.helper store`\n",
        "- To check password and store: `git push --dry-run`\n",
        "# Using DVC\n",
        "- `export AWS_SECRET_ACCESS_KEY=xxxx`\n",
        "- `export AWS_ACCESS_KEY_ID=xxx`\n",
        "- `mkdir -p /content/GPT-NeoX-Colab/models/codecompletion`\n",
        "- `cd /content/GPT-NeoX-Colab/models/codecompletion`\n",
        "- `tar -cf global_step7000.tar -C /content/gpt-neox/checkpoints global_step7000`\n",
        "- `gzip global_step7000.tar`\n",
        "- `dvc add global_step7000.tar.gz`\n",
        "- `cd ../..`\n",
        "- `git add .`\n",
        "- `git commit -m\"add to dvc\"`\n",
        "- `git push`\n",
        "- `dvc  push`\n",
        "\n"
      ],
      "metadata": {
        "id": "ULDVX1xFmpCj"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}