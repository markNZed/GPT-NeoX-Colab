{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/codecompletion_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8tGrS9KJu7QA"
   },
   "outputs": [],
   "source": [
    "# We could modify these paths to \"stub\" behavior for test/dev\n",
    "workspaceDir = \"/content\"\n",
    "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
    "GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpgI19mPrtvy"
   },
   "source": [
    "# Clone CodeXGLUE Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0qBcQotWw1U0",
    "outputId": "edc6ace3-dcac-4d4a-abea-00f798f85174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CodeXGLUE'...\n",
      "remote: Enumerating objects: 3373, done.\u001b[K\n",
      "remote: Counting objects: 100% (3372/3372), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1534/1534), done.\u001b[K\n",
      "remote: Total 3373 (delta 1746), reused 3329 (delta 1733), pack-reused 1 (from 1)\u001b[K\n",
      "Receiving objects: 100% (3373/3373), 213.15 MiB | 20.36 MiB/s, done.\n",
      "Resolving deltas: 100% (1746/1746), done.\n",
      "Updating files: 100% (400/400), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/microsoft/CodeXGLUE.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOySwjeyktsH",
    "outputId": "49cd1723-9596-4ef8-dbd3-dd4114ffe745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'GPT-NeoX-Colab'...\n",
      "remote: Enumerating objects: 1348, done.\u001b[K\n",
      "remote: Counting objects: 100% (296/296), done.\u001b[K\n",
      "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
      "remote: Total 1348 (delta 181), reused 285 (delta 179), pack-reused 1052 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1348/1348), 15.06 MiB | 32.33 MiB/s, done.\n",
      "Resolving deltas: 100% (758/758), done.\n",
      "/content/GPT-NeoX-Colab\n",
      "\u001b[33mWARNING: pip is using lazily downloaded wheels using HTTP range requests to obtain dependency information. This experimental feature is enabled through --use-feature=fast-deps and it is not ready for production.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: pip is using lazily downloaded wheels using HTTP range requests to obtain dependency information. This experimental feature is enabled through --use-feature=fast-deps and it is not ready for production.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: pip is using lazily downloaded wheels using HTTP range requests to obtain dependency information. This experimental feature is enabled through --use-feature=fast-deps and it is not ready for production.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: pip is using lazily downloaded wheels using HTTP range requests to obtain dependency information. This experimental feature is enabled through --use-feature=fast-deps and it is not ready for production.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: pip is using lazily downloaded wheels using HTTP range requests to obtain dependency information. This experimental feature is enabled through --use-feature=fast-deps and it is not ready for production.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: pip is using lazily downloaded wheels using HTTP range requests to obtain dependency information. This experimental feature is enabled through --use-feature=fast-deps and it is not ready for production.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: pip is using lazily downloaded wheels using HTTP range requests to obtain dependency information. This experimental feature is enabled through --use-feature=fast-deps and it is not ready for production.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.4/901.4 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.7/548.7 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.1/569.1 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.1/113.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pysftp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.0/426.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.5/456.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.6/367.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.2/722.2 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for configobj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33mWARNING: pip is using lazily downloaded wheels using HTTP range requests to obtain dependency information. This experimental feature is enabled through --use-feature=fast-deps and it is not ready for production.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for GPTNeoXColab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Data retrieval successful.\n",
      "Data retrieval successful.\n",
      "/content/GPT-NeoX-Colab/models\n",
      "CPU times: user 11 s, sys: 1.18 s, total: 12.2 s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "#@title Clone GPT-NeoX-Colab\n",
    "%%time\n",
    "%cd {workspaceDir}\n",
    "# Don't use --depth 1 because that does not play nice with git-annex\n",
    "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
    "%cd {GPTNeoXColabDir}\n",
    "#%pip install --use-feature=fast-deps -q -r requirements_colab.txt\n",
    "!cat requirements_colab.txt | xargs -n 1 -P 8 pip install --use-feature=fast-deps -q\n",
    "%pip install --use-feature=fast-deps -q .\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
    "import GPTNeoXColab\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"data/codecompletion/token_completion.tar.gz\")\n",
    "#GPTNeoXColab.utils.colab.fetch_data(\"models/codecompletion.tar.gz\")\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"models/codecompletion/global_step7000_HF.tar.gz\")\n",
    "#%cd {GPTNeoXColabDir}/data/codecompletion\n",
    "#!tar -xzf token_completion.tar.gz\n",
    "%cd {GPTNeoXColabDir}/models\n",
    "!tar -xzf global_step7000_HF.tar.gz\n",
    "!mv global_step7000_HF latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ7f8hTqipgF"
   },
   "source": [
    "# Using Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmOgA5alzT2A",
    "outputId": "b5138fa4-3994-4971-8b1c-ccbe69c6e79a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "--2024-11-14 09:27:03--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.234.112, 52.216.50.144, 52.217.204.16, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.234.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: ‘gpt2-vocab.json’\n",
      "\n",
      "gpt2-vocab.json     100%[===================>]   1018K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-11-14 09:27:03 (10.4 MB/s) - ‘gpt2-vocab.json’ saved [1042301/1042301]\n",
      "\n",
      "--2024-11-14 09:27:03--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.234.112, 52.216.50.144, 52.217.204.16, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.234.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [text/plain]\n",
      "Saving to: ‘gpt2-merges.txt’\n",
      "\n",
      "gpt2-merges.txt     100%[===================>] 445.62K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2024-11-14 09:27:03 (4.63 MB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd {workspaceDir}\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json &\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsoKIdFh99O_"
   },
   "outputs": [],
   "source": [
    "#!pip show datasets\n",
    "#!pip install datasets==1.18.0\n",
    "#!pip install hf-transfer\n",
    "#!pip install lm-eval --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNB4sSsS-RN3"
   },
   "source": [
    "# HuggingFace Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztr-ItKf_G1M",
    "outputId": "cf7f1c2d-b1ad-4072-8b2a-66a00d2f380b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Generated text: import sys , os \n",
      " import imp \n",
      " from optparse import make_option \n",
      " from django . conf import settings \n",
      " from django (' , : ) , \n",
      "' ) \n",
      "STR_LITERALEOL>' , \n",
      "STR_LITERAL\" NUM_LITERAL self .' 'STR_LITERAL' <NUM_LIT:OL>' : \n",
      "STR_LITERALEOL> def' , \"<STR_LIT:' : \n",
      "' ) , : ) \n",
      "' \n",
      "' , \n",
      " self , '<STR_LIT:_LIT>' \n",
      " : \n",
      "' \n",
      "' \n",
      "NUM_LIT:OL>' )\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "%cd {workspaceDir}\n",
    "\n",
    "# Initialize the tokenizer with your vocabulary and merge files\n",
    "tokenizer = GPT2Tokenizer(vocab_file=f\"{workspaceDir}/gpt2-vocab.json\", merges_file=f\"{workspaceDir}/gpt2-merges.txt\")\n",
    "\n",
    "# Load your model\n",
    "model_path = f\"{GPTNeoXColabDir}/models/codecompletion/latest\"\n",
    "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Prompt the user for input\n",
    "input_text = \"\"\"<s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django\"\"\"\n",
    "\n",
    "# Tokenize and prepare input\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
    "\n",
    "# Generate text with specified pad_token_id and attention_mask\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=200,          # Adjust this for desired output length\n",
    "        temperature=0.7,        # Controls creativity\n",
    "        top_k=50,               # Controls diversity\n",
    "        top_p=0.9,              # Nucleus sampling\n",
    "        num_return_sequences=1, # Number of sequences to return\n",
    "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
    "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0].tolist())\n",
    "\n",
    "# Function to replace special tokens with original representations\n",
    "def replace_special_tokens(text):\n",
    "    replacements = {\n",
    "        \"<EOL>\": \"\\n\",  # Replace with actual newline\n",
    "        \"<s>\": \"\",\n",
    "        \"</s>\": \"\",     # Remove end token\n",
    "        \"<STR_LIT>\": \"STR_LITERAL\",  # Example replacement, adjust as necessary\n",
    "        \"<NUM_LIT>\": \"NUM_LITERAL\",   # Example replacement, adjust as necessary\n",
    "    }\n",
    "\n",
    "    for token, replacement in replacements.items():\n",
    "        text = text.replace(token, replacement)\n",
    "\n",
    "    return text.strip()  # Strip leading/trailing whitespace\n",
    "\n",
    "# Replace special tokens in the generated text\n",
    "final_text = replace_special_tokens(generated_text)\n",
    "\n",
    "# Print the final output\n",
    "print(\"Generated text:\", final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2P97thGi_Qab",
    "outputId": "29ae68d9-a6c1-4247-cccf-8aa0bcdd1c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150\n"
     ]
    }
   ],
   "source": [
    "!cp {workspaceDir}/gpt2-vocab.json {GPTNeoXColabDir}/models/codecompletion/vocab.json\n",
    "!cp {workspaceDir}/gpt2-merges.txt {GPTNeoXColabDir}/models/codecompletion/merges.txt\n",
    "!cp {GPTNeoXColabDir}/data/codecompletion/token_completion.tar.gz {workspaceDir}/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150\n",
    "%cd {workspaceDir}/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150\n",
    "!tar -xzf token_completion.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmLwXfGIc8uJ",
    "outputId": "1bb2078f-3e7f-4692-c741-dcde7338f3ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/CodeXGLUE/Code-Code/CodeCompletion-token/code\n",
      "config.json  generation_config.json  merges.txt  model.safetensors  vocab.json\n",
      "2024-11-14 09:54:19.241482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-14 09:54:19.261285: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-14 09:54:19.266933: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-14 09:54:20.353954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "11/14/2024 09:54:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False, world size: 1\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizerFast'. \n",
      "The class this function is called from is 'GPT2Tokenizer'.\n",
      "You are using a model of type gpt_neox to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /content/GPT-NeoX-Colab/models/codecompletion and are newly initialized: ['h.0.attn.c_attn.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_proj.bias', 'h.0.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.0.ln_1.weight', 'h.0.ln_2.bias', 'h.0.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_attn.weight', 'h.1.attn.c_proj.bias', 'h.1.attn.c_proj.weight', 'h.1.ln_1.bias', 'h.1.ln_1.weight', 'h.1.ln_2.bias', 'h.1.ln_2.weight', 'h.1.mlp.c_fc.bias', 'h.1.mlp.c_fc.weight', 'h.1.mlp.c_proj.bias', 'h.1.mlp.c_proj.weight', 'h.2.attn.c_attn.bias', 'h.2.attn.c_attn.weight', 'h.2.attn.c_proj.bias', 'h.2.attn.c_proj.weight', 'h.2.ln_1.bias', 'h.2.ln_1.weight', 'h.2.ln_2.bias', 'h.2.ln_2.weight', 'h.2.mlp.c_fc.bias', 'h.2.mlp.c_fc.weight', 'h.2.mlp.c_proj.bias', 'h.2.mlp.c_proj.weight', 'h.3.attn.c_attn.bias', 'h.3.attn.c_attn.weight', 'h.3.attn.c_proj.bias', 'h.3.attn.c_proj.weight', 'h.3.ln_1.bias', 'h.3.ln_1.weight', 'h.3.ln_2.bias', 'h.3.ln_2.weight', 'h.3.mlp.c_fc.bias', 'h.3.mlp.c_fc.weight', 'h.3.mlp.c_proj.bias', 'h.3.mlp.c_proj.weight', 'h.4.attn.c_attn.bias', 'h.4.attn.c_attn.weight', 'h.4.attn.c_proj.bias', 'h.4.attn.c_proj.weight', 'h.4.ln_1.bias', 'h.4.ln_1.weight', 'h.4.ln_2.bias', 'h.4.ln_2.weight', 'h.4.mlp.c_fc.bias', 'h.4.mlp.c_fc.weight', 'h.4.mlp.c_proj.bias', 'h.4.mlp.c_proj.weight', 'h.5.attn.c_attn.bias', 'h.5.attn.c_attn.weight', 'h.5.attn.c_proj.bias', 'h.5.attn.c_proj.weight', 'h.5.ln_1.bias', 'h.5.ln_1.weight', 'h.5.ln_2.bias', 'h.5.ln_2.weight', 'h.5.mlp.c_fc.bias', 'h.5.mlp.c_fc.weight', 'h.5.mlp.c_proj.bias', 'h.5.mlp.c_proj.weight', 'lm_head.weight', 'ln_f.bias', 'ln_f.weight', 'wpe.weight', 'wte.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "11/14/2024 09:54:22 - INFO - __main__ -   Model has a total of 45817344 trainable parameters\n",
      "11/14/2024 09:54:22 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='../dataset/py150/token_completion', langs='en_US.UTF-8', output_dir='../dataset/py150', model_type='gpt2', pretrain_dir='/content/GPT-NeoX-Colab/models/codecompletion', config_dir=None, tokenizer_dir=None, lit_file='../dataset/py150/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=2048, do_train=False, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=4, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=5000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=42, not_pretrain=False, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=-1, server_ip='', server_port='', log_file='../completion_python_eval.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)\n",
      "11/14/2024 09:54:23 - INFO - __main__ -   Data size: 50000\n",
      "11/14/2024 09:54:23 - WARNING - __main__ -   load 0\n"
     ]
    }
   ],
   "source": [
    "%cd {workspaceDir}/CodeXGLUE/Code-Code/CodeCompletion-token/code\n",
    "!ls {GPTNeoXColabDir}/models/codecompletion\n",
    "!python -u run_lm.py \\\n",
    "        --data_dir=../dataset/py150/token_completion \\\n",
    "        --lit_file=../dataset/py150/literals.json \\\n",
    "        --langs=$LANG \\\n",
    "        --output_dir=../dataset/py150 \\\n",
    "        --pretrain_dir=/content/GPT-NeoX-Colab/models/codecompletion \\\n",
    "        --log_file=../completion_python_eval.log \\\n",
    "        --model_type=gpt2 \\\n",
    "        --block_size=2048 \\\n",
    "        --do_eval \\\n",
    "        --per_gpu_eval_batch_size=4 \\\n",
    "        --logging_steps=100 \\\n",
    "        --seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kCsk5nkETae",
    "outputId": "28e325c4-75ec-4c49-9bdc-e7555b3e0a61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  generation_config.json  merges.txt  model.safetensors  vocab.json\n"
     ]
    }
   ],
   "source": [
    "ls {GPTNeoXColabDir}/models/codecompletion"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
