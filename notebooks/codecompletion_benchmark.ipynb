{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/codecompletion_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8tGrS9KJu7QA"
      },
      "outputs": [],
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "DOCKER = False\n",
        "workspaceDir = \"/content\"\n",
        "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
        "if DOCKER:\n",
        "    GPTNeoXColabDir = f\"/workspace\"\n",
        "else:\n",
        "    GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpgI19mPrtvy"
      },
      "source": [
        "# Clone CodeXGLUE Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0qBcQotWw1U0"
      },
      "outputs": [],
      "source": [
        "# Not using this at but for a final sanity check we should use the data and evaluate.py from here\n",
        "#%cd {workspaceDir}\n",
        "#!git clone --depth 1 https://github.com/microsoft/CodeXGLUE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOySwjeyktsH",
        "outputId": "ed85ff10-8381-4ff0-928d-b336b10f9a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'GPT-NeoX-Colab' already exists and is not an empty directory.\n",
            "/content/GPT-NeoX-Colab\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.9/251.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.0/426.0 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.5/456.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.6/367.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.4/901.4 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.7/548.7 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.4/571.4 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.1/113.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.2/722.2 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for configobj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pysftp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for GPTNeoXColab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "CPU times: user 998 ms, sys: 181 ms, total: 1.18 s\n",
            "Wall time: 2min 42s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX-Colab\n",
        "if DOCKER:\n",
        "    %cd {GPTNeoXColabDir}\n",
        "else:\n",
        "    %cd {workspaceDir}\n",
        "    # Don't use --depth 1 because that does not play nice with git-annex\n",
        "    !git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "    %cd {GPTNeoXColabDir}\n",
        "    %pip install -q -r requirements_colab.txt\n",
        "    %pip install -q ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {GPTNeoXColabDir}\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
        "import GPTNeoXColab\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/codecompletion/token_completion.tar.gz\")\n",
        "%cd {GPTNeoXColabDir}/data/codecompletion\n",
        "if not os.path.exists(f\"data/codecompletion/token_completion\"):\n",
        "    !tar -xzf token_completion.tar.gz\n",
        "%cd {GPTNeoXColabDir}\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"models/codecompletion/global_step7000_HF.tar.gz\")\n",
        "%cd {GPTNeoXColabDir}/models/codecompletion\n",
        "if not os.path.exists(f\"latest\"):\n",
        "    !tar -xzf global_step7000_HF.tar.gz\n",
        "    !mv global_step7000_HF latest"
      ],
      "metadata": {
        "id": "yu4yRpEzCyF6",
        "outputId": "16e01391-a108-4107-939d-f621eeec5101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n",
            "Data retrieval successful.\n",
            "/content/GPT-NeoX-Colab/data/codecompletion\n",
            "/content/GPT-NeoX-Colab\n",
            "Data retrieval successful.\n",
            "/content/GPT-NeoX-Colab/models/codecompletion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ7f8hTqipgF"
      },
      "source": [
        "# Using Byte-Pair Encoding Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmOgA5alzT2A",
        "outputId": "27200673-21b0-4cfe-a913-4be9b27807f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab/models/codecompletion/latest\n",
            "--2024-11-15 09:11:56--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.53.24, 52.217.117.224, 16.182.96.152, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.53.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‘gpt2-vocab.json’\n",
            "\n",
            "gpt2-vocab.json     100%[===================>]   1018K   897KB/s    in 1.1s    \n",
            "\n",
            "2024-11-15 09:11:58 (897 KB/s) - ‘gpt2-vocab.json’ saved [1042301/1042301]\n",
            "\n",
            "--2024-11-15 09:11:58--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.53.24, 52.217.117.224, 16.182.96.152, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.53.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [text/plain]\n",
            "Saving to: ‘gpt2-merges.txt’\n",
            "\n",
            "gpt2-merges.txt     100%[===================>] 445.62K   586KB/s    in 0.8s    \n",
            "\n",
            "2024-11-15 09:12:00 (586 KB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd {GPTNeoXColabDir}/models/codecompletion/latest\n",
        "if not os.path.exists(\"gpt2-vocab.json\"):\n",
        "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
        "    !mv gpt2-vocab.json vocab.json\n",
        "if not os.path.exists(\"gpt2-merges.txt\"):\n",
        "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
        "    !mv gpt2-merges.txt merges.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNB4sSsS-RN3"
      },
      "source": [
        "# HuggingFace Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztr-ItKf_G1M",
        "outputId": "e4038014-0efa-4112-e328-7e5cc0c92b31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Generated text: <s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django . db import models <EOL> from django . db . models import Q <EOL> from django . db . models import Q <EOL> from django . db . models import Q <EOL> from django . db . models import Q <EOL> from django . db . models import Q <EOL> from django . db . models import Q <EOL> from django . db . models import Q <EOL> from django . db . models import Q <EOL> from django . db . models import Q <EOL> from django . db . models import Q <EOL> from django . db . models import Q <EOL> from django . db . models\n",
            "Final text: import sys , os \n",
            " import imp \n",
            " from optparse import make_option \n",
            " from django . conf import settings \n",
            " from django . db import models \n",
            " from django . db . models import Q \n",
            " from django . db . models import Q \n",
            " from django . db . models import Q \n",
            " from django . db . models import Q \n",
            " from django . db . models import Q \n",
            " from django . db . models import Q \n",
            " from django . db . models import Q \n",
            " from django . db . models import Q \n",
            " from django . db . models import Q \n",
            " from django . db . models import Q \n",
            " from django . db . models import Q \n",
            " from django . db . models\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "%cd {workspaceDir}\n",
        "\n",
        "# Initialize the tokenizer with your vocabulary and merge files\n",
        "tokenizer = GPT2Tokenizer(vocab_file=f\"{GPTNeoXColabDir}/models/codecompletion/latest/vocab.json\", merges_file=f\"{GPTNeoXColabDir}/models/codecompletion/latest/merges.txt\")\n",
        "\n",
        "# Load your model\n",
        "model_path = f\"{GPTNeoXColabDir}/models/codecompletion/latest\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user for input\n",
        "input_text = \"\"\"<s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django\"\"\"\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output[0].tolist())\n",
        "print(\"Generated text:\", generated_text)\n",
        "\n",
        "# Function to replace special tokens with original representations\n",
        "def replace_special_tokens(text):\n",
        "    \"\"\"\n",
        "    Replaces special tokens in the generated text with more readable or context-appropriate representations.\n",
        "    \"\"\"\n",
        "    replacements = {\n",
        "        \"<EOL>\": \"\\n\",          # Replace with actual newline for code formatting\n",
        "        \"<s>\": \"\",              # Remove start token as it's not necessary in final output\n",
        "        \"</s>\": \"\",             # Remove end token as it's not necessary in final output\n",
        "        \"<pad>\": \"\",            # Remove padding tokens\n",
        "        \"<|UNKNOWN|>\": \"[UNK]\", # Represent unknown tokens in a readable way\n",
        "        \"<STR_LIT>\": \"\\\"STRING_LITERAL\\\"\",  # Placeholder for string literals\n",
        "        \"<NUM_LIT>\": \"0\",       # Placeholder for numeric literals\n",
        "        \"<BOOL_LIT>\": \"True\",   # Placeholder for boolean literals (e.g., True/False)\n",
        "        \"<COMMENT>\": \"# COMMENT\",  # Placeholder for comments in the code\n",
        "    }\n",
        "\n",
        "    # Replace each special token in text with its corresponding value in `replacements`\n",
        "    for token, replacement in replacements.items():\n",
        "        text = text.replace(token, replacement)\n",
        "\n",
        "    return text.strip()  # Strip leading/trailing whitespace for clean output\n",
        "\n",
        "# Replace special tokens in the generated text\n",
        "final_text = replace_special_tokens(generated_text)\n",
        "\n",
        "# Print the final output\n",
        "print(\"Final text:\", final_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Hm2e4KH4BOjM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import gc\n",
        "\n",
        "class EvalDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, logger, file_type='train', seq_length=1024):\n",
        "        if not os.path.exists(args.output_dir):\n",
        "            os.makedirs(args.output_dir)\n",
        "        cached_file = os.path.join(args.output_dir, file_type+\"_blocksize_%d\"%(seq_length))\n",
        "        if os.path.exists(cached_file) and not args.overwrite_cache:\n",
        "            with open(cached_file, 'rb') as handle:\n",
        "                self.inputs = pickle.load(handle)\n",
        "\n",
        "        else:\n",
        "            self.inputs = []\n",
        "\n",
        "            datafile = os.path.join(args.data_dir, f\"{file_type}.txt\")\n",
        "            with open(datafile) as f:\n",
        "                data = f.readlines()\n",
        "\n",
        "            length = len(data)\n",
        "            logger.info(\"Data size: %d\"%(length))\n",
        "            input_ids = []\n",
        "            for idx,x in enumerate(data):\n",
        "                x = x.strip()\n",
        "                if x.startswith(\"<s>\") and x.endswith(\"</s>\"):\n",
        "                    pass\n",
        "                else:\n",
        "                    x = \"<s> \" + x + \" </s>\"\n",
        "                try:\n",
        "                    input_ids.extend(tokenizer.encode(x))\n",
        "                except Exception:\n",
        "                    pass\n",
        "                if idx % (length//10) == 0:\n",
        "                    percent = idx / (length//10) * 10\n",
        "                    logger.info(\"load %d\"%(percent))\n",
        "                if args.max_eval_length is not None and idx > args.max_eval_length:\n",
        "                    logger.info(f\"max eval length reached at {idx}\")\n",
        "                    break\n",
        "            del data\n",
        "            gc.collect()\n",
        "\n",
        "            logger.info(f\"tokens: {len(input_ids)}\")\n",
        "            self.split(input_ids, tokenizer, logger, seq_length=seq_length)\n",
        "            del input_ids\n",
        "            gc.collect()\n",
        "\n",
        "            with open(cached_file, 'wb') as handle:\n",
        "                pickle.dump(self.inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def split(self, input_ids, tokenizer, logger, seq_length=1024):\n",
        "        sample = []\n",
        "        i = 0\n",
        "        while i < len(input_ids):\n",
        "            sample = input_ids[i: i+seq_length]\n",
        "            if len(sample) == seq_length:\n",
        "                for j in range(seq_length):\n",
        "                    if tokenizer.convert_ids_to_tokens(sample[seq_length-1-j])[0] == '\\u0120' or tokenizer.convert_ids_to_tokens(sample[seq_length-1-j]).startswith(\"<NUM_LIT\"):\n",
        "                        break\n",
        "                    if sample[seq_length-1-j] in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id]:\n",
        "                        if sample[seq_length-1-j] != tokenizer.bos_token_id:\n",
        "                            j -= 1\n",
        "                        break\n",
        "                if j == seq_length-1:\n",
        "                    print(tokenizer.decode(sample))\n",
        "                    exit()\n",
        "                sample = sample[: seq_length-1-j]\n",
        "            # print(len(sample))\n",
        "            i += len(sample)\n",
        "            pad_len = seq_length-len(sample)\n",
        "            sample += [tokenizer.pad_token_id]*pad_len\n",
        "            self.inputs.append(sample)\n",
        "\n",
        "            if len(self.inputs) % 10000 == 0:\n",
        "                logger.info(f\"{len(self.inputs)} samples\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.inputs[item])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oA3pTtlIBOjQ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SequentialSampler\n",
        "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
        "from types import SimpleNamespace\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "def decode_token_ids(token_ids, tokenizer):\n",
        "    \"\"\"\n",
        "    Convert token IDs to a string of code, handling special tokens and spacing.\n",
        "    \"\"\"\n",
        "    decoded_code = \"\"\n",
        "    for token_id in token_ids:\n",
        "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "        if token.startswith('\\u0120') and not decoded_code.endswith(\" \"):  # Handles space prefixes\n",
        "            decoded_code += \" \" + token[1:]\n",
        "        else:\n",
        "            decoded_code += token\n",
        "    return decoded_code.strip()\n",
        "\n",
        "def eval_acc(args, model, tokenizer, file_type='test'):\n",
        "    \"\"\"\n",
        "    Evaluate the model’s token-level code completion accuracy.\n",
        "    \"\"\"\n",
        "    # Load evaluation dataset\n",
        "    eval_dataset = EvalDataset(tokenizer, args, logger, file_type=file_type, seq_length=args.seq_length)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=SequentialSampler(eval_dataset), batch_size=args.eval_batch_size)\n",
        "    model.to(args.device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize counters for accuracy\n",
        "    total_correct, total_predictions = 0, 0\n",
        "    total_pred_tokens, total_gt_tokens = [], []\n",
        "\n",
        "    # Iterate through batches in the evaluation dataset\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        inputs = batch.to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            predicted_token_ids = outputs.logits.argmax(-1)  # Get predicted tokens\n",
        "\n",
        "        pred_ids = predicted_token_ids.cpu()\n",
        "        gt_ids = inputs.cpu()\n",
        "\n",
        "        # Process predictions and ground truths\n",
        "        all_pred = []\n",
        "        all_gt = []\n",
        "        prev_pred = None\n",
        "        for pred_seq, gt_seq in zip(pred_ids, gt_ids):\n",
        "            pred_seq = pred_seq.tolist()\n",
        "            gt_seq = gt_seq.tolist()\n",
        "\n",
        "            now_pred = []\n",
        "            now_gt = []\n",
        "            for i, (pred_id, gt_id) in enumerate(zip(pred_seq, gt_seq)):\n",
        "                gt_token = tokenizer.convert_ids_to_tokens(gt_id)\n",
        "                pred_token = tokenizer.convert_ids_to_tokens(pred_id)\n",
        "\n",
        "                if i == 0:\n",
        "                    if gt_token in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\"]:\n",
        "                        now_gt = [gt_id]\n",
        "                        now_pred = [0] if prev_pred is None else [prev_pred]\n",
        "                        all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
        "                        all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
        "                        now_gt = []\n",
        "                        now_pred = []\n",
        "                    else:\n",
        "                        now_gt = [gt_id]\n",
        "                        now_pred = [0] if prev_pred is None else [prev_pred]\n",
        "                else:\n",
        "                    if gt_token.startswith('\\u0120'):\n",
        "                        if len(now_gt) > 0:\n",
        "                            try:\n",
        "                                all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
        "                            except IndexError:\n",
        "                                all_pred.append(\"<SPACE>\")\n",
        "                            all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
        "                            now_gt = []\n",
        "                            now_pred = []\n",
        "                    if gt_token in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\"] or gt_token.startswith(\"<NUM_LIT\"):\n",
        "                        if len(now_gt) > 0:\n",
        "                            try:\n",
        "                                all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
        "                            except IndexError:\n",
        "                                all_pred.append(\"<SPACE>\")\n",
        "                            all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
        "                        now_gt = [gt_id]\n",
        "                        now_pred = [pred_seq[i-1]]\n",
        "                        try:\n",
        "                            all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
        "                        except IndexError:\n",
        "                            all_pred.append(\"<SPACE>\")\n",
        "                        all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
        "                        now_gt = []\n",
        "                        now_pred = []\n",
        "                        continue\n",
        "                    now_gt.append(gt_id)\n",
        "                    now_pred.append(pred_seq[i-1])\n",
        "\n",
        "        assert len(all_pred) == len(all_gt)\n",
        "\n",
        "        total_pred_tokens.extend(all_pred)\n",
        "        total_gt_tokens.extend(all_gt)\n",
        "\n",
        "        # Calculate batch accuracy\n",
        "        for pred_token, gt_token in zip(all_pred, all_gt):\n",
        "            if gt_token not in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\"]:\n",
        "                total_predictions += 1\n",
        "                if pred_token == gt_token:\n",
        "                    total_correct += 1\n",
        "\n",
        "        # Logging progress\n",
        "        if step % args.logging_steps == 0:\n",
        "            accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
        "            logger.info(f\"Step {step} processed with cumulative accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    # Final accuracy calculation\n",
        "    accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
        "    logger.info(f\"Final Test Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "\n",
        "    # Call post_process to generate predictions.txt and answers.txt\n",
        "    pred_file = os.path.join(args.output_dir, \"predictions.txt\")\n",
        "    gt_file = os.path.join(args.output_dir, \"answers.txt\")\n",
        "    true_texts = open(os.path.join(args.data_dir, f\"{file_type}.txt\")).readlines()\n",
        "    total_samples = post_process(args, total_pred_tokens, total_gt_tokens, true_texts, pred_file, gt_file)\n",
        "    logger.info(f\"Evaluated on {total_samples} samples, saved predictions at {pred_file} and ground truths at {gt_file}\")\n",
        "\n",
        "\n",
        "    return total_predictions, total_correct\n",
        "\n",
        "def post_process(args, preds, gts, true_gts, pred_file_path, gt_file_path):\n",
        "    \"\"\"\n",
        "    Save the post-processed predictions and ground truths, and verify with the expected true ground truths.\n",
        "\n",
        "    Args:\n",
        "        args: General arguments or configuration settings (unused here).\n",
        "        preds: List of predicted tokens from the model.\n",
        "        gts: List of ground truth tokens for each prediction.\n",
        "        true_gts: List of full ground truth sequences for each input, used for verification.\n",
        "        pred_file_path: Path to the file where the processed predictions will be saved.\n",
        "        gt_file_path: Path to the file where the processed ground truths will be saved.\n",
        "\n",
        "    Returns:\n",
        "        int: The count of sequences processed and saved.\n",
        "    \"\"\"\n",
        "    with open(pred_file_path, \"w\") as pred_file, open(gt_file_path, \"w\") as gt_file:\n",
        "        count = 0\n",
        "        new_gt = []\n",
        "        new_pred = []\n",
        "\n",
        "        for pred, gt in zip(preds, gts):\n",
        "            if gt in [\"\", \"<pad>\"]:\n",
        "                continue\n",
        "            new_gt.append(gt)\n",
        "            new_pred.append(pred.replace(\" \", \"\"))\n",
        "\n",
        "            if gt == \"</s>\":\n",
        "                gt_str = \" \".join(new_gt)\n",
        "                pred_str = \" \".join(new_pred)\n",
        "                assert gt_str == true_gts[count].strip(), f\"Sample {count} mismatch between ground truth and expected text\"\n",
        "                pred_file.write(pred_str + \"\\n\")\n",
        "                gt_file.write(gt_str + \"\\n\")\n",
        "                count += 1\n",
        "                new_gt = []\n",
        "                new_pred = []\n",
        "\n",
        "    return count\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Reset the root logger to avoid inherited handlers\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "# Configure your specific logger\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Check if logger already has handlers; clear them if necessary\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "# Create a StreamHandler to ensure logging messages appear in the notebook\n",
        "stream_handler = logging.StreamHandler()\n",
        "stream_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Set a formatter for consistent log message formatting\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s -   %(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
        "stream_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the StreamHandler to the logger\n",
        "logger.addHandler(stream_handler)\n"
      ],
      "metadata": {
        "id": "U9QOv0YAIfyH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Single logging test: This should only appear once.\")"
      ],
      "metadata": {
        "id": "uEVXQ1m4J3qB",
        "outputId": "9add8b11-684a-4590-e406-26cfa5503636",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/15/2024 09:29:00 - INFO - __main__ -   Single logging test: This should only appear once.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model_path = f\"{GPTNeoXColabDir}/models/codecompletion/latest\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set up evaluation arguments\n",
        "args = {\n",
        "    \"n_gpu\": torch.cuda.device_count(),\n",
        "    \"per_gpu_eval_batch_size\": 1,\n",
        "    \"logging_steps\": 1,\n",
        "    \"output_dir\": f\"{GPTNeoXColabDir}/out\",\n",
        "    \"data_dir\": f\"{GPTNeoXColabDir}/data/codecompletion/token_completion\",\n",
        "    \"device\": device,\n",
        "    \"no_cuda\": False,\n",
        "    \"seq_length\": 2048,\n",
        "    \"max_eval_length\": 10,\n",
        "    \"overwrite_cache\": True,\n",
        "    \"eval_batch_size\": 1,\n",
        "    \"local_rank\": -1,\n",
        "}\n",
        "\n",
        "# Wrap args dictionary in a namespace to allow dot notation\n",
        "args = SimpleNamespace(**args)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_path, sep_token='<EOL>', bos_token='<s>', eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>')\n",
        "model = GPTNeoXForCausalLM.from_pretrained(pretrained_model_path)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "if total_params >= 1e9:\n",
        "    readable_params = f\"{total_params / 1e9:.2f}B\"  # Billions\n",
        "elif total_params >= 1e6:\n",
        "    readable_params = f\"{total_params / 1e6:.2f}M\"  # Millions\n",
        "else:\n",
        "    readable_params = f\"{total_params:,}\"  # Less than a million, use commas\n",
        "logger.info(f\"Model has {readable_params} trainable parameters\")\n",
        "\n",
        "# Evaluate model\n",
        "total_predictions, total_correct = eval_acc(args, model, tokenizer, 'test')\n",
        "accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
        "logger.info(f\"Test accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "1xmqKGg6DxUH",
        "outputId": "d57cf696-c310-4992-8256-5f79faca74c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizerFast'. \n",
            "The class this function is called from is 'GPT2Tokenizer'.\n",
            "11/15/2024 09:29:06 - INFO - __main__ -   Model has 44.65M trainable parameters\n",
            "11/15/2024 09:29:06 - INFO - __main__ -   Data size: 50000\n",
            "11/15/2024 09:29:06 - INFO - __main__ -   load 0\n",
            "11/15/2024 09:29:06 - INFO - __main__ -   max eval length reached at 11\n",
            "11/15/2024 09:29:06 - INFO - __main__ -   tokens: 23540\n",
            "11/15/2024 09:29:08 - INFO - __main__ -   Step 0 processed with cumulative accuracy: 30.84%\n",
            "11/15/2024 09:29:09 - INFO - __main__ -   Step 1 processed with cumulative accuracy: 33.42%\n",
            "11/15/2024 09:29:11 - INFO - __main__ -   Step 2 processed with cumulative accuracy: 33.31%\n",
            "11/15/2024 09:29:12 - INFO - __main__ -   Step 3 processed with cumulative accuracy: 30.62%\n",
            "11/15/2024 09:29:13 - INFO - __main__ -   Step 4 processed with cumulative accuracy: 30.73%\n",
            "11/15/2024 09:29:15 - INFO - __main__ -   Step 5 processed with cumulative accuracy: 29.80%\n",
            "11/15/2024 09:29:17 - INFO - __main__ -   Step 6 processed with cumulative accuracy: 30.04%\n",
            "11/15/2024 09:29:18 - INFO - __main__ -   Step 7 processed with cumulative accuracy: 29.81%\n",
            "11/15/2024 09:29:19 - INFO - __main__ -   Step 8 processed with cumulative accuracy: 29.49%\n",
            "11/15/2024 09:29:21 - INFO - __main__ -   Step 9 processed with cumulative accuracy: 30.32%\n",
            "11/15/2024 09:29:22 - INFO - __main__ -   Step 10 processed with cumulative accuracy: 29.96%\n",
            "11/15/2024 09:29:23 - INFO - __main__ -   Step 11 processed with cumulative accuracy: 30.51%\n",
            "11/15/2024 09:29:23 - INFO - __main__ -   Final Test Accuracy: 30.51%\n",
            "11/15/2024 09:29:23 - INFO - __main__ -   Evaluated on 12 samples, saved predictions at /content/GPT-NeoX-Colab/out/predictions.txt and ground truths at /content/GPT-NeoX-Colab/out/answers.txt\n",
            "11/15/2024 09:29:23 - INFO - __main__ -   Test accuracy: 30.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bNfCUIl3BOjS",
        "outputId": "d382a402-b64f-4b66-8aa4-d5b6f2b784bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INFO:__main__:Total 10600 tokens, accuracy: 34.69\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "# Run evaluator.py on the generated files\n",
        "evaluator_script = f\"{GPTNeoXColabDir}/scripts/evaluator.py\"\n",
        "answers_file = f\"{GPTNeoXColabDir}/out/answers.txt\"\n",
        "predictions_file = f\"{GPTNeoXColabDir}/out/predictions.txt\"\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\"python\", evaluator_script, \"--answers\", answers_file, \"--predictions\", predictions_file],\n",
        "        check=True,\n",
        "        capture_output=True,  # Capture stdout and stderr\n",
        "        text=True  # Decode output as text\n",
        "    )\n",
        "    # Print the output from evaluator.py\n",
        "    print(result.stdout)\n",
        "    print(result.stderr)  # Only if you want to see errors, if any\n",
        "except subprocess.CalledProcessError as e:\n",
        "    logger.error(f\"Evaluator script failed with error: {e}\")\n",
        "    print(e.output)  # Print the error message if the script fails\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}