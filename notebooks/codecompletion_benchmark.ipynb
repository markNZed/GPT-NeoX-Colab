{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/codecompletion_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8tGrS9KJu7QA"
      },
      "outputs": [],
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "workspaceDir = \"/content\"\n",
        "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
        "GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpgI19mPrtvy"
      },
      "source": [
        "# Clone CodeXGLUE Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qBcQotWw1U0",
        "outputId": "aae4faf4-bdec-4c7b-b909-e78ed049938b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CodeXGLUE'...\n",
            "remote: Enumerating objects: 3373, done.\u001b[K\n",
            "remote: Counting objects: 100% (3372/3372), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1534/1534), done.\u001b[K\n",
            "remote: Total 3373 (delta 1748), reused 3326 (delta 1733), pack-reused 1 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3373/3373), 213.15 MiB | 17.56 MiB/s, done.\n",
            "Resolving deltas: 100% (1748/1748), done.\n",
            "Updating files: 100% (400/400), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/microsoft/CodeXGLUE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOySwjeyktsH",
        "outputId": "7d4d1ba7-0406-47a1-9a41-bf2051117756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'GPT-NeoX-Colab' already exists and is not an empty directory.\n",
            "/content/GPT-NeoX-Colab\n",
            "\u001b[33mWARNING: pip is using lazily downloaded wheels using HTTP range requests to obtain dependency information. This experimental feature is enabled through --use-feature=fast-deps and it is not ready for production.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for GPTNeoXColab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Data retrieval successful.\n",
            "Data retrieval successful.\n",
            "/content/GPT-NeoX-Colab/models/codecompletion\n",
            "CPU times: user 107 ms, sys: 19.4 ms, total: 126 ms\n",
            "Wall time: 14.8 s\n"
          ]
        }
      ],
      "source": [
        "#@title Clone GPT-NeoX-Colab\n",
        "%%time\n",
        "%cd {workspaceDir}\n",
        "# Don't use --depth 1 because that does not play nice with git-annex\n",
        "!git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "%cd {GPTNeoXColabDir}\n",
        "%pip install -q -r requirements_colab.txt\n",
        "%pip install --use-feature=fast-deps -q .\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
        "import GPTNeoXColab\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/codecompletion/token_completion.tar.gz\")\n",
        "#GPTNeoXColab.utils.colab.fetch_data(\"models/codecompletion.tar.gz\")\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"models/codecompletion/global_step7000_HF.tar.gz\")\n",
        "#%cd {GPTNeoXColabDir}/data/codecompletion\n",
        "#!tar -xzf token_completion.tar.gz\n",
        "%cd {GPTNeoXColabDir}/models/codecompletion\n",
        "!tar -xzf global_step7000_HF.tar.gz\n",
        "!mv global_step7000_HF latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ7f8hTqipgF"
      },
      "source": [
        "# Using Byte-Pair Encoding Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmOgA5alzT2A",
        "outputId": "bc976522-2b73-4793-ef9f-75a377faf117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2024-11-14 20:32:51--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.138.64, 52.216.109.5, 16.182.70.80, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.138.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‘gpt2-vocab.json’\n",
            "\n",
            "gpt2-vocab.json     100%[===================>]   1018K  1.05MB/s    in 0.9s    \n",
            "\n",
            "2024-11-14 20:32:53 (1.05 MB/s) - ‘gpt2-vocab.json’ saved [1042301/1042301]\n",
            "\n",
            "--2024-11-14 20:32:53--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.138.64, 52.216.109.5, 16.182.70.80, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.138.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [text/plain]\n",
            "Saving to: ‘gpt2-merges.txt’\n",
            "\n",
            "gpt2-merges.txt     100%[===================>] 445.62K   588KB/s    in 0.8s    \n",
            "\n",
            "2024-11-14 20:32:54 (588 KB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd {workspaceDir}\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json &\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNB4sSsS-RN3"
      },
      "source": [
        "# HuggingFace Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztr-ItKf_G1M",
        "outputId": "ace8c1b9-4d58-4cda-ace4-37108df07546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Generated text: import sys , os \n",
            " import imp \n",
            " from optparse import make_option \n",
            " from django . conf import settings \n",
            " from django . conf . urls import url \n",
            " from django . template import RequestContext , RequestContext \n",
            " from django . template import RequestContext , RequestContext , RequestContext , RequestContext \n",
            " from django . template import RequestContext , RequestContext , RequestContext , RequestContext \n",
            " from django . template import RequestContext , RequestContext \n",
            " from django . template import RequestContext , RequestContext , RequestContext \n",
            " from django . template import RequestContext \n",
            " from django . template import RequestContext \n",
            " from django . template import RequestContext \n",
            " from django . template import RequestContext \n",
            " from django . template import RequestContext <\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "%cd {workspaceDir}\n",
        "\n",
        "# Initialize the tokenizer with your vocabulary and merge files\n",
        "tokenizer = GPT2Tokenizer(vocab_file=f\"{workspaceDir}/gpt2-vocab.json\", merges_file=f\"{workspaceDir}/gpt2-merges.txt\")\n",
        "\n",
        "# Load your model\n",
        "model_path = f\"{GPTNeoXColabDir}/models/codecompletion/latest\"\n",
        "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user for input\n",
        "input_text = \"\"\"<s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django\"\"\"\n",
        "\n",
        "# Tokenize and prepare input\n",
        "input_ids = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
        "\n",
        "# Generate text with specified pad_token_id and attention_mask\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,          # Adjust this for desired output length\n",
        "        temperature=0.7,        # Controls creativity\n",
        "        top_k=50,               # Controls diversity\n",
        "        top_p=0.9,              # Nucleus sampling\n",
        "        num_return_sequences=1, # Number of sequences to return\n",
        "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
        "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
        "    )\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output[0].tolist())\n",
        "\n",
        "# Function to replace special tokens with original representations\n",
        "def replace_special_tokens(text):\n",
        "    replacements = {\n",
        "        \"<EOL>\": \"\\n\",  # Replace with actual newline\n",
        "        \"<s>\": \"\",\n",
        "        \"</s>\": \"\",     # Remove end token\n",
        "        \"<STR_LIT>\": \"STR_LITERAL\",  # Example replacement, adjust as necessary\n",
        "        \"<NUM_LIT>\": \"NUM_LITERAL\",   # Example replacement, adjust as necessary\n",
        "    }\n",
        "\n",
        "    for token, replacement in replacements.items():\n",
        "        text = text.replace(token, replacement)\n",
        "\n",
        "    return text.strip()  # Strip leading/trailing whitespace\n",
        "\n",
        "# Replace special tokens in the generated text\n",
        "final_text = replace_special_tokens(generated_text)\n",
        "\n",
        "# Print the final output\n",
        "print(\"Generated text:\", final_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dsoKIdFh99O_"
      },
      "outputs": [],
      "source": [
        "%pip show datasets\n",
        "%pip install datasets==1.18.0\n",
        "%pip install hf-transfer\n",
        "%pip install lm-eval --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P97thGi_Qab",
        "outputId": "0a202ddb-93ce-467f-dd85-220f7f62878c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150\n"
          ]
        }
      ],
      "source": [
        "!cp {workspaceDir}/gpt2-vocab.json {GPTNeoXColabDir}/models/codecompletion/vocab.json\n",
        "!cp {workspaceDir}/gpt2-merges.txt {GPTNeoXColabDir}/models/codecompletion/merges.txt\n",
        "!cp {GPTNeoXColabDir}/data/codecompletion/token_completion.tar.gz {workspaceDir}/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150\n",
        "%cd {workspaceDir}/CodeXGLUE/Code-Code/CodeCompletion-token/dataset/py150\n",
        "!tar -xzf token_completion.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmLwXfGIc8uJ",
        "outputId": "5e5f6cb9-f65d-4a28-d3a3-71eefd6a20e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CodeXGLUE/Code-Code/CodeCompletion-token/code\n",
            "global_step7000_HF.tar.gz  latest  merges.txt  vocab.json\n",
            "2024-11-14 20:37:31.759786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 20:37:31.774528: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 20:37:31.778750: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 20:37:32.879915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2024 20:37:34 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False, world size: 1\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/CodeXGLUE/Code-Code/CodeCompletion-token/code/run_lm.py\", line 715, in <module>\n",
            "    main()\n",
            "  File \"/content/CodeXGLUE/Code-Code/CodeCompletion-token/code/run_lm.py\", line 677, in main\n",
            "    model = model_class.from_pretrained(pretrained)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3763, in from_pretrained\n",
            "    raise EnvironmentError(\n",
            "OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /content/GPT-NeoX-Colab/models/codecompletion.\n"
          ]
        }
      ],
      "source": [
        "# This is not the way we are plannin gto run the model but I leave it hear for now. It needs a GPU but HF does not.\n",
        "%cd {workspaceDir}/CodeXGLUE/Code-Code/CodeCompletion-token/code\n",
        "!ls {GPTNeoXColabDir}/models/codecompletion\n",
        "!python -u run_lm.py \\\n",
        "        --data_dir=../dataset/py150/token_completion \\\n",
        "        --lit_file=../dataset/py150/literals.json \\\n",
        "        --langs=$LANG \\\n",
        "        --output_dir=../dataset/py150 \\\n",
        "        --pretrain_dir=/content/GPT-NeoX-Colab/models/codecompletion \\\n",
        "        --log_file=../completion_python_eval.log \\\n",
        "        --model_type=gpt2 \\\n",
        "        --block_size=2048 \\\n",
        "        --do_eval \\\n",
        "        --per_gpu_eval_batch_size=4 \\\n",
        "        --logging_steps=100 \\\n",
        "        --seed=42"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}