{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/codecompletion_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8tGrS9KJu7QA"
   },
   "outputs": [],
   "source": [
    "# We could modify these paths to \"stub\" behavior for test/dev\n",
    "DOCKER = True\n",
    "workspaceDir = \"/content\"\n",
    "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
    "if DOCKER:\n",
    "    GPTNeoXColabDir = f\"/workspace\"\n",
    "else:\n",
    "    GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpgI19mPrtvy"
   },
   "source": [
    "# Clone CodeXGLUE Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0qBcQotWw1U0",
    "outputId": "aae4faf4-bdec-4c7b-b909-e78ed049938b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "fatal: destination path 'CodeXGLUE' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%cd {workspaceDir}\n",
    "!git clone --depth 1 https://github.com/microsoft/CodeXGLUE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOySwjeyktsH",
    "outputId": "7d4d1ba7-0406-47a1-9a41-bf2051117756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data retrieval successful.\n",
      "/workspace/data/codecompletion\n",
      "Data retrieval successful.\n",
      "/workspace/models/codecompletion\n",
      "CPU times: user 191 ms, sys: 50.4 ms, total: 242 ms\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#@title Clone GPT-NeoX-Colab\n",
    "if DOCKER:\n",
    "    %cd {GPTNeoXColabDir}\n",
    "else:\n",
    "    %cd {workspaceDir}\n",
    "    # Don't use --depth 1 because that does not play nice with git-annex\n",
    "    !git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
    "    %cd {GPTNeoXColabDir}\n",
    "    %pip install -q -r requirements_colab.txt\n",
    "    %pip install --use-feature=fast-deps -q .\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
    "import GPTNeoXColab\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"data/codecompletion/token_completion.tar.gz\")\n",
    "%cd {GPTNeoXColabDir}/data/codecompletion\n",
    "if not os.path.exists(f\"data/codecompletion/token_completion\"):\n",
    "    !tar -xzf token_completion.tar.gz\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"models/codecompletion/global_step7000_HF.tar.gz\")\n",
    "%cd {GPTNeoXColabDir}/models/codecompletion\n",
    "if not os.path.exists(f\"latest\"):\n",
    "    !tar -xzf global_step7000_HF.tar.gz\n",
    "    !mv global_step7000_HF latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ7f8hTqipgF"
   },
   "source": [
    "# Using Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmOgA5alzT2A",
    "outputId": "bc976522-2b73-4793-ef9f-75a377faf117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/models/codecompletion/latest\n",
      "--2024-11-15 08:00:17--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.12.118, 16.15.193.59, 3.5.24.248, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.12.118|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: ‘gpt2-vocab.json’\n",
      "\n",
      "gpt2-vocab.json     100%[===================>]   1018K   680KB/s    in 1.5s    \n",
      "\n",
      "2024-11-15 08:00:19 (680 KB/s) - ‘gpt2-vocab.json’ saved [1042301/1042301]\n",
      "\n",
      "--2024-11-15 08:00:19--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 3.5.24.29, 52.216.12.118, 16.15.193.59, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|3.5.24.29|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [text/plain]\n",
      "Saving to: ‘gpt2-merges.txt’\n",
      "\n",
      "gpt2-merges.txt     100%[===================>] 445.62K   330KB/s    in 1.4s    \n",
      "\n",
      "2024-11-15 08:00:21 (330 KB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd {GPTNeoXColabDir}/models/codecompletion/latest\n",
    "if not os.path.exists(\"gpt2-vocab.json\"):\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
    "    !mv gpt2-vocab.json vocab.json\n",
    "if not os.path.exists(\"gpt2-merges.txt\"):\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
    "    !mv gpt2-merges.txt merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNB4sSsS-RN3"
   },
   "source": [
    "# HuggingFace Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztr-ItKf_G1M",
    "outputId": "ace8c1b9-4d58-4cda-ace4-37108df07546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Generated text: <s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django . conf . urls import url <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import\n",
      "Final text: import sys , os \n",
      " import imp \n",
      " from optparse import make_option \n",
      " from django . conf import settings \n",
      " from django . conf . urls import url \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "%cd {workspaceDir}\n",
    "\n",
    "# Initialize the tokenizer with your vocabulary and merge files\n",
    "tokenizer = GPT2Tokenizer(vocab_file=f\"{GPTNeoXColabDir}/models/codecompletion/latest/vocab.json\", merges_file=f\"{GPTNeoXColabDir}/models/codecompletion/latest/merges.txt\")\n",
    "\n",
    "# Load your model\n",
    "model_path = f\"{GPTNeoXColabDir}/models/codecompletion/latest\"\n",
    "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Prompt the user for input\n",
    "input_text = \"\"\"<s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django\"\"\"\n",
    "\n",
    "# Tokenize and prepare input\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
    "\n",
    "# Generate text with specified pad_token_id and attention_mask\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=200,          # Adjust this for desired output length\n",
    "        temperature=0.7,        # Controls creativity\n",
    "        top_k=50,               # Controls diversity\n",
    "        top_p=0.9,              # Nucleus sampling\n",
    "        num_return_sequences=1, # Number of sequences to return\n",
    "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
    "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0].tolist())\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "# Function to replace special tokens with original representations\n",
    "def replace_special_tokens(text):\n",
    "    \"\"\"\n",
    "    Replaces special tokens in the generated text with more readable or context-appropriate representations.\n",
    "    \"\"\"\n",
    "    replacements = {\n",
    "        \"<EOL>\": \"\\n\",          # Replace with actual newline for code formatting\n",
    "        \"<s>\": \"\",              # Remove start token as it's not necessary in final output\n",
    "        \"</s>\": \"\",             # Remove end token as it's not necessary in final output\n",
    "        \"<pad>\": \"\",            # Remove padding tokens\n",
    "        \"<|UNKNOWN|>\": \"[UNK]\", # Represent unknown tokens in a readable way\n",
    "        \"<STR_LIT>\": \"\\\"STRING_LITERAL\\\"\",  # Placeholder for string literals\n",
    "        \"<NUM_LIT>\": \"0\",       # Placeholder for numeric literals\n",
    "        \"<BOOL_LIT>\": \"True\",   # Placeholder for boolean literals (e.g., True/False)\n",
    "        \"<COMMENT>\": \"# COMMENT\",  # Placeholder for comments in the code\n",
    "    }\n",
    "\n",
    "    # Replace each special token in text with its corresponding value in `replacements`\n",
    "    for token, replacement in replacements.items():\n",
    "        text = text.replace(token, replacement)\n",
    "\n",
    "    return text.strip()  # Strip leading/trailing whitespace for clean output\n",
    "\n",
    "# Replace special tokens in the generated text\n",
    "final_text = replace_special_tokens(generated_text)\n",
    "\n",
    "# Print the final output\n",
    "print(\"Final text:\", final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, logger, file_type='train', seq_length=1024):\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.makedirs(args.output_dir)\n",
    "        cached_file = os.path.join(args.output_dir, file_type+\"_blocksize_%d\"%(seq_length))\n",
    "        if os.path.exists(cached_file) and not args.overwrite_cache:\n",
    "            with open(cached_file, 'rb') as handle:\n",
    "                self.inputs = pickle.load(handle)\n",
    "\n",
    "        else:\n",
    "            self.inputs = []\n",
    "\n",
    "            datafile = os.path.join(args.data_dir, f\"{file_type}.txt\")\n",
    "            with open(datafile) as f:\n",
    "                data = f.readlines()\n",
    "\n",
    "            length = len(data)\n",
    "            logger.info(\"Data size: %d\"%(length))\n",
    "            input_ids = []\n",
    "            for idx,x in enumerate(data):\n",
    "                x = x.strip()\n",
    "                if x.startswith(\"<s>\") and x.endswith(\"</s>\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    x = \"<s> \" + x + \" </s>\"\n",
    "                try:\n",
    "                    input_ids.extend(tokenizer.encode(x))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if idx % (length//10) == 0:\n",
    "                    percent = idx / (length//10) * 10\n",
    "                    logger.warning(\"load %d\"%(percent))\n",
    "                if args.max_eval_length is not None and idx > args.max_eval_length:\n",
    "                    logger.info(f\"max eval length reached at {idx}\")\n",
    "                    break\n",
    "            del data\n",
    "            gc.collect()\n",
    "\n",
    "            logger.info(f\"tokens: {len(input_ids)}\")\n",
    "            self.split(input_ids, tokenizer, logger, seq_length=seq_length)\n",
    "            del input_ids\n",
    "            gc.collect()\n",
    "\n",
    "            with open(cached_file, 'wb') as handle:\n",
    "                pickle.dump(self.inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def split(self, input_ids, tokenizer, logger, seq_length=1024):\n",
    "        sample = []\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            sample = input_ids[i: i+seq_length]\n",
    "            if len(sample) == seq_length:\n",
    "                for j in range(seq_length):\n",
    "                    if tokenizer.convert_ids_to_tokens(sample[seq_length-1-j])[0] == '\\u0120' or tokenizer.convert_ids_to_tokens(sample[seq_length-1-j]).startswith(\"<NUM_LIT\"):\n",
    "                        break\n",
    "                    if sample[seq_length-1-j] in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id]:\n",
    "                        if sample[seq_length-1-j] != tokenizer.bos_token_id:\n",
    "                            j -= 1\n",
    "                        break\n",
    "                if j == seq_length-1:\n",
    "                    print(tokenizer.decode(sample))\n",
    "                    exit()\n",
    "                sample = sample[: seq_length-1-j]\n",
    "            # print(len(sample))\n",
    "            i += len(sample)\n",
    "            pad_len = seq_length-len(sample)\n",
    "            sample += [tokenizer.pad_token_id]*pad_len\n",
    "            self.inputs.append(sample)\n",
    "\n",
    "            if len(self.inputs) % 10000 == 0:\n",
    "                logger.info(f\"{len(self.inputs)} samples\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.inputs[item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizerFast'. \n",
      "The class this function is called from is 'GPT2Tokenizer'.\n",
      "11/15/2024 08:45:23 - INFO - __main__ -   Model has 44.65M trainable parameters\n",
      "11/15/2024 08:45:23 - INFO - __main__ -   Data size: 50000\n",
      "11/15/2024 08:45:23 - WARNING - __main__ -   load 0\n",
      "11/15/2024 08:45:23 - INFO - __main__ -   max eval length reached at 11\n",
      "11/15/2024 08:45:24 - INFO - __main__ -   tokens: 23540\n",
      "11/15/2024 08:45:27 - INFO - __main__ -   Step 0 processed with cumulative accuracy: 30.84%\n",
      "11/15/2024 08:45:30 - INFO - __main__ -   Step 1 processed with cumulative accuracy: 33.42%\n",
      "11/15/2024 08:45:32 - INFO - __main__ -   Step 2 processed with cumulative accuracy: 33.31%\n",
      "11/15/2024 08:45:35 - INFO - __main__ -   Step 3 processed with cumulative accuracy: 30.62%\n",
      "11/15/2024 08:45:39 - INFO - __main__ -   Step 4 processed with cumulative accuracy: 30.73%\n",
      "11/15/2024 08:45:42 - INFO - __main__ -   Step 5 processed with cumulative accuracy: 29.80%\n",
      "11/15/2024 08:45:45 - INFO - __main__ -   Step 6 processed with cumulative accuracy: 30.04%\n",
      "11/15/2024 08:45:47 - INFO - __main__ -   Step 7 processed with cumulative accuracy: 29.81%\n",
      "11/15/2024 08:45:49 - INFO - __main__ -   Step 8 processed with cumulative accuracy: 29.49%\n",
      "11/15/2024 08:45:51 - INFO - __main__ -   Step 9 processed with cumulative accuracy: 30.32%\n",
      "11/15/2024 08:45:53 - INFO - __main__ -   Step 10 processed with cumulative accuracy: 29.96%\n",
      "11/15/2024 08:45:55 - INFO - __main__ -   Step 11 processed with cumulative accuracy: 30.51%\n",
      "11/15/2024 08:45:55 - INFO - __main__ -   Final Test Accuracy: 30.51%\n",
      "11/15/2024 08:45:56 - INFO - __main__ -   Evaluated on 12 samples, saved predictions at /workspace/out/predictions.txt and ground truths at /workspace/out/answers.txt\n",
      "11/15/2024 08:45:56 - INFO - __main__ -   Test accuracy: 30.51%\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "from types import SimpleNamespace\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def decode_token_ids(token_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert token IDs to a string of code, handling special tokens and spacing.\n",
    "    \"\"\"\n",
    "    decoded_code = \"\"\n",
    "    for token_id in token_ids:\n",
    "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "        if token.startswith('\\u0120') and not decoded_code.endswith(\" \"):  # Handles space prefixes\n",
    "            decoded_code += \" \" + token[1:]\n",
    "        else:\n",
    "            decoded_code += token\n",
    "    return decoded_code.strip()\n",
    "\n",
    "def eval_acc(args, model, tokenizer, file_type='test'):\n",
    "    \"\"\"\n",
    "    Evaluate the model’s token-level code completion accuracy.\n",
    "    \"\"\"\n",
    "    # Load evaluation dataset\n",
    "    eval_dataset = EvalDataset(tokenizer, args, logger, file_type=file_type, seq_length=args.seq_length)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=SequentialSampler(eval_dataset), batch_size=args.eval_batch_size)\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters for accuracy\n",
    "    total_correct, total_predictions = 0, 0\n",
    "    total_pred_tokens, total_gt_tokens = [], []\n",
    "\n",
    "    # Iterate through batches in the evaluation dataset\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        inputs = batch.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            predicted_token_ids = outputs.logits.argmax(-1)  # Get predicted tokens\n",
    "\n",
    "        pred_ids = predicted_token_ids.cpu()\n",
    "        gt_ids = inputs.cpu()\n",
    "\n",
    "        # Process predictions and ground truths\n",
    "        all_pred = []\n",
    "        all_gt = []\n",
    "        prev_pred = None\n",
    "        for pred_seq, gt_seq in zip(pred_ids, gt_ids):\n",
    "            pred_seq = pred_seq.tolist()\n",
    "            gt_seq = gt_seq.tolist()\n",
    "\n",
    "            now_pred = []\n",
    "            now_gt = []\n",
    "            for i, (pred_id, gt_id) in enumerate(zip(pred_seq, gt_seq)):\n",
    "                gt_token = tokenizer.convert_ids_to_tokens(gt_id)\n",
    "                pred_token = tokenizer.convert_ids_to_tokens(pred_id)\n",
    "\n",
    "                if i == 0:\n",
    "                    if gt_token in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\"]:\n",
    "                        now_gt = [gt_id]\n",
    "                        now_pred = [0] if prev_pred is None else [prev_pred]\n",
    "                        all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
    "                        all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
    "                        now_gt = []\n",
    "                        now_pred = []\n",
    "                    else:\n",
    "                        now_gt = [gt_id]\n",
    "                        now_pred = [0] if prev_pred is None else [prev_pred]\n",
    "                else:\n",
    "                    if gt_token.startswith('\\u0120'):\n",
    "                        if len(now_gt) > 0:\n",
    "                            try:\n",
    "                                all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
    "                            except IndexError:\n",
    "                                all_pred.append(\"<SPACE>\")\n",
    "                            all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
    "                            now_gt = []\n",
    "                            now_pred = []\n",
    "                    if gt_token in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\"] or gt_token.startswith(\"<NUM_LIT\"):\n",
    "                        if len(now_gt) > 0:\n",
    "                            try:\n",
    "                                all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
    "                            except IndexError:\n",
    "                                all_pred.append(\"<SPACE>\")\n",
    "                            all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
    "                        now_gt = [gt_id]\n",
    "                        now_pred = [pred_seq[i-1]]\n",
    "                        try:\n",
    "                            all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
    "                        except IndexError:\n",
    "                            all_pred.append(\"<SPACE>\")\n",
    "                        all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
    "                        now_gt = []\n",
    "                        now_pred = []\n",
    "                        continue\n",
    "                    now_gt.append(gt_id)\n",
    "                    now_pred.append(pred_seq[i-1])\n",
    "\n",
    "        assert len(all_pred) == len(all_gt)\n",
    "\n",
    "        total_pred_tokens.extend(all_pred)\n",
    "        total_gt_tokens.extend(all_gt)\n",
    "\n",
    "        # Calculate batch accuracy\n",
    "        for pred_token, gt_token in zip(all_pred, all_gt):\n",
    "            if gt_token not in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\"]:\n",
    "                total_predictions += 1\n",
    "                if pred_token == gt_token:\n",
    "                    total_correct += 1\n",
    "\n",
    "        # Logging progress\n",
    "        if step % args.logging_steps == 0:\n",
    "            accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
    "            logger.info(f\"Step {step} processed with cumulative accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    # Final accuracy calculation\n",
    "    accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
    "    logger.info(f\"Final Test Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "\n",
    "    # Call post_process to generate predictions.txt and answers.txt\n",
    "    pred_file = os.path.join(args.output_dir, \"predictions.txt\")\n",
    "    gt_file = os.path.join(args.output_dir, \"answers.txt\")\n",
    "    true_texts = open(os.path.join(args.data_dir, f\"{file_type}.txt\")).readlines()\n",
    "    total_samples = post_process(args, total_pred_tokens, total_gt_tokens, true_texts, pred_file, gt_file)\n",
    "    logger.info(f\"Evaluated on {total_samples} samples, saved predictions at {pred_file} and ground truths at {gt_file}\")\n",
    "\n",
    "\n",
    "    return total_predictions, total_correct\n",
    "\n",
    "def post_process(args, preds, gts, true_gts, pred_file_path, gt_file_path):\n",
    "    \"\"\"\n",
    "    Save the post-processed predictions and ground truths, and verify with the expected true ground truths.\n",
    "\n",
    "    Args:\n",
    "        args: General arguments or configuration settings (unused here).\n",
    "        preds: List of predicted tokens from the model.\n",
    "        gts: List of ground truth tokens for each prediction.\n",
    "        true_gts: List of full ground truth sequences for each input, used for verification.\n",
    "        pred_file_path: Path to the file where the processed predictions will be saved.\n",
    "        gt_file_path: Path to the file where the processed ground truths will be saved.\n",
    "\n",
    "    Returns:\n",
    "        int: The count of sequences processed and saved.\n",
    "    \"\"\"\n",
    "    with open(pred_file_path, \"w\") as pred_file, open(gt_file_path, \"w\") as gt_file:\n",
    "        count = 0\n",
    "        new_gt = []\n",
    "        new_pred = []\n",
    "\n",
    "        for pred, gt in zip(preds, gts):\n",
    "            if gt in [\"\", \"<pad>\"]:\n",
    "                continue\n",
    "            new_gt.append(gt)\n",
    "            new_pred.append(pred.replace(\" \", \"\"))\n",
    "\n",
    "            if gt == \"</s>\":\n",
    "                gt_str = \" \".join(new_gt)\n",
    "                pred_str = \" \".join(new_pred)\n",
    "                assert gt_str == true_gts[count].strip(), f\"Sample {count} mismatch between ground truth and expected text\"\n",
    "                pred_file.write(pred_str + \"\\n\")\n",
    "                gt_file.write(gt_str + \"\\n\")\n",
    "                count += 1\n",
    "                new_gt = []\n",
    "                new_pred = []\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load model, tokenizer, and execute evaluation.\n",
    "    \"\"\"\n",
    "    pretrained_model_path = f\"{GPTNeoXColabDir}/models/codecompletion/latest\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set up evaluation arguments\n",
    "    args = {\n",
    "        \"n_gpu\": torch.cuda.device_count(),\n",
    "        \"per_gpu_eval_batch_size\": 1,\n",
    "        \"logging_steps\": 1, \n",
    "        \"output_dir\": f\"{GPTNeoXColabDir}/out\",\n",
    "        \"data_dir\": f\"{GPTNeoXColabDir}/data/codecompletion/token_completion\",\n",
    "        \"device\": device,\n",
    "        \"no_cuda\": False,\n",
    "        \"seq_length\": 2048,\n",
    "        \"max_eval_length\": 10,\n",
    "        \"overwrite_cache\": True,\n",
    "        \"eval_batch_size\": 1,\n",
    "        \"local_rank\": -1,\n",
    "    }\n",
    "\n",
    "    # Wrap args dictionary in a namespace to allow dot notation\n",
    "    args = SimpleNamespace(**args)\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                        level=logging.INFO)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_path, sep_token='<EOL>', bos_token='<s>', eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>')\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(pretrained_model_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    if total_params >= 1e9:\n",
    "        readable_params = f\"{total_params / 1e9:.2f}B\"  # Billions\n",
    "    elif total_params >= 1e6:\n",
    "        readable_params = f\"{total_params / 1e6:.2f}M\"  # Millions\n",
    "    else:\n",
    "        readable_params = f\"{total_params:,}\"  # Less than a million, use commas\n",
    "    logger.info(f\"Model has {readable_params} trainable parameters\")\n",
    "\n",
    "    # Evaluate model\n",
    "    total_predictions, total_correct = eval_acc(args, model, tokenizer, 'test')\n",
    "    accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
    "    logger.info(f\"Test accuracy: {accuracy:.2%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total 10600 tokens, accuracy: 34.69\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "# Run evaluator.py on the generated files\n",
    "evaluator_script = f\"{GPTNeoXColabDir}/scripts/evaluator.py\"\n",
    "answers_file = f\"{GPTNeoXColabDir}/out/answers.txt\"\n",
    "predictions_file = f\"{GPTNeoXColabDir}/out/predictions.txt\"\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [\"python\", evaluator_script, \"--answers\", answers_file, \"--predictions\", predictions_file],\n",
    "        check=True,\n",
    "    )\n",
    "except subprocess.CalledProcessError as e:\n",
    "    logger.error(f\"Evaluator script failed with error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
