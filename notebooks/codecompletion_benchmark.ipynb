{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/codecompletion_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8tGrS9KJu7QA"
   },
   "outputs": [],
   "source": [
    "# We could modify these paths to \"stub\" behavior for test/dev\n",
    "DOCKER = True\n",
    "workspaceDir = \"/content\"\n",
    "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
    "if DOCKER:\n",
    "    GPTNeoXColabDir = f\"/workspace\"\n",
    "else:\n",
    "    GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpgI19mPrtvy"
   },
   "source": [
    "# Clone CodeXGLUE Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0qBcQotWw1U0",
    "outputId": "aae4faf4-bdec-4c7b-b909-e78ed049938b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "fatal: destination path 'CodeXGLUE' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%cd {workspaceDir}\n",
    "!git clone --depth 1 https://github.com/microsoft/CodeXGLUE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOySwjeyktsH",
    "outputId": "7d4d1ba7-0406-47a1-9a41-bf2051117756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data retrieval successful.\n",
      "/workspace/data/codecompletion\n",
      "Data retrieval successful.\n",
      "/workspace/models/codecompletion\n",
      "CPU times: user 191 ms, sys: 50.4 ms, total: 242 ms\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#@title Clone GPT-NeoX-Colab\n",
    "if DOCKER:\n",
    "    %cd {GPTNeoXColabDir}\n",
    "else:\n",
    "    %cd {workspaceDir}\n",
    "    # Don't use --depth 1 because that does not play nice with git-annex\n",
    "    !git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
    "    %cd {GPTNeoXColabDir}\n",
    "    %pip install -q -r requirements_colab.txt\n",
    "    %pip install --use-feature=fast-deps -q .\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
    "import GPTNeoXColab\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"data/codecompletion/token_completion.tar.gz\")\n",
    "%cd {GPTNeoXColabDir}/data/codecompletion\n",
    "if not os.path.exists(f\"data/codecompletion/token_completion\"):\n",
    "    !tar -xzf token_completion.tar.gz\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"models/codecompletion/global_step7000_HF.tar.gz\")\n",
    "%cd {GPTNeoXColabDir}/models/codecompletion\n",
    "if not os.path.exists(f\"latest\"):\n",
    "    !tar -xzf global_step7000_HF.tar.gz\n",
    "    !mv global_step7000_HF latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ7f8hTqipgF"
   },
   "source": [
    "# Using Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmOgA5alzT2A",
    "outputId": "bc976522-2b73-4793-ef9f-75a377faf117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/models/codecompletion/latest\n",
      "--2024-11-15 08:00:17--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.12.118, 16.15.193.59, 3.5.24.248, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.12.118|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: ‘gpt2-vocab.json’\n",
      "\n",
      "gpt2-vocab.json     100%[===================>]   1018K   680KB/s    in 1.5s    \n",
      "\n",
      "2024-11-15 08:00:19 (680 KB/s) - ‘gpt2-vocab.json’ saved [1042301/1042301]\n",
      "\n",
      "--2024-11-15 08:00:19--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 3.5.24.29, 52.216.12.118, 16.15.193.59, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|3.5.24.29|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [text/plain]\n",
      "Saving to: ‘gpt2-merges.txt’\n",
      "\n",
      "gpt2-merges.txt     100%[===================>] 445.62K   330KB/s    in 1.4s    \n",
      "\n",
      "2024-11-15 08:00:21 (330 KB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd {GPTNeoXColabDir}/models/codecompletion/latest\n",
    "if not os.path.exists(\"gpt2-vocab.json\"):\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
    "    !mv gpt2-vocab.json vocab.json\n",
    "if not os.path.exists(\"gpt2-merges.txt\"):\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
    "    !mv gpt2-merges.txt merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNB4sSsS-RN3"
   },
   "source": [
    "# HuggingFace Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztr-ItKf_G1M",
    "outputId": "ace8c1b9-4d58-4cda-ace4-37108df07546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Generated text: <s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django . conf . urls import url <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import unittest <EOL> from django . utils import\n",
      "Final text: import sys , os \n",
      " import imp \n",
      " from optparse import make_option \n",
      " from django . conf import settings \n",
      " from django . conf . urls import url \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import unittest \n",
      " from django . utils import\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "%cd {workspaceDir}\n",
    "\n",
    "# Initialize the tokenizer with your vocabulary and merge files\n",
    "tokenizer = GPT2Tokenizer(vocab_file=f\"{GPTNeoXColabDir}/models/codecompletion/latest/vocab.json\", merges_file=f\"{GPTNeoXColabDir}/models/codecompletion/latest/merges.txt\")\n",
    "\n",
    "# Load your model\n",
    "model_path = f\"{GPTNeoXColabDir}/models/codecompletion/latest\"\n",
    "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Prompt the user for input\n",
    "input_text = \"\"\"<s> import sys , os <EOL> import imp <EOL> from optparse import make_option <EOL> from django . conf import settings <EOL> from django\"\"\"\n",
    "\n",
    "# Tokenize and prepare input\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "attention_mask = torch.ones_like(input_ids)  # Create an attention mask for non-padded input\n",
    "\n",
    "# Generate text with specified pad_token_id and attention_mask\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=200,          # Adjust this for desired output length\n",
    "        temperature=0.7,        # Controls creativity\n",
    "        top_k=50,               # Controls diversity\n",
    "        top_p=0.9,              # Nucleus sampling\n",
    "        num_return_sequences=1, # Number of sequences to return\n",
    "        pad_token_id=model.config.eos_token_id,  # Set pad_token_id explicitly\n",
    "        do_sample=True           # Enable sampling mode to use temperature and top_p\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0].tolist())\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "# Function to replace special tokens with original representations\n",
    "def replace_special_tokens(text):\n",
    "    \"\"\"\n",
    "    Replaces special tokens in the generated text with more readable or context-appropriate representations.\n",
    "    \"\"\"\n",
    "    replacements = {\n",
    "        \"<EOL>\": \"\\n\",          # Replace with actual newline for code formatting\n",
    "        \"<s>\": \"\",              # Remove start token as it's not necessary in final output\n",
    "        \"</s>\": \"\",             # Remove end token as it's not necessary in final output\n",
    "        \"<pad>\": \"\",            # Remove padding tokens\n",
    "        \"<|UNKNOWN|>\": \"[UNK]\", # Represent unknown tokens in a readable way\n",
    "        \"<STR_LIT>\": \"\\\"STRING_LITERAL\\\"\",  # Placeholder for string literals\n",
    "        \"<NUM_LIT>\": \"0\",       # Placeholder for numeric literals\n",
    "        \"<BOOL_LIT>\": \"True\",   # Placeholder for boolean literals (e.g., True/False)\n",
    "        \"<COMMENT>\": \"# COMMENT\",  # Placeholder for comments in the code\n",
    "    }\n",
    "\n",
    "    # Replace each special token in text with its corresponding value in `replacements`\n",
    "    for token, replacement in replacements.items():\n",
    "        text = text.replace(token, replacement)\n",
    "\n",
    "    return text.strip()  # Strip leading/trailing whitespace for clean output\n",
    "\n",
    "# Replace special tokens in the generated text\n",
    "final_text = replace_special_tokens(generated_text)\n",
    "\n",
    "# Print the final output\n",
    "print(\"Final text:\", final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, logger, file_type='train', seq_length=1024):\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.makedirs(args.output_dir)\n",
    "        cached_file = os.path.join(args.output_dir, file_type+\"_blocksize_%d\"%(seq_length))\n",
    "        if os.path.exists(cached_file) and not args.overwrite_cache:\n",
    "            with open(cached_file, 'rb') as handle:\n",
    "                self.inputs = pickle.load(handle)\n",
    "\n",
    "        else:\n",
    "            self.inputs = []\n",
    "\n",
    "            datafile = os.path.join(args.data_dir, f\"{file_type}.txt\")\n",
    "            with open(datafile) as f:\n",
    "                data = f.readlines()\n",
    "\n",
    "            length = len(data)\n",
    "            logger.info(\"Data size: %d\"%(length))\n",
    "            input_ids = []\n",
    "            for idx,x in enumerate(data):\n",
    "                x = x.strip()\n",
    "                if x.startswith(\"<s>\") and x.endswith(\"</s>\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    x = \"<s> \" + x + \" </s>\"\n",
    "                try:\n",
    "                    input_ids.extend(tokenizer.encode(x))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if idx % (length//10) == 0:\n",
    "                    percent = idx / (length//10) * 10\n",
    "                    logger.warning(\"load %d\"%(percent))\n",
    "                if args.max_eval_length is not None and idx > args.max_eval_length:\n",
    "                    logger.info(f\"max eval length reached at {idx}\")\n",
    "                    break\n",
    "            del data\n",
    "            gc.collect()\n",
    "\n",
    "            logger.info(f\"tokens: {len(input_ids)}\")\n",
    "            self.split(input_ids, tokenizer, logger, seq_length=seq_length)\n",
    "            del input_ids\n",
    "            gc.collect()\n",
    "\n",
    "            with open(cached_file, 'wb') as handle:\n",
    "                pickle.dump(self.inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def split(self, input_ids, tokenizer, logger, seq_length=1024):\n",
    "        sample = []\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            sample = input_ids[i: i+seq_length]\n",
    "            if len(sample) == seq_length:\n",
    "                for j in range(seq_length):\n",
    "                    if tokenizer.convert_ids_to_tokens(sample[seq_length-1-j])[0] == '\\u0120' or tokenizer.convert_ids_to_tokens(sample[seq_length-1-j]).startswith(\"<NUM_LIT\"):\n",
    "                        break\n",
    "                    if sample[seq_length-1-j] in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id]:\n",
    "                        if sample[seq_length-1-j] != tokenizer.bos_token_id:\n",
    "                            j -= 1\n",
    "                        break\n",
    "                if j == seq_length-1:\n",
    "                    print(tokenizer.decode(sample))\n",
    "                    exit()\n",
    "                sample = sample[: seq_length-1-j]\n",
    "            # print(len(sample))\n",
    "            i += len(sample)\n",
    "            pad_len = seq_length-len(sample)\n",
    "            sample += [tokenizer.pad_token_id]*pad_len\n",
    "            self.inputs.append(sample)\n",
    "\n",
    "            if len(self.inputs) % 10000 == 0:\n",
    "                logger.info(f\"{len(self.inputs)} samples\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.inputs[item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizerFast'. \n",
      "The class this function is called from is 'GPT2Tokenizer'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/15/2024 08:31:00 - INFO - __main__ -   Model has 44.65M trainable parameters\n",
      "11/15/2024 08:31:00 - INFO - __main__ -   Data size: 50000\n",
      "11/15/2024 08:31:00 - WARNING - __main__ -   load 0\n",
      "11/15/2024 08:31:00 - INFO - __main__ -   max eval length reached at 11\n",
      "11/15/2024 08:31:01 - INFO - __main__ -   tokens: 23540\n",
      "11/15/2024 08:31:04 - INFO - __main__ -   Step 0 processed with cumulative accuracy: 0.00%\n",
      "11/15/2024 08:31:06 - INFO - __main__ -   Step 1 processed with cumulative accuracy: 12.50%\n",
      "11/15/2024 08:31:08 - INFO - __main__ -   Step 2 processed with cumulative accuracy: 10.00%\n",
      "11/15/2024 08:31:10 - INFO - __main__ -   Step 3 processed with cumulative accuracy: 7.69%\n",
      "11/15/2024 08:31:13 - INFO - __main__ -   Step 4 processed with cumulative accuracy: 7.69%\n",
      "11/15/2024 08:31:16 - INFO - __main__ -   Step 5 processed with cumulative accuracy: 6.25%\n",
      "11/15/2024 08:31:19 - INFO - __main__ -   Step 6 processed with cumulative accuracy: 7.41%\n",
      "11/15/2024 08:31:22 - INFO - __main__ -   Step 7 processed with cumulative accuracy: 5.71%\n",
      "11/15/2024 08:31:24 - INFO - __main__ -   Step 8 processed with cumulative accuracy: 4.65%\n",
      "11/15/2024 08:31:27 - INFO - __main__ -   Step 9 processed with cumulative accuracy: 4.44%\n",
      "11/15/2024 08:31:30 - INFO - __main__ -   Step 10 processed with cumulative accuracy: 4.17%\n",
      "11/15/2024 08:31:34 - INFO - __main__ -   Step 11 processed with cumulative accuracy: 4.08%\n",
      "11/15/2024 08:31:34 - INFO - __main__ -   Final Test Accuracy: 4.08%\n",
      "11/15/2024 08:31:34 - INFO - __main__ -   Test accuracy: 4.08%\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "from types import SimpleNamespace\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "Code Completion Evaluation Pipeline\n",
    "\n",
    "This script evaluates a pre-trained language model’s token-level accuracy on code completion tasks.\n",
    "The script includes functions to decode token IDs, calculate accuracy, and process prediction batches\n",
    "for evaluation. It also verifies predictions against ground truth data and saves results to an output file.\n",
    "\n",
    "Modules and Functions:\n",
    "- `decode_token_ids`: Converts token IDs into readable code strings, managing special tokens and spacing.\n",
    "- `calculate_accuracy`: Compares predicted tokens with ground truth tokens and calculates accuracy.\n",
    "- `process_batch_predictions`: Processes predictions and ground truths from batches, converting them into lists of tokens.\n",
    "- `eval_acc`: The main evaluation function that loads data, predicts, decodes, and calculates accuracy.\n",
    "- `post_process`: Saves processed predictions to a file and verifies each sequence against expected ground truth.\n",
    "- `main`: The entry point, which loads the model, sets configurations, and initiates the evaluation pipeline.\n",
    "\n",
    "Dependencies:\n",
    "- Libraries: `torch`, `transformers`, `numpy`, and `torch.utils.data`.\n",
    "- Custom Modules: `EvalDataset` (assumed to be a dataset module for evaluation tasks).\n",
    "- Assumes access to a pre-trained GPT-2-based model for code completion.\n",
    "\n",
    "Usage:\n",
    "To run the script, ensure all dependencies are installed and specify the model and dataset paths.\n",
    "Logging will provide progress updates and final evaluation metrics.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def decode_token_ids(token_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert token IDs to a string of code, handling special tokens and spacing.\n",
    "    \"\"\"\n",
    "    decoded_code = \"\"\n",
    "    for token_id in token_ids:\n",
    "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "        # The character '\\u0120' is the Unicode representation of a non-breaking space with an extra semantic \n",
    "        # meaning in tokenization. Specifically, it’s often used to indicate that a token is preceded by a space.\n",
    "        if token.startswith('\\u0120') and not decoded_code.endswith(\" \"):  # Handles space prefixes\n",
    "            decoded_code += \" \" + token[1:]\n",
    "        else:\n",
    "            decoded_code += token\n",
    "    return decoded_code.strip()\n",
    "\n",
    "def calculate_accuracy(pred_tokens, gt_tokens, special_tokens=[\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\"]):\n",
    "    \"\"\"\n",
    "    Calculate accuracy by comparing predicted tokens to ground truth tokens.\n",
    "    \"\"\"\n",
    "    correct_count = sum(1 for pred, gt in zip(pred_tokens, gt_tokens) if gt not in special_tokens and pred == gt)\n",
    "    total_count = sum(1 for gt in gt_tokens if gt not in special_tokens)\n",
    "    return correct_count, total_count\n",
    "\n",
    "def process_batch_predictions(batch_predictions, batch_ground_truths, tokenizer):\n",
    "    \"\"\"\n",
    "    Process batch of predictions and ground truths into readable token lists.\n",
    "    \"\"\"\n",
    "    all_pred_tokens, all_gt_tokens = [], []\n",
    "\n",
    "    for predicted_ids, gt_ids in zip(batch_predictions, batch_ground_truths):\n",
    "        pred_tokens, gt_tokens = [], []\n",
    "        for i, (pred_id, gt_id) in enumerate(zip(predicted_ids, gt_ids)):\n",
    "            gt_id = gt_id.item()  # Convert tensor to int\n",
    "            pred_id = pred_id.item()  # Convert tensor to int\n",
    "\n",
    "            gt_token = tokenizer.convert_ids_to_tokens(gt_id)\n",
    "\n",
    "            if gt_token in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\"]:  # Skip special tokens\n",
    "                break\n",
    "            elif gt_token.startswith('\\u0120') and pred_tokens:  # New token starts with a space\n",
    "                all_pred_tokens.append(decode_token_ids(pred_tokens, tokenizer))\n",
    "                all_gt_tokens.append(decode_token_ids(gt_tokens, tokenizer))\n",
    "                pred_tokens, gt_tokens = [], []\n",
    "\n",
    "            pred_tokens.append(pred_id)\n",
    "            gt_tokens.append(gt_id)\n",
    "\n",
    "    return all_pred_tokens, all_gt_tokens\n",
    "\n",
    "def eval_acc(args, model, tokenizer, file_type='test'):\n",
    "    \"\"\"\n",
    "    Evaluate the model’s token-level code completion accuracy.\n",
    "    \"\"\"\n",
    "    # Load evaluation dataset\n",
    "    eval_dataset = EvalDataset(tokenizer, args, logger, file_type=file_type, seq_length=args.seq_length)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=SequentialSampler(eval_dataset), batch_size=args.eval_batch_size)\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters for accuracy\n",
    "    total_correct, total_predictions = 0, 0\n",
    "    all_pred_tokens, all_gt_tokens = [], []\n",
    "\n",
    "    # Iterate through batches in the evaluation dataset\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        inputs = batch.to(args.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            predicted_token_ids = outputs.logits.argmax(-1)  # Get predicted tokens\n",
    "\n",
    "        # Decode batch predictions and ground truths\n",
    "        batch_pred_tokens, batch_gt_tokens = process_batch_predictions(predicted_token_ids.cpu(), inputs.cpu(), tokenizer)\n",
    "        all_pred_tokens.extend(batch_pred_tokens)\n",
    "        all_gt_tokens.extend(batch_gt_tokens)\n",
    "\n",
    "        # Calculate batch accuracy\n",
    "        batch_correct, batch_total = calculate_accuracy(batch_pred_tokens, batch_gt_tokens)\n",
    "        total_correct += batch_correct\n",
    "        total_predictions += batch_total\n",
    "\n",
    "        # Logging progress\n",
    "        if step % args.logging_steps == 0:\n",
    "            accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
    "            logger.info(f\"Step {step} processed with cumulative accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    # Final accuracy calculation\n",
    "    accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
    "    logger.info(f\"Final Test Accuracy: {accuracy:.2%}\")\n",
    "    return accuracy, all_pred_tokens, all_gt_tokens\n",
    "\n",
    "def post_process(args, predictions, ground_truths, true_texts, saved_file_path):\n",
    "    \"\"\"\n",
    "    Save the post-processed predictions and verify with the ground truth texts.\n",
    "\n",
    "    Args:\n",
    "        args: General arguments or configuration settings (unused here).\n",
    "        predictions: List of predicted tokens from the model.\n",
    "        ground_truths: List of ground truth tokens for each prediction.\n",
    "        true_texts: List of full ground truth sequences for each input, used for verification.\n",
    "        saved_file_path: Path to the file where the processed predictions will be saved.\n",
    "\n",
    "    Returns:\n",
    "        int: The count of sequences processed and saved.\n",
    "    \"\"\"\n",
    "    # Open the specified file in write mode to save processed predictions\n",
    "    with open(saved_file_path, \"w\") as wf:\n",
    "        count = 0  # Initialize a counter to track the number of completed sequences\n",
    "        current_pred, current_gt = [], []  # Lists to accumulate tokens for each sequence\n",
    "\n",
    "        # Iterate through each predicted and ground truth token pair\n",
    "        for pred, gt in zip(predictions, ground_truths):\n",
    "            # Skip empty or padding tokens in the ground truth, as they are not meaningful\n",
    "            if gt in [\"\", \"<pad>\"]:\n",
    "                continue\n",
    "            \n",
    "            # Append the current ground truth token to the list for the sequence\n",
    "            current_gt.append(gt)\n",
    "            # Append the current prediction, removing any extra spaces\n",
    "            current_pred.append(pred.replace(\" \", \"\"))\n",
    "\n",
    "            # Check if the current token is an end-of-sequence token\n",
    "            if gt == \"</s>\":\n",
    "                # Verify that the accumulated ground truth tokens match the expected text\n",
    "                assert \" \".join(current_gt) == true_texts[count].strip(), f\"Mismatch in sample {count}\"\n",
    "                \n",
    "                # Write the joined prediction sequence as a line in the file\n",
    "                wf.write(\" \".join(current_pred) + \"\\n\")\n",
    "                \n",
    "                # Increment the count of completed sequences\n",
    "                count += 1\n",
    "                \n",
    "                # Clear the lists to start accumulating tokens for the next sequence\n",
    "                current_pred, current_gt = [], []\n",
    "\n",
    "    # Return the total number of processed sequences\n",
    "    return count\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load model, tokenizer, and execute evaluation.\n",
    "    \"\"\"\n",
    "    pretrained_model_path = f\"{GPTNeoXColabDir}/models/codecompletion/latest\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set up evaluation arguments\n",
    "    args = {\n",
    "        \"n_gpu\": torch.cuda.device_count(),\n",
    "        \"per_gpu_eval_batch_size\": 1,\n",
    "        \"logging_steps\": 1,\n",
    "        \"output_dir\": f\"{GPTNeoXColabDir}/out\",\n",
    "        \"data_dir\": f\"{GPTNeoXColabDir}/data/codecompletion/token_completion\",\n",
    "        \"device\": device,\n",
    "        \"no_cuda\": False,\n",
    "        \"seq_length\": 2048,\n",
    "        \"max_eval_length\": 10,\n",
    "        \"overwrite_cache\": True,\n",
    "        \"eval_batch_size\": 1\n",
    "    }\n",
    "\n",
    "    # Wrap args dictionary in a namespace to allow dot notation\n",
    "    args = SimpleNamespace(**args)\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                        level=logging.INFO)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_path, sep_token='<EOL>', bos_token='<s>', eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>')\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(pretrained_model_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    if total_params >= 1e9:\n",
    "        readable_params = f\"{total_params / 1e9:.2f}B\"  # Billions\n",
    "    elif total_params >= 1e6:\n",
    "        readable_params = f\"{total_params / 1e6:.2f}M\"  # Millions\n",
    "    else:\n",
    "        readable_params = f\"{total_params:,}\"  # Less than a million, use commas\n",
    "    logger.info(f\"Model has {readable_params} trainable parameters\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    accuracy, predictions, ground_truths = eval_acc(args, model, tokenizer, 'test')\n",
    "    logger.info(f\"Test accuracy: {accuracy:.2%}\")\n",
    "\n",
    "        # Post-process predictions\n",
    "    true_texts = [...]  # Load or specify the true ground truth texts for verification\n",
    "    saved_file_path = f\"{args.output_dir}/predictions.txt\"\n",
    "    count = post_process(args, predictions, ground_truths, true_texts, saved_file_path)\n",
    "    logger.info(f\"Post-processed and saved {count} sequences to {saved_file_path}\")\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
