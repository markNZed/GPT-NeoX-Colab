{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/codecompletion_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHH8FdQuAv-P",
        "outputId": "98ac948c-d633-41a0-9c5f-78e9bcd80e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  DOCKER = False\n",
        "except:\n",
        "  DOCKER = True\n",
        "print(DOCKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8tGrS9KJu7QA"
      },
      "outputs": [],
      "source": [
        "# We could modify these paths to \"stub\" behavior for test/dev\n",
        "workspaceDir = \"/content\"\n",
        "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
        "if DOCKER:\n",
        "    GPTNeoXColabDir = f\"/workspace\"\n",
        "else:\n",
        "    GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpgI19mPrtvy"
      },
      "source": [
        "# Clone CodeXGLUE Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0qBcQotWw1U0"
      },
      "outputs": [],
      "source": [
        "# Not using this at but for a final sanity check we should use the data and evaluate.py from here\n",
        "#%cd {workspaceDir}\n",
        "#!git clone --depth 1 https://github.com/microsoft/CodeXGLUE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOySwjeyktsH",
        "outputId": "4156fe78-a05a-4c76-b893-b907b5447238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'GPT-NeoX-Colab'...\n",
            "remote: Enumerating objects: 1465, done.\u001b[K\n",
            "remote: Counting objects: 100% (413/413), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 1465 (delta 255), reused 347 (delta 209), pack-reused 1052 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1465/1465), 14.80 MiB | 8.36 MiB/s, done.\n",
            "Resolving deltas: 100% (832/832), done.\n",
            "/content/GPT-NeoX-Colab\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.9/251.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.0/426.0 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.5/456.5 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.6/367.6 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.4/901.4 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.7/548.7 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.4/571.4 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.1/113.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.2/722.2 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for configobj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pysftp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for GPTNeoXColab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "CPU times: user 779 ms, sys: 130 ms, total: 908 ms\n",
            "Wall time: 2min 41s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title Clone GPT-NeoX-Colab\n",
        "if DOCKER:\n",
        "    %cd {GPTNeoXColabDir}\n",
        "else:\n",
        "    %cd {workspaceDir}\n",
        "    # Don't use --depth 1 because that does not play nice with git-annex\n",
        "    !git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
        "    %cd {GPTNeoXColabDir}\n",
        "    %pip install -q -r requirements_colab.txt\n",
        "    %pip install -q ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu4yRpEzCyF6",
        "outputId": "c26ca74f-72d0-497e-db54-524ba4d9a57c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab\n",
            "Data retrieval successful.\n",
            "/content/GPT-NeoX-Colab/data/codecompletion\n",
            "/content/GPT-NeoX-Colab\n",
            "Data retrieval successful.\n",
            "/content/GPT-NeoX-Colab/models/codecompletion\n"
          ]
        }
      ],
      "source": [
        "%cd {GPTNeoXColabDir}\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
        "import GPTNeoXColab\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"data/codecompletion/token_completion.tar.gz\")\n",
        "%cd {GPTNeoXColabDir}/data/codecompletion\n",
        "if not os.path.exists(f\"data/codecompletion/token_completion\"):\n",
        "    !tar -xzf token_completion.tar.gz\n",
        "%cd {GPTNeoXColabDir}\n",
        "GPTNeoXColab.utils.colab.fetch_data(\"models/codecompletion/global_step7000_HF.tar.gz\")\n",
        "%cd {GPTNeoXColabDir}/models/codecompletion\n",
        "if not os.path.exists(f\"latest\"):\n",
        "    !tar -xzf global_step7000_HF.tar.gz\n",
        "    !mv global_step7000_HF latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ7f8hTqipgF"
      },
      "source": [
        "# Using Byte-Pair Encoding Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmOgA5alzT2A",
        "outputId": "9a6e6085-3e55-4da1-a40f-d4423be42821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-NeoX-Colab/models/codecompletion/latest\n",
            "--2024-11-17 10:26:05--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.39.104, 52.217.113.240, 3.5.17.116, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.39.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‘gpt2-vocab.json’\n",
            "\n",
            "gpt2-vocab.json     100%[===================>]   1018K   218KB/s    in 5.8s    \n",
            "\n",
            "2024-11-17 10:26:12 (174 KB/s) - ‘gpt2-vocab.json’ saved [1042301/1042301]\n",
            "\n",
            "--2024-11-17 10:26:12--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.240.174, 52.217.70.150, 52.217.65.6, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.240.174|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [text/plain]\n",
            "Saving to: ‘gpt2-merges.txt’\n",
            "\n",
            "gpt2-merges.txt     100%[===================>] 445.62K   513KB/s    in 0.9s    \n",
            "\n",
            "2024-11-17 10:26:14 (513 KB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd {GPTNeoXColabDir}/models/codecompletion/latest\n",
        "if not os.path.exists(\"vocab.json\"):\n",
        "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
        "    !mv gpt2-vocab.json vocab.json\n",
        "if not os.path.exists(\"merges.txt\"):\n",
        "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
        "    !mv gpt2-merges.txt merges.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Hm2e4KH4BOjM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import gc\n",
        "\n",
        "class EvalDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, logger, file_type='train', seq_length=1024):\n",
        "        if not os.path.exists(args.output_dir):\n",
        "            os.makedirs(args.output_dir)\n",
        "        cached_file = os.path.join(args.output_dir, file_type+\"_blocksize_cache_%d\"%(seq_length))\n",
        "        if os.path.exists(cached_file) and not args.overwrite_cache:\n",
        "            with open(cached_file, 'rb') as handle:\n",
        "                self.inputs = pickle.load(handle)\n",
        "\n",
        "        else:\n",
        "            self.inputs = []\n",
        "\n",
        "            datafile = os.path.join(args.data_dir, f\"{file_type}.txt\")\n",
        "            with open(datafile) as f:\n",
        "                data = f.readlines()\n",
        "\n",
        "            length = len(data)\n",
        "            logger.info(\"Data size: %d\"%(length))\n",
        "            input_ids = []\n",
        "            for idx,x in enumerate(data):\n",
        "                x = x.strip()\n",
        "                if x.startswith(\"<s>\") and x.endswith(\"</s>\"):\n",
        "                    pass\n",
        "                else:\n",
        "                    x = \"<s> \" + x + \" </s>\"\n",
        "                try:\n",
        "                    input_ids.extend(tokenizer.encode(x))\n",
        "                except Exception:\n",
        "                    pass\n",
        "                if idx % (length//10) == 0:\n",
        "                    percent = idx / (length//10) * 10\n",
        "                    logger.info(\"Processed %d%% of the dataset\"%(percent))\n",
        "                if args.max_eval_length is not None and (idx + 1) == args.max_eval_length:\n",
        "                    logger.info(f\"max eval length reached at {idx}\")\n",
        "                    break\n",
        "            del data\n",
        "            gc.collect()\n",
        "\n",
        "            logger.info(f\"tokens: {len(input_ids)}\")\n",
        "            self.split(input_ids, tokenizer, logger, seq_length=seq_length)\n",
        "            del input_ids\n",
        "            gc.collect()\n",
        "\n",
        "            with open(cached_file, 'wb') as handle:\n",
        "                pickle.dump(self.inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def split(self, input_ids, tokenizer, logger, seq_length=1024):\n",
        "        sample = []\n",
        "        i = 0\n",
        "        while i < len(input_ids):\n",
        "            sample = input_ids[i: i+seq_length]\n",
        "            if len(sample) == seq_length:\n",
        "                for j in range(seq_length):\n",
        "                    if tokenizer.convert_ids_to_tokens(sample[seq_length-1-j])[0] == '\\u0120' or tokenizer.convert_ids_to_tokens(sample[seq_length-1-j]).startswith(\"<NUM_LIT\"):\n",
        "                        break\n",
        "                    if sample[seq_length-1-j] in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id]:\n",
        "                        if sample[seq_length-1-j] != tokenizer.bos_token_id:\n",
        "                            j -= 1\n",
        "                        break\n",
        "                if j == seq_length-1:\n",
        "                    print(tokenizer.decode(sample))\n",
        "                    exit()\n",
        "                sample = sample[: seq_length-1-j]\n",
        "            # print(len(sample))\n",
        "            i += len(sample)\n",
        "            pad_len = seq_length-len(sample)\n",
        "            sample += [tokenizer.pad_token_id]*pad_len\n",
        "            self.inputs.append(sample)\n",
        "\n",
        "            if len(self.inputs) % 10000 == 0:\n",
        "                logger.info(f\"{len(self.inputs)} samples\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.inputs[item])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oA3pTtlIBOjQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SequentialSampler\n",
        "import os\n",
        "import re\n",
        "\n",
        "def decode_token_ids(token_ids, tokenizer):\n",
        "    \"\"\"\n",
        "    Convert token IDs to a string of code, handling special tokens, spacing, and literals.\n",
        "    \"\"\"\n",
        "    decoded_code = \"\"\n",
        "    for token_id in token_ids:\n",
        "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "\n",
        "        # Handle tokens with a space prefix (e.g., '\\u0120')\n",
        "        if token.startswith('\\u0120'):\n",
        "            if not decoded_code.endswith(\" \"):  # Avoid double spaces\n",
        "                decoded_code += \" \"\n",
        "            decoded_code += token[1:]  # Remove the space marker\n",
        "        # Handle special tokens (bos, eos, sep, pad)\n",
        "        elif token_id in [\n",
        "            tokenizer.bos_token_id,\n",
        "            tokenizer.eos_token_id,\n",
        "            tokenizer.sep_token_id,\n",
        "            tokenizer.pad_token_id\n",
        "        ]:\n",
        "            decoded_code += \" \" + token + \" \"  # Add spaces around special tokens\n",
        "        # Handle literals (e.g., <NUM_LIT>, <STR_LIT>)\n",
        "        elif token.startswith(\"<NUM_LIT\") or token.startswith(\"<STR_LIT\"):\n",
        "            decoded_code += \" \" + token + \" \"\n",
        "        # Handle regular tokens\n",
        "        else:\n",
        "            decoded_code += token\n",
        "\n",
        "    # Stripping here means spaces are ignored - htey should be pre-processed with <SPACE> token ?\n",
        "    return decoded_code.strip()\n",
        "\n",
        "def eval_acc(args, model, tokenizer, file_type='test'):\n",
        "    \"\"\"\n",
        "    Evaluate the model’s token-level code completion accuracy.\n",
        "    \"\"\"\n",
        "    # Load evaluation dataset\n",
        "    eval_dataset = EvalDataset(tokenizer, args, logger, file_type=file_type, seq_length=args.seq_length)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=SequentialSampler(eval_dataset), batch_size=args.eval_batch_size)\n",
        "    model.to(args.device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize counters for accuracy\n",
        "    total_correct, total_predictions = 0, 0\n",
        "    total_pred_tokens, total_gt_tokens = [], []\n",
        "\n",
        "    # Iterate through batches in the evaluation dataset\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        inputs = batch.to(args.device)\n",
        "\n",
        "        # no_grad because only inference\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            predicted_token_ids = outputs.logits.argmax(-1)\n",
        "\n",
        "        # Move from the GPU to CPU (if GPU is being used)\n",
        "        pred_ids = predicted_token_ids.cpu()\n",
        "        gt_ids = inputs.cpu()\n",
        "\n",
        "        # Process predictions and ground truths\n",
        "        all_pred = []\n",
        "        all_gt = []\n",
        "        for pred_seq, gt_seq in zip(pred_ids, gt_ids):\n",
        "            pred_seq = pred_seq.tolist()\n",
        "            gt_seq = gt_seq.tolist()\n",
        "\n",
        "            # Arrays that can store multiple \"sub-tokens\"\n",
        "            # The model may tokenize into smaller tokens than the benchmark uses\n",
        "            now_pred = []\n",
        "            now_gt = []\n",
        "            for i, (pred_id, gt_id) in enumerate(zip(pred_seq, gt_seq)):\n",
        "                gt_token = tokenizer.convert_ids_to_tokens(gt_id)\n",
        "                pred_token = tokenizer.convert_ids_to_tokens(pred_id)\n",
        "\n",
        "                if i == 0:\n",
        "                    if gt_id in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]:\n",
        "                        now_gt = [gt_id]\n",
        "                        # These tokens are excluded from accuracy metrics so insert a placeholder in now_pred\n",
        "                        now_pred = [0]\n",
        "                        all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
        "                        all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
        "                        now_gt = []\n",
        "                        now_pred = []\n",
        "                    else:\n",
        "                        # The prediction is the next token after the ground_truth so we do not use it\n",
        "                        now_gt = [gt_id]\n",
        "                        now_pred = [0]\n",
        "                else:\n",
        "                    # \\u0120 special char indicates the start of a new token\n",
        "                    if gt_token.startswith('\\u0120'):\n",
        "                        # Check not empty because it can be reset to empty\n",
        "                        if len(now_gt) > 0:\n",
        "                            try:\n",
        "                                # only the first word of the decoded string is appended to all_pred\n",
        "                                all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
        "                            except IndexError:\n",
        "                                all_pred.append(\"<SPACE>\")\n",
        "                            all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
        "                            now_gt = []\n",
        "                            now_pred = []\n",
        "                    # We are at a gt_token boundary\n",
        "                    if gt_id in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id] \\\n",
        "                    or gt_token.startswith(\"<NUM_LIT\") or gt_token.startswith(\"<STR_LIT\"):\n",
        "                        if len(now_gt) > 0:\n",
        "                            try:\n",
        "                                # only the first word of the decoded string is appended to all_pred\n",
        "                                all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
        "                            except IndexError:\n",
        "                                all_pred.append(\"<SPACE>\")\n",
        "                            all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
        "                        now_gt = [gt_id]\n",
        "                        now_pred = [pred_seq[i-1]] # Because prediction is one token ahead of gt\n",
        "                        try:\n",
        "                            all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
        "                        except IndexError:\n",
        "                            all_pred.append(\"<SPACE>\")\n",
        "                        all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
        "                        now_gt = []\n",
        "                        now_pred = []\n",
        "                        continue\n",
        "\n",
        "                    now_gt.append(gt_id)\n",
        "                    now_pred.append(pred_seq[i-1]) # Because prediction is one token ahead of gt\n",
        "\n",
        "        assert len(all_pred) == len(all_gt)\n",
        "\n",
        "        total_pred_tokens.extend(all_pred)\n",
        "        total_gt_tokens.extend(all_gt)\n",
        "\n",
        "        # Calculate batch accuracy\n",
        "        for pred_token, gt_token in zip(all_pred, all_gt):\n",
        "            # Also ignore empty token which is caused by whitespace removal in decode_token_ids\n",
        "            # Then benchmark is measuring prediction of functional tokens not layout\n",
        "            if gt_token not in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\", \"\"]:\n",
        "                total_predictions += 1\n",
        "                if pred_token == gt_token:\n",
        "                    total_correct += 1\n",
        "                    #logger.info(f\"Match {total_correct}/{total_predictions} {pred_token} == '{gt_token}'\")\n",
        "                else:\n",
        "                    #logger.info(f\"Mismatch {total_correct}/{total_predictions} {pred_token} != '{gt_token}'\")\n",
        "                    pass\n",
        "\n",
        "        # Logging progress\n",
        "        # The samples will be batched so the steps logged here are relative to the batch size\n",
        "        if step % args.logging_steps == 0:\n",
        "            accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
        "            logger.info(f\"Step {step} processed with cumulative accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    # Final accuracy calculation\n",
        "    accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
        "    logger.info(f\"Final Test Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    # Call post_process to generate predictions.txt and answers.txt\n",
        "    pred_file = os.path.join(args.output_dir, \"predictions.txt\")\n",
        "    answers_file = os.path.join(args.output_dir, \"answers.txt\")\n",
        "    true_texts = open(os.path.join(args.data_dir, f\"{file_type}.txt\")).readlines()\n",
        "    total_samples = post_process(total_pred_tokens, total_gt_tokens, true_texts, pred_file, answers_file)\n",
        "    logger.info(f\"Evaluated on {total_samples} samples, saved predictions at {pred_file} and ground truths at {answers_file}\")\n",
        "\n",
        "    return total_predictions, total_correct\n",
        "\n",
        "def post_process(preds, gts, true_gts, pred_file_path, answers_file_path):\n",
        "    \"\"\"\n",
        "    Save the post-processed predictions and ground truths, and verify with the expected true ground truths.\n",
        "\n",
        "    Args:\n",
        "        preds: List of predicted tokens from the model.\n",
        "        gts: List of ground truth tokens for each prediction.\n",
        "        true_gts: List of full ground truth sequences for each input, used for verification.\n",
        "        pred_file_path: Path to the file where the processed predictions will be saved.\n",
        "        gt_file_path: Path to the file where the processed ground truths will be saved.\n",
        "\n",
        "    Returns:\n",
        "        int: The count of sequences processed and saved.\n",
        "    \"\"\"\n",
        "    with open(pred_file_path, \"w\") as pred_file, open(answers_file_path, \"w\") as answers_file:\n",
        "        count = 0\n",
        "        new_gt = []\n",
        "        new_pred = []\n",
        "\n",
        "        for pred, gt in zip(preds, gts):\n",
        "            if gt in [\"\", \"<pad>\"]:\n",
        "                continue\n",
        "            new_gt.append(gt)\n",
        "            # Spaces are used to separate the tokens\n",
        "            # So we remove extra spaces\n",
        "            new_pred.append(pred.replace(\" \", \"\"))\n",
        "\n",
        "            if gt == \"</s>\":\n",
        "                gt_str = \" \".join(new_gt)\n",
        "                pred_str = \" \".join(new_pred)\n",
        "                assert gt_str == true_gts[count].strip(), f\"Sample {count} mismatch between ground truth and expected text\"\n",
        "                pred_file.write(pred_str + \"\\n\")\n",
        "                answers_file.write(true_gts[count].strip() + \"\\n\")\n",
        "                count += 1\n",
        "                new_gt = []\n",
        "                new_pred = []\n",
        "\n",
        "    return count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U9QOv0YAIfyH"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "# Reset the root logger to avoid inherited handlers\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "# Configure your specific logger\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Check if logger already has handlers; clear them if necessary\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "# Create a StreamHandler to ensure logging messages appear in the notebook\n",
        "stream_handler = logging.StreamHandler()\n",
        "stream_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Set a formatter for consistent log message formatting\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s -   %(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
        "stream_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the StreamHandler to the logger\n",
        "logger.addHandler(stream_handler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xmqKGg6DxUH",
        "outputId": "f799b749-4191-4830-993e-e39227896019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizerFast'. \n",
            "The class this function is called from is 'GPT2Tokenizer'.\n",
            "11/17/2024 10:26:14 - INFO - __main__ -   Model has 44.6 million trainable parameters\n",
            "11/17/2024 10:26:15 - INFO - __main__ -   Data size: 50000\n",
            "11/17/2024 10:26:15 - INFO - __main__ -   Processed 0% of the dataset\n",
            "11/17/2024 10:26:51 - INFO - __main__ -   Processed 10% of the dataset\n",
            "11/17/2024 10:27:33 - INFO - __main__ -   Processed 20% of the dataset\n",
            "11/17/2024 10:28:10 - INFO - __main__ -   Processed 30% of the dataset\n",
            "11/17/2024 10:28:50 - INFO - __main__ -   Processed 40% of the dataset\n",
            "11/17/2024 10:29:29 - INFO - __main__ -   Processed 50% of the dataset\n",
            "11/17/2024 10:30:08 - INFO - __main__ -   Processed 60% of the dataset\n",
            "11/17/2024 10:30:48 - INFO - __main__ -   Processed 70% of the dataset\n",
            "11/17/2024 10:31:27 - INFO - __main__ -   Processed 80% of the dataset\n",
            "11/17/2024 10:32:11 - INFO - __main__ -   Processed 90% of the dataset\n",
            "11/17/2024 10:32:46 - INFO - __main__ -   tokens: 84277947\n",
            "11/17/2024 10:32:46 - INFO - __main__ -   10000 samples\n",
            "11/17/2024 10:32:46 - INFO - __main__ -   20000 samples\n",
            "11/17/2024 10:32:47 - INFO - __main__ -   30000 samples\n",
            "11/17/2024 10:32:47 - INFO - __main__ -   40000 samples\n",
            "11/17/2024 10:32:51 - INFO - __main__ -   Step 0 processed with cumulative accuracy: 34.48%\n",
            "11/17/2024 10:33:40 - INFO - __main__ -   Step 100 processed with cumulative accuracy: 36.69%\n",
            "11/17/2024 10:34:30 - INFO - __main__ -   Step 200 processed with cumulative accuracy: 37.44%\n",
            "11/17/2024 10:35:20 - INFO - __main__ -   Step 300 processed with cumulative accuracy: 37.45%\n",
            "11/17/2024 10:36:11 - INFO - __main__ -   Step 400 processed with cumulative accuracy: 37.27%\n",
            "11/17/2024 10:37:02 - INFO - __main__ -   Step 500 processed with cumulative accuracy: 37.12%\n",
            "11/17/2024 10:37:52 - INFO - __main__ -   Step 600 processed with cumulative accuracy: 36.93%\n",
            "11/17/2024 10:38:43 - INFO - __main__ -   Step 700 processed with cumulative accuracy: 36.76%\n",
            "11/17/2024 10:39:34 - INFO - __main__ -   Step 800 processed with cumulative accuracy: 36.71%\n",
            "11/17/2024 10:40:24 - INFO - __main__ -   Step 900 processed with cumulative accuracy: 36.74%\n",
            "11/17/2024 10:41:15 - INFO - __main__ -   Step 1000 processed with cumulative accuracy: 36.68%\n",
            "11/17/2024 10:42:06 - INFO - __main__ -   Step 1100 processed with cumulative accuracy: 36.71%\n",
            "11/17/2024 10:42:56 - INFO - __main__ -   Step 1200 processed with cumulative accuracy: 36.70%\n",
            "11/17/2024 10:43:47 - INFO - __main__ -   Step 1300 processed with cumulative accuracy: 36.84%\n",
            "11/17/2024 10:44:38 - INFO - __main__ -   Step 1400 processed with cumulative accuracy: 36.80%\n",
            "11/17/2024 10:45:28 - INFO - __main__ -   Step 1500 processed with cumulative accuracy: 36.80%\n",
            "11/17/2024 10:46:19 - INFO - __main__ -   Step 1600 processed with cumulative accuracy: 36.79%\n",
            "11/17/2024 10:47:10 - INFO - __main__ -   Step 1700 processed with cumulative accuracy: 36.82%\n",
            "11/17/2024 10:48:00 - INFO - __main__ -   Step 1800 processed with cumulative accuracy: 36.78%\n",
            "11/17/2024 10:48:51 - INFO - __main__ -   Step 1900 processed with cumulative accuracy: 36.77%\n",
            "11/17/2024 10:49:41 - INFO - __main__ -   Step 2000 processed with cumulative accuracy: 36.82%\n",
            "11/17/2024 10:50:32 - INFO - __main__ -   Step 2100 processed with cumulative accuracy: 36.85%\n",
            "11/17/2024 10:51:23 - INFO - __main__ -   Step 2200 processed with cumulative accuracy: 36.91%\n",
            "11/17/2024 10:52:13 - INFO - __main__ -   Step 2300 processed with cumulative accuracy: 36.87%\n",
            "11/17/2024 10:53:04 - INFO - __main__ -   Step 2400 processed with cumulative accuracy: 36.86%\n",
            "11/17/2024 10:53:55 - INFO - __main__ -   Step 2500 processed with cumulative accuracy: 36.81%\n",
            "11/17/2024 10:54:45 - INFO - __main__ -   Step 2600 processed with cumulative accuracy: 36.78%\n",
            "11/17/2024 10:55:36 - INFO - __main__ -   Step 2700 processed with cumulative accuracy: 36.78%\n",
            "11/17/2024 10:56:27 - INFO - __main__ -   Step 2800 processed with cumulative accuracy: 36.78%\n",
            "11/17/2024 10:57:17 - INFO - __main__ -   Step 2900 processed with cumulative accuracy: 36.80%\n",
            "11/17/2024 10:58:08 - INFO - __main__ -   Step 3000 processed with cumulative accuracy: 36.81%\n",
            "11/17/2024 10:58:59 - INFO - __main__ -   Step 3100 processed with cumulative accuracy: 36.85%\n",
            "11/17/2024 10:59:49 - INFO - __main__ -   Step 3200 processed with cumulative accuracy: 36.85%\n",
            "11/17/2024 11:00:40 - INFO - __main__ -   Step 3300 processed with cumulative accuracy: 36.85%\n",
            "11/17/2024 11:01:31 - INFO - __main__ -   Step 3400 processed with cumulative accuracy: 36.82%\n",
            "11/17/2024 11:02:21 - INFO - __main__ -   Step 3500 processed with cumulative accuracy: 36.80%\n",
            "11/17/2024 11:03:12 - INFO - __main__ -   Step 3600 processed with cumulative accuracy: 36.80%\n",
            "11/17/2024 11:04:03 - INFO - __main__ -   Step 3700 processed with cumulative accuracy: 36.82%\n",
            "11/17/2024 11:04:53 - INFO - __main__ -   Step 3800 processed with cumulative accuracy: 36.83%\n",
            "11/17/2024 11:05:43 - INFO - __main__ -   Step 3900 processed with cumulative accuracy: 36.83%\n",
            "11/17/2024 11:06:34 - INFO - __main__ -   Step 4000 processed with cumulative accuracy: 36.84%\n",
            "11/17/2024 11:07:25 - INFO - __main__ -   Step 4100 processed with cumulative accuracy: 36.83%\n",
            "11/17/2024 11:07:35 - INFO - __main__ -   Final Test Accuracy: 36.83%\n",
            "11/17/2024 11:07:48 - INFO - __main__ -   Evaluated on 50000 samples, saved predictions at /content/GPT-NeoX-Colab/out/predictions.txt and ground truths at /content/GPT-NeoX-Colab/out/answers.txt\n",
            "11/17/2024 11:07:50 - INFO - __main__ -   Test accuracy: 36.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 41min 34s, sys: 7.06 s, total: 41min 41s\n",
            "Wall time: 41min 35s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "import humanize\n",
        "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
        "from types import SimpleNamespace\n",
        "import pickle\n",
        "\n",
        "pretrained_model_path = f\"{GPTNeoXColabDir}/models/codecompletion/latest\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set up evaluation arguments\n",
        "\n",
        "# Total Tokens: 84,277,947\n",
        "# Sequence Length: 2,048\n",
        "# Total Sequences: 84,277,947 / 2,048 ≈ 41,153\n",
        "# Batch Size: 10\n",
        "# Total Steps: ceil(41,153 / 10) = 4,116\n",
        "# Logging Steps: 100\n",
        "# Total Log Events: ceil(4,116 / 100) = 42\n",
        "\n",
        "args = {\n",
        "    \"logging_steps\": 100,\n",
        "    \"output_dir\": f\"{GPTNeoXColabDir}/out\",\n",
        "    \"data_dir\": f\"{GPTNeoXColabDir}/data/codecompletion/token_completion\",\n",
        "    \"device\": device,\n",
        "    \"seq_length\": 2048,\n",
        "    \"max_eval_length\": None,\n",
        "    \"overwrite_cache\": True, # So it will take into account changes in batch size\n",
        "    \"eval_batch_size\": 10,\n",
        "}\n",
        "\n",
        "# Wrap args dictionary in a namespace to allow dot notation\n",
        "args = SimpleNamespace(**args)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_path, sep_token='<EOL>', bos_token='<s>', eos_token='</s>', pad_token='<pad>')\n",
        "model = GPTNeoXForCausalLM.from_pretrained(pretrained_model_path)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "readable_params = humanize.intword(total_params)\n",
        "logger.info(f\"Model has {readable_params} trainable parameters\")\n",
        "\n",
        "# Evaluate model\n",
        "total_predictions, total_correct = eval_acc(args, model, tokenizer, 'test')\n",
        "accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
        "logger.info(f\"Test accuracy: {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNfCUIl3BOjS",
        "outputId": "ab8bff3d-8609-4a63-ed32-11bd0fe7563c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Total 37318111 tokens, accuracy: 36.83\n"
          ]
        }
      ],
      "source": [
        "#@title Run evaluator.py on the generated files\n",
        "answers_file = f\"{GPTNeoXColabDir}/out/answers.txt\"\n",
        "predictions_file = f\"{GPTNeoXColabDir}/out/predictions.txt\"\n",
        "!python {GPTNeoXColabDir}/scripts/evaluator.py --answers {answers_file} --predictions {predictions_file}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzNijRX8Av-t"
      },
      "source": [
        "# Results\n",
        "## Training on 7000 iterations of batch size 8\n",
        "- max_eval_length of 50000 36.83% (41mins on L4 GPU)\n",
        "- max_eval_length of 10000 36.71%\n",
        "- max_eval_length of 100 38.65% (4min38s on laptop CPU)\n",
        "- max_eval_length of 10 35.57% (2min6s on laptop CPU)\n",
        "- max_eval_length of 1 40.0% (8s on laptop CPU)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}