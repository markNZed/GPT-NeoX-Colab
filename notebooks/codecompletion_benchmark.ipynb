{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/markNZed/GPT-NeoX-Colab/blob/main/notebooks/codecompletion_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  DOCKER = False\n",
    "except:\n",
    "  DOCKER = True\n",
    "print(DOCKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8tGrS9KJu7QA"
   },
   "outputs": [],
   "source": [
    "# We could modify these paths to \"stub\" behavior for test/dev\n",
    "workspaceDir = \"/content\"\n",
    "GPTNeoXColabDirName = \"GPT-NeoX-Colab\"\n",
    "if DOCKER:\n",
    "    GPTNeoXColabDir = f\"/workspace\"\n",
    "else:\n",
    "    GPTNeoXColabDir = f\"{workspaceDir}/{GPTNeoXColabDirName}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpgI19mPrtvy"
   },
   "source": [
    "# Clone CodeXGLUE Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0qBcQotWw1U0"
   },
   "outputs": [],
   "source": [
    "# Not using this at but for a final sanity check we should use the data and evaluate.py from here\n",
    "#%cd {workspaceDir}\n",
    "#!git clone --depth 1 https://github.com/microsoft/CodeXGLUE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOySwjeyktsH",
    "outputId": "ed85ff10-8381-4ff0-928d-b336b10f9a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n",
      "CPU times: user 2.04 ms, sys: 1 ms, total: 3.05 ms\n",
      "Wall time: 2.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#@title Clone GPT-NeoX-Colab\n",
    "if DOCKER:\n",
    "    %cd {GPTNeoXColabDir}\n",
    "else:\n",
    "    %cd {workspaceDir}\n",
    "    # Don't use --depth 1 because that does not play nice with git-annex\n",
    "    !git clone https://github.com/markNZed/GPT-NeoX-Colab.git\n",
    "    %cd {GPTNeoXColabDir}\n",
    "    %pip install -q -r requirements_colab.txt\n",
    "    %pip install -q ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yu4yRpEzCyF6",
    "outputId": "16e01391-a108-4107-939d-f621eeec5101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n",
      "Data retrieval successful.\n",
      "/workspace/data/codecompletion\n",
      "/workspace\n",
      "Data retrieval successful.\n",
      "/workspace/models/codecompletion\n"
     ]
    }
   ],
   "source": [
    "%cd {GPTNeoXColabDir}\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(f\"{GPTNeoXColabDir}/.env\")\n",
    "import GPTNeoXColab\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"data/codecompletion/token_completion.tar.gz\")\n",
    "%cd {GPTNeoXColabDir}/data/codecompletion\n",
    "if not os.path.exists(f\"data/codecompletion/token_completion\"):\n",
    "    !tar -xzf token_completion.tar.gz\n",
    "%cd {GPTNeoXColabDir}\n",
    "GPTNeoXColab.utils.colab.fetch_data(\"models/codecompletion/global_step7000_HF.tar.gz\")\n",
    "%cd {GPTNeoXColabDir}/models/codecompletion\n",
    "if not os.path.exists(f\"latest\"):\n",
    "    !tar -xzf global_step7000_HF.tar.gz\n",
    "    !mv global_step7000_HF latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ7f8hTqipgF"
   },
   "source": [
    "# Using Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmOgA5alzT2A",
    "outputId": "27200673-21b0-4cfe-a913-4be9b27807f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/models/codecompletion/latest\n"
     ]
    }
   ],
   "source": [
    "%cd {GPTNeoXColabDir}/models/codecompletion/latest\n",
    "if not os.path.exists(\"vocab.json\"):\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
    "    !mv gpt2-vocab.json vocab.json\n",
    "if not os.path.exists(\"merges.txt\"):\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
    "    !mv gpt2-merges.txt merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Hm2e4KH4BOjM"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import gc\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, logger, file_type='train', seq_length=1024):\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.makedirs(args.output_dir)\n",
    "        cached_file = os.path.join(args.output_dir, file_type+\"_blocksize_%d\"%(seq_length))\n",
    "        if os.path.exists(cached_file) and not args.overwrite_cache:\n",
    "            with open(cached_file, 'rb') as handle:\n",
    "                self.inputs = pickle.load(handle)\n",
    "\n",
    "        else:\n",
    "            self.inputs = []\n",
    "\n",
    "            datafile = os.path.join(args.data_dir, f\"{file_type}.txt\")\n",
    "            with open(datafile) as f:\n",
    "                data = f.readlines()\n",
    "\n",
    "            length = len(data)\n",
    "            logger.info(\"Data size: %d\"%(length))\n",
    "            input_ids = []\n",
    "            for idx,x in enumerate(data):\n",
    "                x = x.strip()\n",
    "                if x.startswith(\"<s>\") and x.endswith(\"</s>\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    x = \"<s> \" + x + \" </s>\"\n",
    "                try:\n",
    "                    input_ids.extend(tokenizer.encode(x))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if idx % (length//10) == 0:\n",
    "                    percent = idx / (length//10) * 10\n",
    "                    logger.info(\"load %d\"%(percent))\n",
    "                if args.max_eval_length is not None and (idx + 1) == args.max_eval_length:\n",
    "                    logger.info(f\"max eval length reached at {idx}\")\n",
    "                    break\n",
    "            del data\n",
    "            gc.collect()\n",
    "\n",
    "            logger.info(f\"tokens: {len(input_ids)}\")\n",
    "            self.split(input_ids, tokenizer, logger, seq_length=seq_length)\n",
    "            del input_ids\n",
    "            gc.collect()\n",
    "\n",
    "            with open(cached_file, 'wb') as handle:\n",
    "                pickle.dump(self.inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def split(self, input_ids, tokenizer, logger, seq_length=1024):\n",
    "        sample = []\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            sample = input_ids[i: i+seq_length]\n",
    "            if len(sample) == seq_length:\n",
    "                for j in range(seq_length):\n",
    "                    if tokenizer.convert_ids_to_tokens(sample[seq_length-1-j])[0] == '\\u0120' or tokenizer.convert_ids_to_tokens(sample[seq_length-1-j]).startswith(\"<NUM_LIT\"):\n",
    "                        break\n",
    "                    if sample[seq_length-1-j] in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id]:\n",
    "                        if sample[seq_length-1-j] != tokenizer.bos_token_id:\n",
    "                            j -= 1\n",
    "                        break\n",
    "                if j == seq_length-1:\n",
    "                    print(tokenizer.decode(sample))\n",
    "                    exit()\n",
    "                sample = sample[: seq_length-1-j]\n",
    "            # print(len(sample))\n",
    "            i += len(sample)\n",
    "            pad_len = seq_length-len(sample)\n",
    "            sample += [tokenizer.pad_token_id]*pad_len\n",
    "            self.inputs.append(sample)\n",
    "\n",
    "            if len(self.inputs) % 10000 == 0:\n",
    "                logger.info(f\"{len(self.inputs)} samples\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.inputs[item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "oA3pTtlIBOjQ"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "from types import SimpleNamespace\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def decode_token_ids(token_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert token IDs to a string of code, handling special tokens, spacing, and literals.\n",
    "    \"\"\"\n",
    "    decoded_code = \"\"\n",
    "    for token_id in token_ids:\n",
    "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "\n",
    "        # Handle tokens with a space prefix (e.g., '\\u0120')\n",
    "        if token.startswith('\\u0120'):\n",
    "            if not decoded_code.endswith(\" \"):  # Avoid double spaces\n",
    "                decoded_code += \" \"\n",
    "            decoded_code += token[1:]  # Remove the space marker\n",
    "        # Handle special tokens (bos, eos, sep, pad)\n",
    "        elif token_id in [\n",
    "            tokenizer.bos_token_id,\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.sep_token_id,\n",
    "            tokenizer.pad_token_id\n",
    "        ]:\n",
    "            decoded_code += \" \" + token + \" \"  # Add spaces around special tokens\n",
    "        # Handle literals (e.g., <NUM_LIT>, <STR_LIT>)\n",
    "        elif token.startswith(\"<NUM_LIT\") or token.startswith(\"<STR_LIT\"):\n",
    "            decoded_code += \" \" + token + \" \"\n",
    "        # Handle regular tokens\n",
    "        else:\n",
    "            decoded_code += token\n",
    "\n",
    "    # Strip any leading/trailing spaces from the final decoded string\n",
    "    return decoded_code.strip()\n",
    "\n",
    "def eval_acc(args, model, tokenizer, file_type='test'):\n",
    "    \"\"\"\n",
    "    Evaluate the model’s token-level code completion accuracy.\n",
    "    \"\"\"\n",
    "    # Load evaluation dataset\n",
    "    eval_dataset = EvalDataset(tokenizer, args, logger, file_type=file_type, seq_length=args.seq_length)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=SequentialSampler(eval_dataset), batch_size=args.eval_batch_size)\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters for accuracy\n",
    "    total_correct, total_predictions = 0, 0\n",
    "    total_pred_tokens, total_gt_tokens = [], []\n",
    "\n",
    "    # Iterate through batches in the evaluation dataset\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        inputs = batch.to(args.device)\n",
    "\n",
    "        # no_grad because only inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            predicted_token_ids = outputs.logits.argmax(-1)\n",
    "\n",
    "        # Move from the GPU to CPU (if GPU is being used)\n",
    "        pred_ids = predicted_token_ids.cpu()\n",
    "        gt_ids = inputs.cpu()\n",
    "\n",
    "        # Process predictions and ground truths\n",
    "        all_pred = []\n",
    "        all_gt = []\n",
    "        for pred_seq, gt_seq in zip(pred_ids, gt_ids):\n",
    "            pred_seq = pred_seq.tolist()\n",
    "            gt_seq = gt_seq.tolist()\n",
    "\n",
    "            # Arrays that can store multiple \"sub-tokens\"\n",
    "            # The model may tokenize into smaller tokens than the benchmark uses\n",
    "            now_pred = []\n",
    "            now_gt = []\n",
    "            for i, (pred_id, gt_id) in enumerate(zip(pred_seq, gt_seq)):\n",
    "                gt_token = tokenizer.convert_ids_to_tokens(gt_id)\n",
    "                pred_token = tokenizer.convert_ids_to_tokens(pred_id)\n",
    "\n",
    "                if i == 0:\n",
    "                    if gt_id in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]:\n",
    "                        now_gt = [gt_id]\n",
    "                        # These tokens are excluded from accuracy metrics so insert a placeholder in now_pred\n",
    "                        now_pred = [0]\n",
    "                        all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
    "                        all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
    "                        now_gt = []\n",
    "                        now_pred = []\n",
    "                    else:\n",
    "                        # The prediction is the next token after the ground_truth so we do not use it\n",
    "                        now_gt = [gt_id]\n",
    "                        now_pred = [0]\n",
    "                else:\n",
    "                    # \\u0120 special char indicates the start of a new token\n",
    "                    if gt_token.startswith('\\u0120'):\n",
    "                        # Check not empty because it can be reset to empty\n",
    "                        if len(now_gt) > 0:\n",
    "                            try:\n",
    "                                # only the first word of the decoded string is appended to all_pred\n",
    "                                all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
    "                            except IndexError:\n",
    "                                all_pred.append(\"<SPACE>\")\n",
    "                            all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
    "                            now_gt = []\n",
    "                            now_pred = []\n",
    "                    # We are at a gt_token boundary\n",
    "                    if gt_id in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id] \\\n",
    "                    or gt_token.startswith(\"<NUM_LIT\") or gt_token.startswith(\"<STR_LIT\"):\n",
    "                        if len(now_gt) > 0:\n",
    "                            try:\n",
    "                                # only the first word of the decoded string is appended to all_pred\n",
    "                                all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
    "                            except IndexError:\n",
    "                                all_pred.append(\"<SPACE>\")\n",
    "                            all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
    "                        now_gt = [gt_id]\n",
    "                        now_pred = [pred_seq[i-1]] # Because prediction is one token ahead of gt\n",
    "                        try:\n",
    "                            all_pred.append(decode_token_ids(now_pred, tokenizer).strip().split()[0])\n",
    "                        except IndexError:\n",
    "                            all_pred.append(\"<SPACE>\")\n",
    "                        all_gt.append(decode_token_ids(now_gt, tokenizer).strip())\n",
    "                        now_gt = []\n",
    "                        now_pred = []\n",
    "                        continue\n",
    "                    \n",
    "                    now_gt.append(gt_id)\n",
    "                    now_pred.append(pred_seq[i-1]) # Because prediction is one token ahead of gt\n",
    "\n",
    "        assert len(all_pred) == len(all_gt)\n",
    "\n",
    "        total_pred_tokens.extend(all_pred)\n",
    "        total_gt_tokens.extend(all_gt)\n",
    "\n",
    "        # Calculate batch accuracy\n",
    "        for pred_token, gt_token in zip(all_pred, all_gt):\n",
    "            if gt_token not in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\", \"\"]:\n",
    "                total_predictions += 1\n",
    "                if pred_token == gt_token:\n",
    "                    total_correct += 1\n",
    "                    #logger.info(f\"Match {total_correct}/{total_predictions} {pred_token} == '{gt_token}'\")\n",
    "                else:\n",
    "                    #logger.info(f\"Mismatch {total_correct}/{total_predictions} {pred_token} != '{gt_token}'\")\n",
    "                    pass\n",
    "\n",
    "        # Logging progress\n",
    "        if step % args.logging_steps == 0:\n",
    "            accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
    "            logger.info(f\"Step {step} processed with cumulative accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    # Final accuracy calculation\n",
    "    accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
    "    logger.info(f\"Final Test Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    # Call post_process to generate predictions.txt and answers.txt\n",
    "    pred_file = os.path.join(args.output_dir, \"predictions.txt\")\n",
    "    gt_file = os.path.join(args.output_dir, \"answers.txt\")\n",
    "    true_texts = open(os.path.join(args.data_dir, f\"{file_type}.txt\")).readlines()\n",
    "    total_samples = post_process(total_pred_tokens, total_gt_tokens, true_texts, pred_file, gt_file)\n",
    "    logger.info(f\"Evaluated on {total_samples} samples, saved predictions at {pred_file} and ground truths at {gt_file}\")\n",
    "\n",
    "\n",
    "    return total_predictions, total_correct\n",
    "\n",
    "def post_process(preds, gts, true_gts, pred_file_path, gt_file_path):\n",
    "    \"\"\"\n",
    "    Save the post-processed predictions and ground truths, and verify with the expected true ground truths.\n",
    "\n",
    "    Args:\n",
    "        preds: List of predicted tokens from the model.\n",
    "        gts: List of ground truth tokens for each prediction.\n",
    "        true_gts: List of full ground truth sequences for each input, used for verification.\n",
    "        pred_file_path: Path to the file where the processed predictions will be saved.\n",
    "        gt_file_path: Path to the file where the processed ground truths will be saved.\n",
    "\n",
    "    Returns:\n",
    "        int: The count of sequences processed and saved.\n",
    "    \"\"\"\n",
    "    with open(pred_file_path, \"w\") as pred_file, open(gt_file_path, \"w\") as gt_file:\n",
    "        count = 0\n",
    "        new_gt = []\n",
    "        new_pred = []\n",
    "\n",
    "        for pred, gt in zip(preds, gts):\n",
    "            if gt in [\"\", \"<pad>\"]:\n",
    "                continue\n",
    "            new_gt.append(gt)\n",
    "            # Spaces are used to separate the tokens in prediction.txt\n",
    "            # So we remove extrs spaces\n",
    "            new_pred.append(pred.replace(\" \", \"\"))\n",
    "\n",
    "            if gt == \"</s>\":\n",
    "                gt_str = \" \".join(new_gt)\n",
    "                pred_str = \" \".join(new_pred)\n",
    "                if gt_str != true_gts[count].strip():\n",
    "                    print(f\"gt_str   {gt_str}\")\n",
    "                    print(f\"true_gts {true_gts[count].strip()}\")\n",
    "                    raise Exception(f\"Sample {count} mismatch between ground truth and expected text\")\n",
    "                assert gt_str == true_gts[count].strip(), f\"Sample {count} mismatch between ground truth and expected text\"\n",
    "                pred_file.write(pred_str + \"\\n\")\n",
    "                gt_file.write(gt_str + \"\\n\")\n",
    "                count += 1\n",
    "                new_gt = []\n",
    "                new_pred = []\n",
    "\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "U9QOv0YAIfyH"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Reset the root logger to avoid inherited handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Configure your specific logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Check if logger already has handlers; clear them if necessary\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# Create a StreamHandler to ensure logging messages appear in the notebook\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Set a formatter for consistent log message formatting\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s -   %(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the StreamHandler to the logger\n",
    "logger.addHandler(stream_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xmqKGg6DxUH",
    "outputId": "d57cf696-c310-4992-8256-5f79faca74c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizerFast'. \n",
      "The class this function is called from is 'GPT2Tokenizer'.\n",
      "11/16/2024 00:04:14 - INFO - __main__ -   Model has 44.6 million trainable parameters\n",
      "11/16/2024 00:04:14 - INFO - __main__ -   Data size: 50000\n",
      "11/16/2024 00:04:14 - INFO - __main__ -   load 0\n",
      "11/16/2024 00:04:15 - INFO - __main__ -   max eval length reached at 9\n",
      "11/16/2024 00:04:15 - INFO - __main__ -   tokens: 20751\n",
      "11/16/2024 00:04:18 - INFO - __main__ -   Step 0 processed with cumulative accuracy: 34.67%\n",
      "11/16/2024 00:04:21 - INFO - __main__ -   Step 1 processed with cumulative accuracy: 37.67%\n",
      "11/16/2024 00:04:24 - INFO - __main__ -   Step 2 processed with cumulative accuracy: 37.95%\n",
      "11/16/2024 00:04:26 - INFO - __main__ -   Step 3 processed with cumulative accuracy: 35.33%\n",
      "11/16/2024 00:04:29 - INFO - __main__ -   Step 4 processed with cumulative accuracy: 35.34%\n",
      "11/16/2024 00:04:32 - INFO - __main__ -   Step 5 processed with cumulative accuracy: 34.13%\n",
      "11/16/2024 00:04:34 - INFO - __main__ -   Step 6 processed with cumulative accuracy: 34.31%\n",
      "11/16/2024 00:04:37 - INFO - __main__ -   Step 7 processed with cumulative accuracy: 33.90%\n",
      "11/16/2024 00:04:40 - INFO - __main__ -   Step 8 processed with cumulative accuracy: 33.54%\n",
      "11/16/2024 00:04:43 - INFO - __main__ -   Step 9 processed with cumulative accuracy: 34.48%\n",
      "11/16/2024 00:04:46 - INFO - __main__ -   Step 10 processed with cumulative accuracy: 34.57%\n",
      "11/16/2024 00:04:46 - INFO - __main__ -   Final Test Accuracy: 34.57%\n",
      "11/16/2024 00:04:47 - INFO - __main__ -   Evaluated on 10 samples, saved predictions at /workspace/out/predictions.txt and ground truths at /workspace/out/answers.txt\n",
      "11/16/2024 00:04:47 - INFO - __main__ -   Test accuracy: 34.57%\n"
     ]
    }
   ],
   "source": [
    "import humanize\n",
    "\n",
    "pretrained_model_path = f\"{GPTNeoXColabDir}/models/codecompletion/latest\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set up evaluation arguments\n",
    "args = {\n",
    "    \"logging_steps\": 1,\n",
    "    \"output_dir\": f\"{GPTNeoXColabDir}/out\",\n",
    "    \"data_dir\": f\"{GPTNeoXColabDir}/data/codecompletion/token_completion\",\n",
    "    \"device\": device,\n",
    "    \"seq_length\": 2048,\n",
    "    \"max_eval_length\": 10,\n",
    "    \"overwrite_cache\": True,\n",
    "    \"eval_batch_size\": 1,\n",
    "}\n",
    "\n",
    "# Wrap args dictionary in a namespace to allow dot notation\n",
    "args = SimpleNamespace(**args)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_path, sep_token='<EOL>', bos_token='<s>', eos_token='</s>', pad_token='<pad>')\n",
    "model = GPTNeoXForCausalLM.from_pretrained(pretrained_model_path)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "readable_params = humanize.intword(total_params) \n",
    "logger.info(f\"Model has {readable_params} trainable parameters\")\n",
    "\n",
    "# Evaluate model\n",
    "total_predictions, total_correct = eval_acc(args, model, tokenizer, 'test')\n",
    "accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
    "logger.info(f\"Test accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNfCUIl3BOjS",
    "outputId": "d382a402-b64f-4b66-8aa4-d5b6f2b784bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total 9412 tokens, accuracy: 34.57\n"
     ]
    }
   ],
   "source": [
    "#@title Run evaluator.py on the generated files\n",
    "evaluator_script = f\"{GPTNeoXColabDir}/scripts/evaluator.py\"\n",
    "answers_file = f\"{GPTNeoXColabDir}/out/answers.txt\"\n",
    "predictions_file = f\"{GPTNeoXColabDir}/out/predictions.txt\"\n",
    "!python {evaluator_script} --answers {answers_file} --predictions {predictions_file}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
